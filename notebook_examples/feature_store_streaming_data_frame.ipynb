{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "@notebook{feature_store_streaming_data_frame.ipynb,\n",
    "    title: Enhancing Real-time Capabilities: Streaming Use Cases in Feature Store.,\n",
    "    summary: Carrying out schema enforcement and schema evolution on Feature Store.,\n",
    "    developed_on: fspyspark32_p38_cpu_v3,\n",
    "    keywords: feature store, querying, streaming\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!odsc conda install -s fspyspark32_p38_cpu_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2022, 2023 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "***\n",
    "\n",
    "# <font color=\"red\">Enhancing Real-time Capabilities: Streaming Use Cases in Feature Store</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "---\n",
    "Managing many datasets, data sources and transformations for machine learning is complex and costly. Poorly cleaned data, data issues, bugs in transformations, data drift, and training serving skew all lead to increased model development time and poor model performance. Feature store can be used to solve many of the problems becuase it provides a centralised way to transform and access data for training and serving time. Feature store helps define a standardised pipeline for ingestion of data and querying of data. This notebook shows how schema enforcement and schema evolution are carried out in Feature Store\n",
    "\n",
    "Compatible conda pack: [PySpark 3.2 and Feature store](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8\n",
    "\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href='#introduction'>1. Introduction</a>\n",
    "- <a href='#pre_requisites'>2. Pre-requisites to Running this Notebook</a>\n",
    "    - <a href='#setup_setup'>2.1. Setup</a>\n",
    "    - <a href='#policies_'>2.2. Policies</a>\n",
    "    - <a href='#authentication'>2.3. Authentication</a>\n",
    "    - <a href='#variables'>2.4. Variables</a>\n",
    "- <a href='#dataexploration'>3. Streaming data</a>\n",
    "    - <a href='#dataexploration'>3.1. Exploration of data stream in feature store</a>\n",
    "    - <a href='#feature_store'>3.2. Create feature store logical entities</a>\n",
    "    - <a href='#ingestion_modes'>3.3. Ingestion Modes</a>\n",
    "        - <a href='#append'>3.3.1. Append</a>\n",
    "        - <a href='#complete'>3.3.2. Complete</a>\n",
    "- <a href='#references'>4. References</a>\n",
    "\n",
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='introduction'></a>\n",
    "# 1. Introduction\n",
    "\n",
    "OCI Data Science feature store is a stack-based API solution that's deployed using OCI Resource Manager in your tenancy.\n",
    "\n",
    "Review the following key terms to understand the Data Science feature store:\n",
    "\n",
    "\n",
    "* **Feature Vector**: Set of feature values for any one primary or identifier key. For example, all or a subset of features of customer id ‘2536’ can be called as one feature vector.\n",
    "\n",
    "* **Feature**: A feature is an individual measurable property or characteristic of a phenomenon being observed.\n",
    "\n",
    "* **Entity**: An entity is a group of semantically related features. The first step a consumer of features would typically do when accessing the feature store service is to list the entities and the entities associated features. Or, an entity is an object or concept that is described by its features. Examples of entities are customer, product, transaction, review, image, document, and so on.\n",
    "\n",
    "* **Feature Group**: A feature group in a feature store is a collection of related features that are often used together in machine learning (ML) models. It serves as an organizational unit within the feature store for you to manage, version, and share features across different ML projects. By organizing features into groups, data scientists and ML engineers can efficiently discover, reuse, and collaborate on features reducing the redundant work and ensuring consistency in feature engineering.\n",
    "\n",
    "* **Feature Group Job**: A feature group job is the processing instance of a feature group. Each feature group job includes validation and statistics results.\n",
    "\n",
    "* **Dataset**: A dataset is a collection of features that are used together to either train a model or perform model inference.\n",
    "\n",
    "* **Dataset Job**: A dataset job is the processing instance of a dataset. Each dataset job includes validation and statistics results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='pre_requisites'></a>\n",
    "# 2. Pre-requisites to Running this Notebook\n",
    "Notebook Sessions are accessible using the PySpark 3.2 and Feature Store Python 3.8 (fspyspark32_p38_cpu_v2) conda environment.\n",
    "\n",
    "You can customize `fspyspark32_p38_cpu_v2`, publish it, and use it as a runtime environment for a Notebook session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='setup_setup'></a>\n",
    "### 2.1. Setup\n",
    "\n",
    "<a id='setup_spark-defaults'></a>\n",
    "### `spark-defaults.conf`\n",
    "\n",
    "The `spark-defaults.conf` file is used to define the properties that are used by Spark. A templated version is installed when you install a Data Science conda environment that supports PySpark. However, you must update the template so that the Data Catalog metastore can be accessed. You can do this manually. However, the `odsc data-catalog config` commandline tool is ideal for setting up the file because it gathers information about your environment, and uses that to build the file.\n",
    "\n",
    "The `odsc data-catalog config` command line tool needs the `--metastore` option to define the Data Catalog metastore OCID. No other command line option is needed because settings have default values, or they take values from your notebook session environment. Following are common parameters that you may need to override.\n",
    "\n",
    "The `--authentication` option sets the authentication mode. It supports resource principal and API keys. The preferred method for authentication is resource principal, which is sent with `--authentication resource_principal`. If you want to use API keys, then use the `--authentication api_key` option. If the `--authentication` isn't specified, API keys are used. When API keys are used, information from the OCI configuration file is used to create the `spark-defaults.conf` file.\n",
    "\n",
    "Object Storage and Data Catalog are regional services. By default, the region is set to the region your notebook session is running in. This information is taken from the environment variable, `NB_REGION`. Use the `--region` option to override this behavior.\n",
    "\n",
    "The default location of the `spark-defaults.conf` file is `/home/datascience/spark_conf_dir` as defined in the `SPARK_CONF_DIR` environment variable. Use the `--output` option to define the directory where to write the file.\n",
    "\n",
    "You need to determine what settings are appropriate for your configuration. However, the following works for most configurations and is run in a terminal window.\n",
    "\n",
    "```bash\n",
    "odsc data-catalog config --authentication resource_principal --metastore <metastore_id>\n",
    "```\n",
    "For more assistance, use the following command in a terminal window:\n",
    "\n",
    "```bash\n",
    "odsc data-catalog config --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='policies_'></a>\n",
    "### 2.2. Policies\n",
    "This section covers the creation of dynamic groups and policies needed to use the service.\n",
    "\n",
    "* [Data Flow Policies](https://docs.oracle.com/iaas/data-flow/using/policies.htm/)\n",
    "* [Data Catalog Metastore Required Policies](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm)\n",
    "* [Getting Started with Data Flow](https://docs.oracle.com/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "* [About Data Science Policies](https://docs.oracle.com/iaas/data-science/using/policies.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"authentication\"></a>\n",
    "### 2.3. Authentication\n",
    "The [Oracle Accelerated Data Science SDK (ADS)](https://docs.oracle.com/iaas/tools/ads-sdk/latest/index.html) controls the authentication mechanism with the notebook session.<br>\n",
    "To setup authentication use the ```ads.set_auth(\"resource_principal\")``` or ```ads.set_auth(\"api_key\")```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(auth=\"resource_principal\", client_kwargs={\"fs_service_endpoint\": \"<api_gateway_endpoint>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"variables\"></a>\n",
    "### 2.4. Variables\n",
    "To run this notebook, you must provide some information about your tenancy configuration. To create and run a feature store, you must specify a `<compartment_id>` and  `<metastore_id>` for offline feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "compartment_id = os.environ.get(\"NB_SESSION_COMPARTMENT_OCID\")\n",
    "metastore_id = \"<metastore_id>\"\n",
    "\n",
    "# Path to the stream data directory\n",
    "stream_data_dir = \"oci://{bucket}@{namespace}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ads.feature_store.feature_store import FeatureStore\n",
    "from ads.feature_store.feature_group import FeatureGroup\n",
    "from ads.feature_store.model_details import ModelDetails\n",
    "from ads.feature_store.dataset import Dataset\n",
    "from ads.feature_store.common.enums import DatasetIngestionMode\n",
    "\n",
    "from ads.feature_store.feature_group_expectation import ExpectationType\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "from ads.feature_store.feature_store_registrar import FeatureStoreRegistrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='dataexploration'></a>\n",
    "### 3.1. Exploration of stream data in feature store\n",
    "\n",
    "Feature store managed spark session can be created using ```SparkSessionSingleton(metastore_id=<metastore_id>).get_spark_session()```, this session is configured with the specified metastore_id for seamless integration with the Feature Store functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ads.feature_store.common.spark_session_singleton import SparkSessionSingleton\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Get the spark session managed by the feature store\n",
    "spark = SparkSessionSingleton(metastore_id=metastore_id).get_spark_session()\n",
    "\n",
    "# Define the schema for the streaming data frame\n",
    "food_reviews_schema = StructType() \\\n",
    "    .add(\"Time\", \"string\") \\\n",
    "    .add(\"ProductId\", \"string\") \\\n",
    "    .add(\"UserId\", \"string\") \\\n",
    "    .add(\"Score\", \"string\") \\\n",
    "    .add(\"Summary\", \"string\") \\\n",
    "    .add(\"Text\", \"string\")\n",
    "\n",
    "food_reviews_streaming_df = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(food_reviews_schema) \\\n",
    "    .csv(f\"{stream_data_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"feature_store\"></a>\n",
    "### 3.2. Create feature store logical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2.1. Feature Store\n",
    "Feature store is the top level entity for feature store service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_store_resource = (\n",
    "    FeatureStore().\n",
    "    with_description(\"Data consisting of food reviews\").\n",
    "    with_compartment_id(compartment_id).\n",
    "    with_name(\"food reviews\").\n",
    "    with_offline_config(metastore_id=metastore_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_store\"></a>\n",
    "##### Create Feature Store\n",
    "\n",
    "Call the ```.create()``` method of the Feature store instance to create a feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_store = feature_store_resource.create()\n",
    "feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2.2. Entity\n",
    "An entity is a group of semantically related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "entity = feature_store.create_entity(\n",
    "    name=\"food reviews streaming use case\",\n",
    "    description=\"description for food reviews details\"\n",
    ")\n",
    "entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_transformation\"></a>\n",
    "#### 3.2.3 Transformation\n",
    "Transformations in a feature store refers to the operations and processes applied to raw data to create, modify or derive new features that can be used as inputs for ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_average_score_per_product(input_df):\n",
    "    # Perform aggregation to calculate average score for each product\n",
    "    return f\"\"\"\n",
    "        SELECT ProductId, AVG(CAST(Score AS DOUBLE)) AS AvgScore\n",
    "        FROM {input_df}\n",
    "        GROUP BY ProductId\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ads.feature_store.transformation import TransformationMode\n",
    "\n",
    "average_score_transformation = feature_store.create_transformation(\n",
    "    transformation_mode=TransformationMode.SQL,\n",
    "    source_code_func=calculate_average_score_per_product,\n",
    "    name=\"calculate_average_score_per_product\",\n",
    ")\n",
    "average_score_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_relevant_columns(input_df):\n",
    "    # Select relevant columns from the streaming DataFrame\n",
    "    return f\"\"\"\n",
    "        SELECT Time, ProductId, UserId, Score, Summary, Text\n",
    "        FROM {input_df}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ads.feature_store.transformation import TransformationMode\n",
    "\n",
    "transformation = feature_store.create_transformation(\n",
    "    transformation_mode=TransformationMode.SQL,\n",
    "    source_code_func=select_relevant_columns,\n",
    "    name=\"select_relevant_columns\",\n",
    ")\n",
    "transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"ingestion_modes\"></a>\n",
    "### 3.3. Ingestion modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feature store currently offers two modes for streaming ingestion:\n",
    "\n",
    "###### 1. Append Mode (Default)\n",
    "\n",
    "In this default mode, only the new rows added to the Result Table since the last trigger are outputted to the sink. This mode is suitable for queries where the rows added to the Result Table do not change.\n",
    "\n",
    "###### 2. Complete Mode\n",
    "\n",
    "In Complete Mode, the entire Result Table is outputted to the sink after every trigger. This mode is specifically supported for aggregation queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"append\"></a>\n",
    "#### 3.3.1. Append\n",
    "\n",
    "In ``append`` mode, new data is added to the existing table. If the table already exists, the new data is appended to it, extending the dataset. This mode is suitable for scenarios where you want to continuously add new records without modifying or deleting existing data. It preserves the existing data and only appends the new data to the end of the table.\n",
    "\n",
    "For ``append`` mode, transformation attached to the ``FeatureGroup`` should not contain aggregation operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_group_for_append\"></a>\n",
    "##### 3.3.1.1 Feature Group\n",
    "\n",
    "Create feature group for food reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ads.feature_store.statistics_config import StatisticsConfig\n",
    "\n",
    "stats_config = StatisticsConfig().with_is_enabled(False)\n",
    "non_aggregated_fg = entity.create_feature_group(\n",
    "    primary_keys=[\"ProductId\"],\n",
    "    schema_details_dataframe=food_reviews_streaming_df,\n",
    "    statistics_config=stats_config,\n",
    "    name=\"food_reviews_feature_group_with_non_aggregation\",\n",
    "    transformation_id=transformation.id\n",
    ")\n",
    "\n",
    "non_aggregated_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = fg.materialise_stream(input_dataframe=food_reviews_streaming_df, checkpoint_dir=f\"{stream_data_dir}chec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"complete\"></a>\n",
    "#### 3.3.2. Complete\n",
    "\n",
    "Use ``complete`` as ingestion mode when you want to aggregate the data and output the entire results to sink every time. This mode is used only when you have streaming aggregated data. One example would be counting the words on streaming data and aggregating with previous data and output the results to sink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_group_for_complete\"></a>\n",
    "##### 3.3.2.1 Feature Group\n",
    "\n",
    "Create feature group for food reviews with streaming aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ads.feature_store.statistics_config import StatisticsConfig\n",
    "from ads.feature_store.common.enums import (\n",
    "    ExpectationType,\n",
    "    EntityType,\n",
    "    StreamingIngestionMode,\n",
    "    BatchIngestionMode,\n",
    ")\n",
    "\n",
    "stats_config = StatisticsConfig().with_is_enabled(False)\n",
    "aggregated_fg = entity.create_feature_group(\n",
    "    primary_keys=[\"ProductId\"],\n",
    "    schema_details_dataframe=food_reviews_streaming_df,\n",
    "    statistics_config=stats_config,\n",
    "    name=\"food_reviews_feature_group_with_data_aggregation\",\n",
    "    transformation_id=average_score_transformation.id\n",
    ")\n",
    "\n",
    "aggregated_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query = fg.materialise_stream(input_dataframe=food_reviews_streaming_df, checkpoint_dir=\"oci://demo-2@idogsu2ylimg/food-reviews/checkpoint-complete\", ingestion_mode=StreamingIngestionMode.COMPLETE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
