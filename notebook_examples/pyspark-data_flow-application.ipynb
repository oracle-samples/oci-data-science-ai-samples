{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@notebook{pyspark-data_flow-application.ipynb,\n",
    "    title: PySpark,\n",
    "    summary: Develop local PySpark applications and work with remote clusters using Data Flow.,\n",
    "    developed on: pyspark24_p37_cpu_v3,\n",
    "    keywords: pyspark, data flow, \n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6627d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade Oracle ADS to pick up latest features and maintain compatibility with Oracle Cloud Infrastructure.\n",
    "\n",
    "!pip install -U oracle-ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2019, 2022 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n",
    "\n",
    "# <font color=\"red\">PySpark</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "\n",
    "# Overview:\n",
    "\n",
    "This notebook provides Apache Spark operations for customers by bridging the existing local PySpark workflows with cloud based capabilities. Data scientists can use their familiar local environments with JupyterLab and work with remote data and remote clusters simply by selecting a kernel. The operations that will be demonstrated are: how to use the interactive Spark environment and produce a Spark script; how to prepare and create an application; how to prepare and create a run; how to list existing dataflow applications; and how to retrieve and display the logs.\n",
    "\n",
    "The purpose of the `dataflow` module is to provide an efficient and convenient way for users to launch a Spark application and run Spark jobs. The interactive Spark kernel provides a simple and efficient way to edit and build your Spark script, and easy access to read from OCI Object Storage.\n",
    "\n",
    "Developed on [PySpark 2.4 and Data Flow](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.7 (version 3.0)\n",
    "\n",
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href='#kernel'>Build a PySpark Script Using an Interactive Spark Kernel</a>\n",
    "- <a href=\"#ref\">References</a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Datasets are provided as a convenience.  Datasets are considered third-party content and are not considered materials \n",
    "under your agreement with Oracle.\n",
    "    \n",
    "You can access the `orcl_attrition` dataset license [here](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "from ads.dataflow.dataflow import DataFlow\n",
    "from os import path\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel'></a>\n",
    "# Build a PySpark Script Using an Interactive Spark Kernel \n",
    "\n",
    "Set up Spark session in your PySpark conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Python Spark SQL basic example\")\n",
    "    .config(\"spark.driver.cores\", \"4\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Employee Attrition data file from Oracle Cloud Infrastructure Object Storage into an Apache Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_attrition = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(\n",
    "        \"oci://hosted-ds-datasets@bigdatadatasciencelarge/synthetic/orcl_attrition.csv\"\n",
    "    )\n",
    "    .cache()\n",
    ")  # cache the dataset to increase computing speed\n",
    "emp_attrition.createOrReplaceTempView(\"emp_attrition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from emp_attrition limit 5\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize how monthly income and age relate to one another in the context of years in industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot = (\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "          SELECT \n",
    "              Age,\n",
    "              MonthlyIncome,\n",
    "              YearsInIndustry\n",
    "          FROM\n",
    "            emp_attrition \n",
    "          \"\"\"\n",
    "    )\n",
    "    .toPandas()\n",
    "    .plot.scatter(\n",
    "        x=\"Age\",\n",
    "        y=\"MonthlyIncome\",\n",
    "        title=\"Age vs Monthly Income\",\n",
    "        c=\"YearsInIndustry\",\n",
    "        cmap=\"viridis\",\n",
    "        figsize=(12, 12),\n",
    "        ax=ax,\n",
    "    )\n",
    ")\n",
    "plot.set_xlabel(\"Age\")\n",
    "plot.set_ylabel(\"Monthly Income\")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View all of the columns in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show columns from emp_attrition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a few columns using Apache Spark and convert it into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "         SELECT\n",
    "            Age,\n",
    "            MonthlyIncome,\n",
    "            YearsInIndustry\n",
    "          FROM\n",
    "            emp_attrition \"\"\"\n",
    "    )\n",
    "    .limit(10)\n",
    "    .toPandas()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also work with different compression formats within Dataflow. For example snappy parquet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to a snappy parquet file\n",
    "df.to_parquet(\"emp_attrition.parquet.snappy\", compression=\"snappy\")\n",
    "pd.read_parquet(\"emp_attrition.parquet.snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are able to read in this snappy parquet file to an Apache Spark dataframe\n",
    "read_snappy_df = (\n",
    "    SparkSession.builder.appName(\"Snappy Compression Loading Example\")\n",
    "    .config(\"spark.io.compression.codec\", \"org.apache.spark.io.SnappyCompressionCodec\")\n",
    "    .getOrCreate()\n",
    "    .read.format(\"parquet\")\n",
    "    .load(f\"{os.getcwd()}/emp_attrition.parquet.snappy\")\n",
    ")\n",
    "\n",
    "read_snappy_df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: other compression formats Data Flow supports today include snappy parquet (example above) and gzip on both csv and parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have come to a query that we want to run in Data Flow from previous explorations. Please refer to the dataflow.ipynb on how to submit a job to dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_base_folder = tempfile.mkdtemp()\n",
    "data_flow = DataFlow(dataflow_base_folder=dataflow_base_folder)\n",
    "print(\"Data flow directory: {}\".format(dataflow_base_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_file_path = path.join(\n",
    "    dataflow_base_folder, \"example-{}.py\".format(str(uuid.uuid4())[-6:])\n",
    ")\n",
    "script = '''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Create a Spark session\n",
    "    spark = SparkSession \\\\\n",
    "        .builder \\\\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Load a csv file from dataflow public storage\n",
    "    df = spark \\\\\n",
    "        .read \\\\\n",
    "        .format(\"csv\") \\\\\n",
    "        .option(\"header\", \"true\") \\\\\n",
    "        .option(\"multiLine\", \"true\") \\\\\n",
    "        .load(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/synthetic/orcl_attrition.csv\")\n",
    "    \n",
    "    # Create a temp view and do some SQL operations\n",
    "    df.createOrReplaceTempView(\"emp_attrition\")\n",
    "    query_result_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            Age,\n",
    "            MonthlyIncome,\n",
    "            YearsInIndustry\n",
    "        FROM emp_attrition \n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert the filtered Apache Spark DataFrame into JSON format\n",
    "    # Note: we are writing to the Spark stdout log so that we can retrieve the log later at the end of the notebook.\n",
    "    print('\\\\n'.join(query_result_df.toJSON().collect()))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open(pyspark_file_path, \"w\") as f:\n",
    "    print(script.strip(), file=f)\n",
    "\n",
    "print(\"Script path: {}\".format(pyspark_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_bucket = \"test\"  # Update the value\n",
    "logs_bucket = \"dataflow-log\"  # Update the value\n",
    "display_name = \"sample_Data_Flow_app\"\n",
    "\n",
    "app_config = data_flow.prepare_app(\n",
    "    display_name=display_name,\n",
    "    script_bucket=script_bucket,\n",
    "    pyspark_file_path=pyspark_file_path,\n",
    "    logs_bucket=logs_bucket,\n",
    ")\n",
    "\n",
    "app = data_flow.create_app(app_config)\n",
    "\n",
    "run_display_name = \"sample_Data_Flow_run\"\n",
    "run_config = app.prepare_run(run_display_name=run_display_name)\n",
    "\n",
    "run = app.run(run_config, save_log_to_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.oci_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "# References\n",
    "\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
