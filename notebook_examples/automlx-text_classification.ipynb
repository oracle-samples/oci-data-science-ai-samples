{
 "cells": [
  {
   "cell_type": "raw",
   "id": "57470047",
   "metadata": {},
   "source": [
    "@notebook{automlx-text_classification.ipynb,\n",
    "    title: Building and Explaining a Text Classifier using AutoMLx,\n",
    "    summary: build a classifier using the Oracle AutoMLx tool for the public 20newsgroup dataset,\n",
    "    developed on: automlx_p38_cpu_v2,\n",
    "    keywords: automlx, text classification, text classifier,\n",
    "    license: Universal Permissive License v 1.0.,\n",
    "    original source: https://github.com/oracle-samples/automlx/blob/main/demos/OracleAutoMLx_Classification_Text.ipynb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3cf50",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=red>Building and Explaining a Text Classifier using AutoMLx</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle AutoMLx Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1272ea64",
   "metadata": {},
   "source": [
    "AutoMLx Text Classification Demo version 23.1.1.\n",
    "\n",
    "Copyright (c) 2023 Oracle, Inc.  \n",
    "\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36b3df",
   "metadata": {},
   "source": [
    "## Overview of this Notebook\n",
    "\n",
    "In this notebook we will build a classifier using the Oracle AutoMLx tool for the public 20newsgroup dataset. The dataset is a binary classification dataset, and more details about the dataset can be found at http://qwone.com/~jason/20Newsgroups/.\n",
    "We explore the various options provided by the Oracle AutoMLx tool, allowing the user to exercise control over the AutoML training process. We then evaluate the different models trained by AutoML. Finally we provide an overview of the possibilites that Oracle AutoMLx offers for explaining the predictions of the tuned model.\n",
    "\n",
    "---\n",
    "## Prerequisites\n",
    "\n",
    "  - Experience level: Novice (Python and Machine Learning)\n",
    "  - Professional experience: Some industry experience\n",
    "\n",
    "Compatible conda pack: [Oracle AutoML and Model Explanation for Python 3.8 (version 2.0)](oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/Oracle_AutoML_and_Model_Explanation_for_Python_3.8/2.0/automlx_p38_cpu_v2)\n",
    "\n",
    "---\n",
    "\n",
    "## Business Use\n",
    "\n",
    "Data analytics and modeling problems using Machine Learning (ML) are becoming popular and often rely on data science expertise to build accurate ML models. Such modeling tasks primarily involve the following steps:\n",
    "- Preprocess dataset (clean, impute, engineer features, normalize).\n",
    "- Pick an appropriate model for the given dataset and prediction task at hand.\n",
    "- Tune the chosen modelâ€™s hyperparameters for the given dataset.\n",
    "\n",
    "All of these steps are significantly time consuming and heavily rely on data scientist expertise. Unfortunately, to make this problem harder, the best feature subset, model, and hyperparameter choice widely varies with the dataset and the prediction task. Hence, there is no one-size-fits-all solution to achieve reasonably good model performance. Using a simple Python API, AutoML can quickly (faster) jump-start the datascience process with an accurately-tuned model and appropriate features for a given prediction task.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- <a href='#setup'>0. Setup</a>\n",
    "- <a href='#load-data'>1. Load the 20newsgroup Income dataset</a>\n",
    "- <a href='#AutoML'>2. AutoML</a>\n",
    "  - <a href='#Engine'>2.0. Set the engine</a>\n",
    "  - <a href='#provider'>2.1. Create an Instance of Oracle AutoMLx</a>\n",
    "  - <a href='#default'>2.2. Train a Model using AutoML</a>\n",
    "  - <a href='#analyze'>2.3. Analyze the AutoML optimization process </a>\n",
    "      - <a href='#algorithm-selection'>2.3.1. Algorithm Selection</a>\n",
    "      - <a href='#adaptive-sampling'>2.3.2. Adaptive Sampling</a>\n",
    "      - <a href='#feature-selection'>2.3.3. Feature Selection</a>\n",
    "      - <a href='#hyperparameter-tuning'>2.3.4. Hyperparameter Tuning</a>\n",
    "  - <a href='#timebudget'>2.4. Specify a time budget to AutoML</a>\n",
    "  - <a href='#scoringfn'>2.5. Specify a different scoring metric to AutoML</a>\n",
    "- <a href='#MLX'>3. Machine Learning Explainability (MLX)</a>\n",
    "  - <a href='#MLX-initialization'> 3.1. Initialize an MLExplainer</a>\n",
    "  - <a href='#MLX-global'> 3.2. Global Token Importance</a>\n",
    "  - <a href='#MLX-local'> 3.3. Local Token Importance</a>\n",
    "- <a href='#ref'>References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbc027",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup\n",
    "\n",
    "Basic setup for the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16dd3e",
   "metadata": {},
   "source": [
    "Load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Settings for plots\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "plt.rcParams['font.size'] = 15\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_palette(\"bright\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import automl\n",
    "from automl import init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3b18a",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "## Load the 20 News Group dataset\n",
    "We start by reading in the dataset from sklearn. The dataset has already been pre-split into training and test sets. The training set will be used to create a Machine Learning model using AutoML, and the test set will be used to evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1665358",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "target_names = train.target_names\n",
    "\n",
    "X_train, y_train = pd.DataFrame(train.data), pd.DataFrame(train.target)\n",
    "X_test, y_test = pd.DataFrame(test.data), pd.DataFrame(test.target)\n",
    "\n",
    "column_names = [\"Message\"]\n",
    "X_train.columns = column_names\n",
    "X_test.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bab65a",
   "metadata": {},
   "source": [
    "Lets look at a few of the values in the data. 20 NewsGroup is a classification dataset made of text samples. Each sample has an associated class (also called topic), which can be one of the followings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25a94f",
   "metadata": {},
   "source": [
    "We display some examples of data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ccad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c3c0b",
   "metadata": {},
   "source": [
    "We restrict the data to the 4 topics that are related to science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6591bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_labels = [i for i in range(len(target_names)) if 'sci' in target_names[i]]\n",
    "train_science_indices = [i for i in range(len(y_train)) if y_train.iloc[i][0] in science_labels]\n",
    "test_science_indices = [i for i in range(len(y_test)) if y_test.iloc[i][0] in science_labels]\n",
    "\n",
    "X_train = X_train.iloc[train_science_indices]\n",
    "y_train = y_train.iloc[train_science_indices]\n",
    "X_test = X_test.iloc[test_science_indices]\n",
    "y_test = y_test.iloc[test_science_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3317ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebfae39",
   "metadata": {},
   "source": [
    "We further downsample the train set to have a reasonable training time for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _, y_train, _ = train_test_split(X_train, y_train,\n",
    "                                          test_size=0.5,\n",
    "                                          stratify=y_train,\n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d62f25",
   "metadata": {},
   "source": [
    "Finally we generate a validation set and only use that for internal pipeline validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea47f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                      test_size=0.2,\n",
    "                                                      stratify=y_train,\n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6023f68d",
   "metadata": {},
   "source": [
    "<a id='AutoML'></a>\n",
    "## AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6fb9cf",
   "metadata": {},
   "source": [
    "<a id='Engine'></a>\n",
    "### Setting the engine\n",
    "The AutoML pipeline offers the function `init`, which allows to initialize the parallel engine. By default, the AutoML pipeline uses the `dask` parallel engine. One can also set the engine to `local`, which uses python's multiprocessing library for parallelism instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "init(engine='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dd94f",
   "metadata": {},
   "source": [
    "<a id='provider'></a>\n",
    "### Create an instance of Oracle AutoMLx\n",
    "\n",
    "The Oracle AutoMLx solution provides a pipeline that automatically finds a tuned model given a prediction task and a training dataset. In particular it allows to find a tuned model for any supervised prediction task, e.g. classification or regression where the target can be binary, categorical or real-valued.\n",
    "\n",
    "AutoML consists of five main modules:\n",
    "- **Preprocessing** (Feature extraction and selection) : The pipeline extracts tabular features using [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) that it then selects between.\n",
    "- **Algorithm Selection** : Identify the right classification algorithm -in this notebook- for a given dataset, choosing from amongst:\n",
    "    - AdaBoostClassifier\n",
    "    - DecisionTreeClassifier\n",
    "    - ExtraTreesClassifier\n",
    "    - TorchMLPClassifier\n",
    "    - KNeighborsClassifier\n",
    "    - LGBMClassifier\n",
    "    - LinearSVC\n",
    "    - LogisticRegression\n",
    "    - RandomForestClassifier\n",
    "    - SVC\n",
    "    - XGBClassifier\n",
    "    - GaussianNB\n",
    "- **Adaptive Sampling** : Select a subset of the data samples for the model to be trained on.\n",
    "- **Feature Selection** : Select a subset of the features (extracted with TFIDF), based on the previously selected model. In the rest of this notebook, we will implicitly refer to the extracted features as data features.\n",
    "- **Hyperparameter Tuning** : Find the right model parameters that maximize score for the given dataset.\n",
    "\n",
    "All these pieces are readily combined into a simple AutoML pipeline which automates the entire Machine Learning process with minimal user input/interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364355e8",
   "metadata": {},
   "source": [
    "<a id='default'></a>\n",
    "### Train a model using AutoML\n",
    "\n",
    "The AutoML API is quite simple to work with. We create an instance of the pipeline. Next, the training data is passed to the `fit()` function which executes the three previously mentioned steps.\n",
    "\n",
    "A model is then generated and can be used for prediction tasks. We use the roc_auc scoring metric to evaluate the performance of this model on unseen data (`X_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23045380",
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = automl.Pipeline(task='classification')\n",
    "est1.fit(X_train, y_train, X_valid, y_valid, cv=None, col_types=['text'])\n",
    "\n",
    "y_predict = est1.predict(X_test)\n",
    "score_default = f1_score(y_test, y_predict, average=\"micro\")\n",
    "\n",
    "print('F1 Micro Score on test data: {:3.3f}'.format(score_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35327b57",
   "metadata": {},
   "source": [
    "<a id='analyze'></a>\n",
    "### Analyze the AutoML optimization process\n",
    "\n",
    "During the AutoML process, a summary of the optimization process is logged. It consists of:\n",
    "- Information about the training data\n",
    "- Information about the AutoML Pipeline, such as:\n",
    "    - selected features that AutoML found to be most predictive in the training data;\n",
    "    - selected algorithm that was the best choice for this data;\n",
    "    - hyperparameters for the selected algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aabb9d2",
   "metadata": {},
   "source": [
    "AutoML provides a print_summary API to output all the different trials performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "est1.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622d3a3",
   "metadata": {},
   "source": [
    "We also provide the capability to visualize the results of each stage of the AutoML pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e5035",
   "metadata": {},
   "source": [
    "<a id='algorithm-selection'></a>\n",
    "#### Algorithm Selection\n",
    "\n",
    "The plot below shows the scores predicted by Algorithm Selection for each algorithm. The horizontal line shows the average score across all algorithms. Algorithms below the line are colored turquoise, whereas those with a score higher than the mean are colored teal. Here we can see that the `TorchMLPClassifier` achieved the highest predicted score (orange bar), and is chosen for subsequent stages of the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62648003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trial is a tuple of\n",
    "# (algorithm, no. samples, no. features, mean CV score, hyperparameters, \n",
    "# all CV scores, total CV time (s), memory usage (Gb))\n",
    "trials = est1.model_selection_trials_\n",
    "colors = []\n",
    "scores = [x[3] for x in trials]\n",
    "models = [x[0] for x in trials]\n",
    "y_margin = 0.10 * (max(scores) - min(scores))\n",
    "s = pd.Series(scores, index=models).sort_values(ascending=False)\n",
    "\n",
    "for f in s.keys():\n",
    "    if f == '{}_AS'.format(est1.selected_model_):\n",
    "        colors.append('orange')\n",
    "    elif s[f] >= s.mean():\n",
    "        colors.append('teal')\n",
    "    else:\n",
    "        colors.append('turquoise')\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Algorithm Selection Trials\")\n",
    "ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "ax.set_ylabel(est1.inferred_score_metric[0])\n",
    "s.plot.bar(ax=ax, color=colors, edgecolor='black')\n",
    "ax.axhline(y=s.mean(), color='black', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3f6c3",
   "metadata": {},
   "source": [
    "<a id='adaptive-sampling'></a>\n",
    "#### Adaptive Sampling\n",
    "\n",
    "Following Algorithm Selection, Adaptive Sampling aims to find the smallest dataset sample that can be created without compromising validation set score for the chosen model. Given the small size of the training data (948 samples), Adaptive Sampling is not relevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trial is a tuple of\n",
    "# (algorithm, no. samples, no. features, mean CV score, hyperparameters, \n",
    "# all CV scores, total CV time (s), memory usage (Gb))\n",
    "trials = est1.adaptive_sampling_trials_\n",
    "scores = [x[3] for x in trials]\n",
    "n_samples = [x[1] for x in trials]\n",
    "y_margin = 0.10 * (max(scores) - min(scores))\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Adaptive Sampling ({})\".format(trials[0][0]))\n",
    "ax.set_xlabel('Dataset sample size')\n",
    "ax.set_ylabel(est1.inferred_score_metric[0])\n",
    "ax.grid(color='g', linestyle='-', linewidth=0.1)\n",
    "ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "ax.plot(n_samples, scores, 'k:', marker=\"s\", color='teal', markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11ef9e",
   "metadata": {},
   "source": [
    "<a id='feature-selection'></a>\n",
    "#### Feature Selection\n",
    "After finding a sample subset, the next step is to find a relevant feature subset to maximize score for the chosen algorithm. The Feature Selection step identifies the smallest feature subset that does not compromise on the score of the chosen algorithm. The orange line shows the optimal number of features chosen by Feature Selection (in this case, retaining only about 4 000 of the more than 11 000 available words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trial is a tuple of\n",
    "# (algorithm, no. samples, no. features, mean CV score, hyperparameters, \n",
    "# all CV scores, total CV time (s), memory usage (Gb))\n",
    "trials = est1.feature_selection_trials_\n",
    "scores = [x[3] for x in trials]\n",
    "n_features = [x[2] for x in trials]\n",
    "y_margin = 0.10 * (max(scores) - min(scores))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Feature Selection Trials\")\n",
    "ax.set_xlabel(\"Number of Features\")\n",
    "ax.set_ylabel(est1.inferred_score_metric[0])\n",
    "ax.grid(color='g', linestyle='-', linewidth=0.1)\n",
    "ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "ax.plot(n_features, scores, 'k:', marker=\"s\", color='teal', markersize=3)\n",
    "ax.axvline(x=len(est1.selected_features_names_), color='orange', linewidth=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112b342",
   "metadata": {},
   "source": [
    "<a id='hyperparameter-tuning'></a>\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter Tuning is the last stage of the AutoML pipeline, and focuses on improving the chosen algorithm's score on the reduced dataset (after Adaptive Sampling and Feature Selection). We use a novel algorithm to search across many hyperparameters dimensions, and converge automatically when optimal hyperparameters are identified. Each trial in the graph below represents a particular hyperparameters configuration for the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24574b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trial is a tuple of\n",
    "# (algorithm, no. samples, no. features, mean CV score, hyperparameters, \n",
    "# all CV scores, total CV time (s), memory usage (Gb))\n",
    "trials = est1.tuning_trials_\n",
    "scores = [x[3] for x in reversed(trials)]\n",
    "y_margin = 0.10 * (max(scores) - min(scores))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Hyperparameter Tuning Trials\")\n",
    "ax.set_xlabel(\"Iteration $n$\")\n",
    "ax.set_ylabel(est1.inferred_score_metric[0])\n",
    "ax.grid(color='g', linestyle='-', linewidth=0.1)\n",
    "ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "ax.plot(range(1, len(trials) + 1), scores, 'k:', marker=\"s\", color='teal', markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393dc89",
   "metadata": {},
   "source": [
    "<a id='timebudget'></a>\n",
    "### Specify a time budget to AutoML\n",
    "The Oracle AutoMLx tool also supports a user given time budget in seconds. Given the small size of this dataset, we give a small `time budget` of 10 seconds using the time_budget argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83734f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "est2 = automl.Pipeline(task=\"classification\")\n",
    "est2.fit(X_train, y_train, X_valid, y_valid, cv=None, col_types=['text'], time_budget=10)\n",
    "\n",
    "y_predict = est2.predict(X_test)\n",
    "score_default = f1_score(y_test, y_predict, average=\"micro\")\n",
    "\n",
    "print('F1 micro Score on test data: {:3.3f}'.format(score_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf4596",
   "metadata": {},
   "source": [
    "<a id='scoringstr'></a>\n",
    "### Specify a different scoring metric to AutoML\n",
    "By default, the score metric is set to `neg_log_loss` for classifcation and `neg_mean_squared_error` for regression.\n",
    "\n",
    "The user can also choose another scoring metric. The list of possible metrics is given by:\n",
    "- For binary classification, one of 'roc_auc', 'accuracy', 'f1', 'precision', 'recall', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'recall_micro', 'recall_macro', 'recall_weighted', 'recall_samples', 'precision_micro', 'precision_macro', 'precision_weighted', 'precision_samples'\n",
    "- For multiclass classification , one of  'neg_log_loss', 'recall_macro', 'accuracy','f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'recall_micro', 'recall_weighted', 'recall_samples', 'precision_micro', 'precision_macro', 'precision_weighted', 'precision_samples'\n",
    "- For regression, one of 'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error'\n",
    "\n",
    "Here, we ask AutoML to optimize for the 'f1_micro' scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f1cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "est3 = automl.Pipeline(task=\"classification\", score_metric='f1_micro')\n",
    "est3.fit(X_train, y_train, X_valid, y_valid, cv=None, col_types=['text'], time_budget=60)\n",
    "\n",
    "y_predict = est3.predict(X_test)\n",
    "score_default = f1_score(y_test, y_predict, average=\"micro\")\n",
    "\n",
    "print('F1 micro Score on test data: {:3.3f}'.format(score_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd9e4c",
   "metadata": {},
   "source": [
    "<a id='MLX'></a>\n",
    "## Machine Learning Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221c6bd",
   "metadata": {},
   "source": [
    "For a variety of decision-making tasks, getting only a prediction as model output is not sufficient. A user may wish to know why the model outputs that prediction, or which data features are relevant for that prediction. For that purpose the Oracle AutoMLx solution defines the MLExplainer object, which allows to compute a variety of model explanations\n",
    "\n",
    "<a id='MLX-initializing'></a>\n",
    "### Initializing an MLExplainer\n",
    "\n",
    "The MLExplainer object takes as argument the trained model, the training data and labels, as well as the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = automl.MLExplainer(est1,\n",
    "                               X_train,\n",
    "                               y_train,\n",
    "                               target_names=['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'],\n",
    "                               task=\"classification\",\n",
    "                               col_types=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42403b36",
   "metadata": {},
   "source": [
    "<a id='MLX-global'></a>\n",
    "### Model Explanations (Global Token Importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a666b0",
   "metadata": {},
   "source": [
    "For text classification tasks, since we first extract tokens (features), the Oracle AutoMLx solution offers a single way to compute a notion of token importance: Global Token Importance. The notion of Global Token Importance intuitively measures how much a token impacts the model's predictions (relative to the provided train labels). This notion of token importance considers each token independently from all other tokens. Tokens are the most fine-grained building blocks of the NLP model, such as sentences, words, or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e442cf4",
   "metadata": {},
   "source": [
    "#### Computing the importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cdad93",
   "metadata": {},
   "source": [
    "We use a permutation-based method to successively measure the importance of each token (feature). Such a method therefore runs in linear time with respect to the\n",
    "number of features (tokens) in the dataset. \n",
    "\n",
    "The method `explain_model()` allows to compute such feature importances. It also provides 95% confidence intervals for each token importance attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd16443",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_explain_model_default = explainer.explain_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98664b71",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010578a5",
   "metadata": {},
   "source": [
    "There are two options to show the explanation's results:\n",
    "- `to_dataframe()` will return a dataframe of the results.\n",
    "- `show_in_notebook()` will show the results as a bar plot.\n",
    "\n",
    "The features are returned in decreasing order of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_explain_model_default.to_dataframe(n_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5b1ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "result_explain_model_default.show_in_notebook(n_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82c56f",
   "metadata": {},
   "source": [
    "<a id='MLX-local'></a>\n",
    "## Local Token importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c0e6d",
   "metadata": {},
   "source": [
    "For text classification tasks, since we first extract tokens (features), the Oracle AutoMLx solution offers a single way to compute a notion of token importance: Local Token Importance. The notion of Local Token Importance intuitively measures how much a token impacts an instance's predictions (relative to the provided train labels). This notion of token importance considers each token independently from all other tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818489f",
   "metadata": {},
   "source": [
    "#### Compute the importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768e92f",
   "metadata": {},
   "source": [
    "By default we use a surrogate method to successively measure the importance of each token in a given instance. Such a method therefore runs in linear time with respect to the number of features in the dataset.\n",
    "\n",
    "The method `explain_prediction()` allows to compute such feature importances. It also provides 95% confidence intervals for each feature importance attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac125bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "result_explain_prediction_default = explainer.explain_prediction(X_train.iloc[index:index + 1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4815747",
   "metadata": {},
   "source": [
    "There are two options to show the explanation's results:\n",
    " - `to_dataframe()` will return a dataframe of the results.\n",
    " - `show_in_notebook()` will show the results as a bar plot.\n",
    "\n",
    "The features are returned in decreasing order of importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c4850",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_explain_prediction_default[0].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf7b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_explain_prediction_default[0].show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b001e",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "## References\n",
    "* More examples and details: http://automl.oraclecorp.com/\n",
    "* Oracle AutoML http://www.vldb.org/pvldb/vol13/p3166-yakovlev.pdf\n",
    "* scikit-learn https://scikit-learn.org/stable/\n",
    "* Interpretable Machine Learning https://christophm.github.io/interpretable-ml-book/\n",
    "* LIME https://arxiv.org/pdf/1602.04938\n",
    "* 20newsgroup http://qwone.com/~jason/20Newsgroups/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:automlx_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-automlx_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
