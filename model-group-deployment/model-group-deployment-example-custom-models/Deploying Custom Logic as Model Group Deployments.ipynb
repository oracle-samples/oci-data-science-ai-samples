{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22eb3d02",
   "metadata": {},
   "source": [
    "# Business Logic Deployments via MMS on OCI Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c35ff",
   "metadata": {},
   "source": [
    "This notebook walks through how to:\n",
    "- create a model version set, used for version control of models in the model catalog\n",
    "- create a model entry for a model using business logic (i.e. not using an ML model)\n",
    "- create a single model deployment for a model using business logic\n",
    "- create a model group for logic based models\n",
    "- create a model group artifact for logic based models\n",
    "- create a model group history for model groups, this helps with version control and cataloging\n",
    "- create a model deployment for a model group\n",
    "- update a model group history\n",
    "- perform a live update of a model group deployment\n",
    "\n",
    "Whilst this example is simple, it can be used as a foundation for automating deployment and re-deployment of logic based model deployments via multi-model serving on OCI Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaa80d",
   "metadata": {},
   "source": [
    "A note on directory structure...\n",
    "\n",
    "When creating the artifacts for both the model catalog, and the model group artifact bare in mind the directory structure for the multi-model serving will need to look like this:\n",
    "\n",
    "```model-group\n",
    "    runtime.yaml -- for the group\n",
    "    score.py -- for the group\n",
    "    model1.py -- artifact for model 1\n",
    "    model2.py -- artifact for model 2\n",
    "        model1\n",
    "            runtime.yaml\n",
    "            score.py\n",
    "            model.pickle\n",
    "            model1.py\n",
    "        model2\n",
    "            runtime.yaml\n",
    "            score.py\n",
    "            model.pickle\n",
    "            model2.py\n",
    "```\n",
    "We need to add model1.py and model2.py to the model_group.zip file because we're adding custom method imports to our model. It may be that you do not need to add model1.py and model2.py to the individual models for MMS, but for single-model deployments you do need to so it is worth adding the file to the zip and model catalog even if there is some redundancy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f3a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/var/folders/s8/6gv5fnrj3fj79z3d6sxk21nc0000gn/T/ipykernel_18316/229808084.py:6: DeprecationWarning: The `ads.common.model_metadata` is deprecated in `oracle-ads 2.6.8` and will be removed in future release. Use the `ads.model.model_metadata` instead.\n",
      "  from ads.common.model_metadata import UseCaseType\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import oci\n",
    "import ads\n",
    "import tempfile\n",
    "from ads.model import GenericModel\n",
    "from ads.model import ModelVersionSet\n",
    "from ads.common.model_metadata import UseCaseType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4f159",
   "metadata": {},
   "source": [
    "### 0: Create Model Version Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d70003",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_ocid = 'ocid1.datascienceproject...'\n",
    "compartment_ocid = 'ocid1.compartment...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553952b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model version set\n",
    "mvs1 = ModelVersionSet(\n",
    "    name = \"business-model-live-1\",\n",
    "    description = \"business model mvs1\")\n",
    "mvs1.with_compartment_id(compartment_ocid).with_project_id(project_ocid).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4160cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model version set\n",
    "mvs2 = ModelVersionSet(\n",
    "    name = \"business-model-live-2\",\n",
    "    description = \"business model mvs2\")\n",
    "mvs2.with_compartment_id(compartment_ocid).with_project_id(project_ocid).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c514f8",
   "metadata": {},
   "source": [
    "### 1: Deploy Custom Logic as Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851bcbe",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3543fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model1 import model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7e79df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ads.common:In the future model input will be serialized by `cloudpickle` by default. Currently, model input are serialized into a dictionary containing serialized input data and original data type information.Set `model_input_serializer=\"cloudpickle\"` to use cloudpickle model input serializer.\n"
     ]
    }
   ],
   "source": [
    "# basic model to square a given number\n",
    "\n",
    "generic_model1 = GenericModel(artifact_dir='model1_live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bc2a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/.venv/lib/python3.13/site-packages/ads/model/runtime/env_info.py:92: UserWarning: slug will be deprecated. Provide conda pack path instead.                                                      ?, ?it/s]\n",
      "  warnings.warn(\"slug will be deprecated. Provide conda pack path instead.\")\n",
      "\n",
      "ERROR:root:Error occurred in attempt to extract the list of the service conda environments from the object storage for bucket 'service-conda-packs' and namespace 'id19sfcrra6z'. Please make sure that you've provided correct bucket and namespace.\n",
      "INFO:ADS:To auto-extract taxonomy metadata the model must be provided. Supported models: keras, lightgbm, pytorch, sklearn, tensorflow, pyspark, and xgboost.     ?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "algorithm: null\n",
       "artifact_dir:\n",
       "  /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model1_live:\n",
       "  - - .model-ignore\n",
       "    - runtime.yaml\n",
       "    - model.pickle\n",
       "    - test_json_output.json\n",
       "    - model1.py\n",
       "    - score.py\n",
       "framework: null\n",
       "model_deployment_id: null\n",
       "model_id: null"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model1.prepare(inference_conda_env='generalml_p311_cpu_x86_64_v1',\n",
    "                      model_file_name='score.py',\n",
    "                      score_py_uri='custom_score_v2.py',\n",
    "                      inference_python_version='3.11',force_overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d17a1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model()\n",
    "\n",
    "with open('model1_live/model.pickle','wb') as f:\n",
    "    pickle.dump(m,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e8f87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model1_live ...\n",
      "Model is successfully loaded.\n",
      "trying to predict using pickle\n",
      "data received by predict function\n",
      "input number is 5\n",
      "yhat is currently 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 25}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model1.verify({'number':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1c8a867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Actions Needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Details</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>initiate</th>\n",
       "      <th>Done</th>\n",
       "      <th>Initiated the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">prepare()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Done</th>\n",
       "      <th>Generated runtime.yaml</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Available</th>\n",
       "      <th>Serialized model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Done</th>\n",
       "      <th>Populated metadata(Custom, Taxonomy and Provenance)</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verify()</th>\n",
       "      <th>Done</th>\n",
       "      <th>Local tested .predict from score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">save()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Available</th>\n",
       "      <th>Conducted Introspect Test</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uploaded artifact to model catalog</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deploy()</th>\n",
       "      <th>UNKNOWN</th>\n",
       "      <th>Deployed the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict()</th>\n",
       "      <th>Not Available</th>\n",
       "      <th>Called deployment predict endpoint</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Actions Needed\n",
       "Step      Status        Details                                                           \n",
       "initiate  Done          Initiated the model                                               \n",
       "prepare() Done          Generated runtime.yaml                                            \n",
       "                        Generated score.py                                                \n",
       "          Available     Serialized model                                                  \n",
       "          Done          Populated metadata(Custom, Taxonomy and Provenance)               \n",
       "verify()  Done          Local tested .predict from score.py                               \n",
       "save()    Available     Conducted Introspect Test                                         \n",
       "                        Uploaded artifact to model catalog                                \n",
       "deploy()  UNKNOWN       Deployed the model                                                \n",
       "predict() Not Available Called deployment predict endpoint                                "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model1.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f084da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model1_live/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile model1_live/runtime.yaml\n",
    "\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/General_Machine_Learning_for_CPUs_on_Python_3.11/1.0/generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_SLUG: generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_TYPE: data_science\n",
    "    INFERENCE_PYTHON_VERSION: '3.11'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ''\n",
    "  TENANCY_OCID: ''\n",
    "  TRAINING_CODE:\n",
    "    ARTIFACT_DIRECTORY: ''\n",
    "  TRAINING_COMPARTMENT_OCID: ''\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: ''\n",
    "    TRAINING_ENV_SLUG: ''\n",
    "    TRAINING_ENV_TYPE: ''\n",
    "    TRAINING_PYTHON_VERSION: ''\n",
    "  TRAINING_REGION: ''\n",
    "  TRAINING_RESOURCE_OCID: ''\n",
    "  USER_OCID: ''\n",
    "  VM_IMAGE_INTERNAL_ID: ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "502709f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_model1.reload_runtime_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db6ecac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model1_live ...\n",
      "Model is successfully loaded.\n",
      "['.model-ignore', 'runtime.yaml', 'model.pickle', 'test_json_output.json', 'model1.py', 'score.py']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a31843b15943b5abb2bd8cedeee973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model and add it to the model version set\n",
    "model_id = generic_model1.save(compartment_id=compartment_ocid,\n",
    "    project_id=project_ocid,\n",
    "    display_name=\"Business Model 1\",\n",
    "    model_version_set=mvs1,\n",
    "    version_label=\"Version 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35c335ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model1_live ...\n",
      "Model is successfully loaded.\n",
      "trying to predict using pickle\n",
      "data received by predict function\n",
      "input number is 5\n",
      "yhat is currently 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 25}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model1.predict({'number':5},local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9974d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_mdl = generic_model1.deploy(display_name='Business Model 1',deployment_log_group_id='ocid1.loggroup...',deployment_access_log_id='ocid1.log...',deployment_predict_log_id='ocid1.log...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a2949f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 25}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model1.predict({'number':5},local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f44348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 25}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call model directly using OCI\n",
    "import requests\n",
    "from oci.signer import Signer\n",
    "\n",
    "config = oci.config.from_file(\"~/.oci/config\") \n",
    "auth = Signer(\n",
    "  tenancy=config['tenancy'],\n",
    "  user=config['user'],\n",
    "  fingerprint=config['fingerprint'],\n",
    "  private_key_file_location=config['key_file'],\n",
    "  pass_phrase=config['pass_phrase'])\n",
    "\n",
    "endpoint = single_mdl.url+'/predict'\n",
    "body = {'number':5}\n",
    "headers = {} \n",
    "requests.post(endpoint, json=body, auth=auth, headers=headers).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea8da2",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60b4c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model2 import model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ae69e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ads.common:In the future model input will be serialized by `cloudpickle` by default. Currently, model input are serialized into a dictionary containing serialized input data and original data type information.Set `model_input_serializer=\"cloudpickle\"` to use cloudpickle model input serializer.\n"
     ]
    }
   ],
   "source": [
    "# basic model to square a given number\n",
    "\n",
    "generic_model2 = GenericModel(artifact_dir='model2_live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7872ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/.venv/lib/python3.13/site-packages/ads/model/runtime/env_info.py:92: UserWarning: slug will be deprecated. Provide conda pack path instead.                                                      ?, ?it/s]\n",
      "  warnings.warn(\"slug will be deprecated. Provide conda pack path instead.\")\n",
      "\n",
      "ERROR:root:Error occurred in attempt to extract the list of the service conda environments from the object storage for bucket 'service-conda-packs' and namespace 'id19sfcrra6z'. Please make sure that you've provided correct bucket and namespace.\n",
      "INFO:ADS:To auto-extract taxonomy metadata the model must be provided. Supported models: keras, lightgbm, pytorch, sklearn, tensorflow, pyspark, and xgboost.     ?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "algorithm: null\n",
       "artifact_dir:\n",
       "  /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model2_live:\n",
       "  - - .model-ignore\n",
       "    - runtime.yaml\n",
       "    - score.py\n",
       "framework: null\n",
       "model_deployment_id: null\n",
       "model_id: null"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model2.prepare(inference_conda_env='generalml_p311_cpu_x86_64_v1',\n",
    "                      model_file_name='score.py',\n",
    "                     score_py_uri='custom_score_v2.py',\n",
    "                      inference_python_version='3.11',force_overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82e66348",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model()\n",
    "\n",
    "with open('model2_live/model.pickle','wb') as f:\n",
    "    pickle.dump(m,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b384bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model2_live ...\n",
      "Model is successfully loaded.\n",
      "trying to predict using pickle\n",
      "data received by predict function\n",
      "input number is 9\n",
      "yhat is currently 3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model2.verify({'number':9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "590c63a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Actions Needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Details</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>initiate</th>\n",
       "      <th>Done</th>\n",
       "      <th>Initiated the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">prepare()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Done</th>\n",
       "      <th>Generated runtime.yaml</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Available</th>\n",
       "      <th>Serialized model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Done</th>\n",
       "      <th>Populated metadata(Custom, Taxonomy and Provenance)</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verify()</th>\n",
       "      <th>Done</th>\n",
       "      <th>Local tested .predict from score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">save()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Available</th>\n",
       "      <th>Conducted Introspect Test</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uploaded artifact to model catalog</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deploy()</th>\n",
       "      <th>UNKNOWN</th>\n",
       "      <th>Deployed the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict()</th>\n",
       "      <th>Not Available</th>\n",
       "      <th>Called deployment predict endpoint</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Actions Needed\n",
       "Step      Status        Details                                                           \n",
       "initiate  Done          Initiated the model                                               \n",
       "prepare() Done          Generated runtime.yaml                                            \n",
       "                        Generated score.py                                                \n",
       "          Available     Serialized model                                                  \n",
       "          Done          Populated metadata(Custom, Taxonomy and Provenance)               \n",
       "verify()  Done          Local tested .predict from score.py                               \n",
       "save()    Available     Conducted Introspect Test                                         \n",
       "                        Uploaded artifact to model catalog                                \n",
       "deploy()  UNKNOWN       Deployed the model                                                \n",
       "predict() Not Available Called deployment predict endpoint                                "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model2.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3e885a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model2_live/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile model2_live/runtime.yaml\n",
    "\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/General_Machine_Learning_for_CPUs_on_Python_3.11/1.0/generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_SLUG: generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_TYPE: data_science\n",
    "    INFERENCE_PYTHON_VERSION: '3.11'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ''\n",
    "  TENANCY_OCID: ''\n",
    "  TRAINING_CODE:\n",
    "    ARTIFACT_DIRECTORY: ''\n",
    "  TRAINING_COMPARTMENT_OCID: ''\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: ''\n",
    "    TRAINING_ENV_SLUG: ''\n",
    "    TRAINING_ENV_TYPE: ''\n",
    "    TRAINING_PYTHON_VERSION: ''\n",
    "  TRAINING_REGION: ''\n",
    "  TRAINING_RESOURCE_OCID: ''\n",
    "  USER_OCID: ''\n",
    "  VM_IMAGE_INTERNAL_ID: ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5c026e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_model2.reload_runtime_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e05564bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model2_live ...\n",
      "Model is successfully loaded.\n",
      "['.model-ignore', 'runtime.yaml', 'model.pickle', 'model2.py', 'score.py']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e403e7177e4e91898ae5c9a7fccaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the model and add it to the model version set\n",
    "model_id = generic_model2.save(compartment_id=compartment_ocid,\n",
    "    project_id=project_ocid,\n",
    "    display_name=\"Business Model 2\",\n",
    "    model_version_set=mvs2,\n",
    "    version_label=\"Version 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb5b2df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model2_live ...\n",
      "Model is successfully loaded.\n",
      "trying to predict using pickle\n",
      "data received by predict function\n",
      "input number is 9\n",
      "yhat is currently 3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 3.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model2.predict({'number':9},local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c2354",
   "metadata": {},
   "source": [
    "### 3: Create Model Group Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4356f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_group_live/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_group_live/runtime.yaml\n",
    "\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/General_Machine_Learning_for_CPUs_on_Python_3.11/1.0/generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_SLUG: generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_TYPE: data_science\n",
    "    INFERENCE_PYTHON_VERSION: '3.11'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ''\n",
    "  TENANCY_OCID: ''\n",
    "  TRAINING_CODE:\n",
    "    ARTIFACT_DIRECTORY: ''\n",
    "  TRAINING_COMPARTMENT_OCID: ''\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: ''\n",
    "    TRAINING_ENV_SLUG: ''\n",
    "    TRAINING_ENV_TYPE: ''\n",
    "    TRAINING_PYTHON_VERSION: ''\n",
    "  TRAINING_REGION: ''\n",
    "  TRAINING_RESOURCE_OCID: ''\n",
    "  USER_OCID: ''\n",
    "  VM_IMAGE_INTERNAL_ID: ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98fb8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_group_live/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_group_live/score.py\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import logging\n",
    "import cloudpickle\n",
    "\n",
    "model_name = 'model.pickle'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def load_model(model_folder):\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    model_file_name = \"model.pickle\"\n",
    "    model_path = model_folder + '/' + model_file_name\n",
    "    print(f\"Model path is: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        error = \"Model Path doesn't exist : \" + model_path\n",
    "        print(f\"Model path does not exists: {model_path}\")\n",
    "        raise Exception(error)\n",
    "\n",
    "    # TODO: Load the model from the model_dir using the appropriate loader\n",
    "    # Below is a sample code to load a model file using `cloudpickle` which was serialized using `cloudpickle`\n",
    "    # from cloudpickle import cloudpickle\n",
    "    with open(model_path, \"rb\") as file:\n",
    "        model = cloudpickle.load(file)\n",
    "\n",
    "    print(\"Model is successfully loaded\")\n",
    "    return model\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def fetch_data_type_from_schema(input_schema_path=os.path.join(os.path.dirname(os.path.realpath(__file__)), \"input_schema.json\")):\n",
    "    \"\"\"\n",
    "    Returns data type information fetch from input_schema.json.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_schema_path: path of input schema.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_type: data type fetch from input_schema.json.\n",
    "\n",
    "    \"\"\"\n",
    "    data_type = {}\n",
    "    if os.path.exists(input_schema_path):\n",
    "        schema = json.load(open(input_schema_path))\n",
    "        for col in schema['schema']:\n",
    "            data_type[col['name']] = col['dtype']\n",
    "    else:\n",
    "        print(\"input_schema has to be passed in in order to recover the same data type. pass `X_sample` in `ads.model.framework.sklearn_model.SklearnModel.prepare` function to generate the input_schema. Otherwise, the data type might be changed after serialization/deserialization.\")\n",
    "    return data_type\n",
    "\n",
    "def deserialize(data, input_schema_path):\n",
    "    \"\"\"\n",
    "    Deserialize json serialization data to data in original type when sent to predict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: serialized input data.\n",
    "    input_schema_path: path of input schema.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: deserialized input data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, bytes):\n",
    "        logging.warning(\n",
    "            \"bytes are passed directly to the model. If the model expects a specific data format, you need to write the conversion logic in `deserialize()` yourself.\"\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    data_type = data.get('data_type', '') if isinstance(data, dict) else ''\n",
    "    json_data = data.get('data', data) if isinstance(data, dict) else data\n",
    "\n",
    "    if \"numpy.ndarray\" in data_type:\n",
    "        load_bytes = BytesIO(base64.b64decode(json_data.encode('utf-8')))\n",
    "        return np.load(load_bytes, allow_pickle=True)\n",
    "    if \"pandas.core.series.Series\" in data_type:\n",
    "        return pd.Series(json_data)\n",
    "    if \"pandas.core.frame.DataFrame\" in data_type or isinstance(json_data, str):\n",
    "        return pd.read_json(json_data, dtype=fetch_data_type_from_schema(input_schema_path))\n",
    "    if isinstance(json_data, dict):\n",
    "        return pd.DataFrame.from_dict(json_data)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "def pre_inference(data, input_schema_path):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Data format as expected by the predict API of the core estimator.\n",
    "    input_schema_path: path of input schema.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: Data format after any processing.\n",
    "\n",
    "    \"\"\"\n",
    "    data = deserialize(data, input_schema_path)\n",
    "    return data\n",
    "\n",
    "def post_inference(yhat):\n",
    "    \"\"\"\n",
    "    Post-process the model results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yhat: Data format after calling model.predict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    yhat: Data format after any processing.\n",
    "\n",
    "    \"\"\"\n",
    "    return yhat.tolist()\n",
    "\n",
    "def predict(data, model, input_schema_path=os.path.join(os.path.dirname(os.path.realpath(__file__)), \"input_schema.json\")):\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Pandas DataFrame\n",
    "    input_schema_path: path of input schema.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: {'prediction': output from model.predict method}\n",
    "\n",
    "    \"\"\"\n",
    "    #input = pre_inference(data, input_schema_path)\n",
    "    #yhat = post_inference(model.predict(input))\n",
    "    yhat = model.predict(data)\n",
    "    return {'prediction': yhat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58a14c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: model_group_live/ (stored 0%)\n",
      "updating: model_group_live/runtime.yaml (deflated 53%)\n",
      "updating: model_group_live/score.py (deflated 67%)\n",
      "  adding: model_group_live/model2.py (deflated 56%)\n",
      "  adding: model_group_live/model1.py (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model_group_live.zip model_group_live/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9f38c",
   "metadata": {},
   "source": [
    "### 4: Create Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "002fc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = oci.config.from_file()\n",
    "\n",
    "# Initialize client\n",
    "data_science_client =oci.data_science.DataScienceClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd4d0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_group_response = data_science_client.create_model_group(\n",
    "    create_base_model_group_details=oci.data_science.models.CreateModelGroupDetails(\n",
    "        create_type=\"CREATE\",\n",
    "        compartment_id=compartment_ocid,\n",
    "        project_id=project_ocid,\n",
    "        model_group_details=oci.data_science.models.HomogeneousModelGroupDetails(\n",
    "            type=\"HOMOGENEOUS\"),\n",
    "        member_model_entries=oci.data_science.models.MemberModelEntries(\n",
    "            member_model_details=[\n",
    "                oci.data_science.models.MemberModelDetails(\n",
    "                    model_id=generic_model1.model_id,\n",
    "                    inference_key=\"square\"),\n",
    "                    oci.data_science.models.MemberModelDetails(\n",
    "                    model_id=generic_model2.model_id,\n",
    "                    inference_key=\"square-root\")]),\n",
    "        display_name=\"Business-Model-Group-Live\",\n",
    "        description=\"Example of creating a Homogenous Model Group on OCI Data Science for custom logic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1d30ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING\n"
     ]
    }
   ],
   "source": [
    "print(create_model_group_response.data.lifecycle_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5863d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group_id = create_model_group_response.data.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b3532",
   "metadata": {},
   "source": [
    "### 5: Upload Model Group Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b99cf875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name = \"model_group_live.zip\"\n",
    "content_disposition = \"attachment;filename={}\".format(os.path.basename(file_name))\n",
    "\n",
    "with open(file_name, \"rb\") as f:\n",
    "    response = data_science_client.create_model_group_artifact(\n",
    "        model_group_id=model_group_id,\n",
    "        model_group_artifact=f,\n",
    "        content_disposition=content_disposition\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24371719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "print(response.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b1be6",
   "metadata": {},
   "source": [
    "### 6: Create Model Group Version Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = \"Business-Model-Group-Live-v2\"\n",
    "description = \"An example model group version set for business logic.\"\n",
    "\n",
    "#Create Model Group Version Set details\n",
    "create_details = oci.data_science.models.CreateModelGroupVersionHistoryDetails(\n",
    "   compartment_id=compartment_ocid,\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    project_id = project_ocid,\n",
    "    latest_model_group_id = model_group_id\n",
    ")\n",
    "\n",
    "# Create the Model GroupVersion Set\n",
    "response = data_science_client.create_model_group_version_history(create_details)\n",
    "print(\"Created Model GroupVersion Set: \", response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6678e8b",
   "metadata": {},
   "source": [
    "### 7: Deploy Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Compute Shape \n",
    "instance_shape_config_details = oci.data_science.models.ModelDeploymentInstanceShapeConfigDetails(\n",
    "    memory_in_gbs=16,\n",
    "    ocpus=1\n",
    ")\n",
    "\n",
    "# Set Instance Type\n",
    "instance_configuration = oci.data_science.models.InstanceConfiguration(\n",
    "    instance_shape_name=\"VM.Standard.E4.Flex\",\n",
    "    model_deployment_instance_shape_config_details=instance_shape_config_details\n",
    ")\n",
    "\n",
    "# Set Scaling Policy\n",
    "scaling_policy = oci.data_science.models.FixedSizeScalingPolicy(\n",
    "    policy_type=\"FIXED_SIZE\",\n",
    "    instance_count=1  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Set Instance Config Details\n",
    "infrastructure_config_details = oci.data_science.models.InstancePoolInfrastructureConfigurationDetails(\n",
    "    infrastructure_type=\"INSTANCE_POOL\",\n",
    "    instance_configuration=instance_configuration,\n",
    "    scaling_policy=scaling_policy\n",
    ")\n",
    "\n",
    "# Set Environment Details (NOTE: Change this to use BYOC option)\n",
    "environment_config_details = oci.data_science.models.DefaultModelDeploymentEnvironmentConfigurationDetails(\n",
    "    environment_configuration_type=\"DEFAULT\",\n",
    "    environment_variables={\"WEB_CONCURRENCY\":\"1\"}\n",
    ")\n",
    "\n",
    "# Set Model Group Details\n",
    "model_group_config_details = oci.data_science.models.ModelGroupConfigurationDetails(\n",
    "    model_group_id=model_group_id\n",
    ")\n",
    "\n",
    "# Set Model Group Deployment Details (infrastructure + model group)\n",
    "model_group_deployment_config_details = oci.data_science.models.ModelGroupDeploymentConfigurationDetails(\n",
    "    deployment_type=\"MODEL_GROUP\",\n",
    "    model_group_configuration_details=model_group_config_details,\n",
    "    infrastructure_configuration_details=infrastructure_config_details,\n",
    "    environment_configuration_details=environment_config_details\n",
    ")\n",
    "\n",
    "# Set logging details\n",
    "category_log_details = oci.data_science.models.CategoryLogDetails(\n",
    "    access=oci.data_science.models.LogDetails(\n",
    "        log_group_id='ocid1.loggroup...',\n",
    "        log_id='ocid1.log...'\n",
    "    ),\n",
    "    predict=oci.data_science.models.LogDetails(\n",
    "        log_group_id='ocid1.loggroup...',\n",
    "        log_id='ocid1.log...'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create Model Deployment using above\n",
    "create_model_deployment_details = oci.data_science.models.CreateModelDeploymentDetails(\n",
    "    display_name='Business Logic Model Group',\n",
    "    description='Model Group Deployment for business logic',\n",
    "    compartment_id=compartment_ocid,\n",
    "    project_id=project_ocid,\n",
    "    model_deployment_configuration_details=model_group_deployment_config_details,\n",
    "    category_log_details=category_log_details  # or omit entirely if logging not required\n",
    ")\n",
    "\n",
    "response = data_science_client.create_model_deployment(\n",
    "    create_model_deployment_details=create_model_deployment_details\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c77248ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group_url = response.data.model_deployment_url+'/predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e201d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deployment_ocid=response.data.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93950e24",
   "metadata": {},
   "source": [
    "### 8: Score Model Group Deployment via REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0bd159d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 81}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = {'number':9} \n",
    "\n",
    "headers = {'Content-Type':'application/json',\n",
    "          'model-key':'square'} # all we change is the model key\n",
    "requests.post(model_group_url, json=body, auth=auth, headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14001060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 3.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'Content-Type':'application/json',\n",
    "          'model-key':'square-root'} # all we change is the model key\n",
    "requests.post(model_group_url, json=body, auth=auth, headers=headers).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b36fec8",
   "metadata": {},
   "source": [
    "### 8: Change Business Model 2 - Update Version Set\n",
    "\n",
    "Here we make it return the square root of the absolute value of the number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d814c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model3 import model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d3d1bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ads.common:In the future model input will be serialized by `cloudpickle` by default. Currently, model input are serialized into a dictionary containing serialized input data and original data type information.Set `model_input_serializer=\"cloudpickle\"` to use cloudpickle model input serializer.\n"
     ]
    }
   ],
   "source": [
    "# basic model to square a given number\n",
    "\n",
    "generic_model3 = GenericModel(artifact_dir='model3_live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa961316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/.venv/lib/python3.13/site-packages/ads/model/runtime/env_info.py:92: UserWarning: slug will be deprecated. Provide conda pack path instead.                                                      ?, ?it/s]\n",
      "  warnings.warn(\"slug will be deprecated. Provide conda pack path instead.\")\n",
      "\n",
      "ERROR:root:Error occurred in attempt to extract the list of the service conda environments from the object storage for bucket 'service-conda-packs' and namespace 'id19sfcrra6z'. Please make sure that you've provided correct bucket and namespace.\n",
      "INFO:ADS:To auto-extract taxonomy metadata the model must be provided. Supported models: keras, lightgbm, pytorch, sklearn, tensorflow, pyspark, and xgboost.     ?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "algorithm: null\n",
       "artifact_dir:\n",
       "  /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model3_live:\n",
       "  - - .model-ignore\n",
       "    - runtime.yaml\n",
       "    - score.py\n",
       "framework: null\n",
       "model_deployment_id: null\n",
       "model_id: null"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model3.prepare(inference_conda_env='generalml_p311_cpu_x86_64_v1',\n",
    "                      model_file_name='score.py',\n",
    "                      score_py_uri='custom_score_v2.py',\n",
    "                      inference_python_version='3.11',force_overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad568436",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model()\n",
    "\n",
    "with open('model3_live/model.pickle','wb') as f:\n",
    "    pickle.dump(m,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f1e91383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model3_live ...\n",
      "Model is successfully loaded.\n",
      "trying to predict using pickle\n",
      "data received by predict function\n",
      "input number is 9\n",
      "yhat is currently 3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 3.0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model3.verify({'number':9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97bc5b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Actions Needed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Step</th>\n",
       "      <th>Status</th>\n",
       "      <th>Details</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>initiate</th>\n",
       "      <th>Done</th>\n",
       "      <th>Initiated the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">prepare()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Done</th>\n",
       "      <th>Generated runtime.yaml</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Generated score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Available</th>\n",
       "      <th>Serialized model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Done</th>\n",
       "      <th>Populated metadata(Custom, Taxonomy and Provenance)</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verify()</th>\n",
       "      <th>Done</th>\n",
       "      <th>Local tested .predict from score.py</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">save()</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Available</th>\n",
       "      <th>Conducted Introspect Test</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uploaded artifact to model catalog</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deploy()</th>\n",
       "      <th>UNKNOWN</th>\n",
       "      <th>Deployed the model</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predict()</th>\n",
       "      <th>Not Available</th>\n",
       "      <th>Called deployment predict endpoint</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            Actions Needed\n",
       "Step      Status        Details                                                           \n",
       "initiate  Done          Initiated the model                                               \n",
       "prepare() Done          Generated runtime.yaml                                            \n",
       "                        Generated score.py                                                \n",
       "          Available     Serialized model                                                  \n",
       "          Done          Populated metadata(Custom, Taxonomy and Provenance)               \n",
       "verify()  Done          Local tested .predict from score.py                               \n",
       "save()    Available     Conducted Introspect Test                                         \n",
       "                        Uploaded artifact to model catalog                                \n",
       "deploy()  UNKNOWN       Deployed the model                                                \n",
       "predict() Not Available Called deployment predict endpoint                                "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model3.summary_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd5a8615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model3_live/runtime.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile model3_live/runtime.yaml\n",
    "\n",
    "MODEL_ARTIFACT_VERSION: '3.0'\n",
    "MODEL_DEPLOYMENT:\n",
    "  INFERENCE_CONDA_ENV:\n",
    "    INFERENCE_ENV_PATH: oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/General_Machine_Learning_for_CPUs_on_Python_3.11/1.0/generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_SLUG: generalml_p311_cpu_x86_64_v1\n",
    "    INFERENCE_ENV_TYPE: data_science\n",
    "    INFERENCE_PYTHON_VERSION: '3.11'\n",
    "MODEL_PROVENANCE:\n",
    "  PROJECT_OCID: ''\n",
    "  TENANCY_OCID: ''\n",
    "  TRAINING_CODE:\n",
    "    ARTIFACT_DIRECTORY: ''\n",
    "  TRAINING_COMPARTMENT_OCID: ''\n",
    "  TRAINING_CONDA_ENV:\n",
    "    TRAINING_ENV_PATH: ''\n",
    "    TRAINING_ENV_SLUG: ''\n",
    "    TRAINING_ENV_TYPE: ''\n",
    "    TRAINING_PYTHON_VERSION: ''\n",
    "  TRAINING_REGION: ''\n",
    "  TRAINING_RESOURCE_OCID: ''\n",
    "  USER_OCID: ''\n",
    "  VM_IMAGE_INTERNAL_ID: ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edf52433",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_model3.reload_runtime_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a10aa01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading model.pickle from model directory /Users/hsnart/Documents/Custom Demos and Blogs/Business Logic MMS/model3_live ...\n",
      "Model is successfully loaded.\n",
      "['.model-ignore', 'runtime.yaml', 'model3.py', 'model.pickle', 'score.py']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edc50566b1f40c9b91a4e873adc2312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = generic_model3.save(compartment_id=compartment_ocid,\n",
    "    project_id=project_ocid,\n",
    "    display_name=\"Business Model 2\",\n",
    "    model_version_set=mvs2,\n",
    "    version_label=\"Version 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b93fd",
   "metadata": {},
   "source": [
    "### 9: Create New Model Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "440d66f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: model_group_live/ (stored 0%)\n",
      "updating: model_group_live/runtime.yaml (deflated 53%)\n",
      "updating: model_group_live/score.py (deflated 67%)\n",
      "updating: model_group_live/model2.py (deflated 56%)\n",
      "updating: model_group_live/model1.py (deflated 56%)\n",
      "  adding: model_group_live/model3.py (deflated 55%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model_group_live.zip model_group_live/\n",
    "# add model3.py to the zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e427e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = oci.config.from_file()\n",
    "\n",
    "# Initialize client\n",
    "data_science_client =oci.data_science.DataScienceClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e313845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_group_response = data_science_client.create_model_group(\n",
    "    create_base_model_group_details=oci.data_science.models.CreateModelGroupDetails(\n",
    "        create_type=\"CREATE\",\n",
    "        compartment_id=compartment_ocid,\n",
    "        project_id=project_ocid,\n",
    "        model_group_details=oci.data_science.models.HomogeneousModelGroupDetails(\n",
    "            type=\"HOMOGENEOUS\"),\n",
    "        member_model_entries=oci.data_science.models.MemberModelEntries(\n",
    "            member_model_details=[\n",
    "                oci.data_science.models.MemberModelDetails(\n",
    "                    model_id=generic_model1.model_id,\n",
    "                    inference_key=\"square\"),\n",
    "                    oci.data_science.models.MemberModelDetails(\n",
    "                    model_id=generic_model3.model_id, # update model to model3 from model2\n",
    "                    inference_key=\"square-root\")]),\n",
    "        display_name=\"Business-Model-Group-Live-v2\",\n",
    "        description=\"Example of creating a Homogenous Model Group on OCI Data Science for custom logic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62959f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_group_id = create_model_group_response.data.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "02e3e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name = \"model_group_live.zip\"\n",
    "content_disposition = \"attachment;filename={}\".format(os.path.basename(file_name))\n",
    "\n",
    "with open(file_name, \"rb\") as f:\n",
    "    response = data_science_client.create_model_group_artifact(\n",
    "        model_group_id=new_model_group_id,\n",
    "        model_group_artifact=f,\n",
    "        content_disposition=content_disposition\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745d9d6",
   "metadata": {},
   "source": [
    "### 10: Update Model Group Version Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ebef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = \"Business-Model-Group-Live\"\n",
    "description = \"An example model group version set for business logic.\"\n",
    "\n",
    "#Create Model Group Version Set details\n",
    "update_details = oci.data_science.models.UpdateModelGroupVersionHistoryDetails(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    latest_model_group_id = new_model_group_id)\n",
    "\n",
    "# Create the Model GroupVersion Set\n",
    "response = data_science_client.update_model_group_version_history(model_group_version_history_id='ocid1.dscmodelgroupversionhistory...',update_model_group_version_history_details=update_details)\n",
    "print(\"Created Model GroupVersion Set: \", response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145607ab",
   "metadata": {},
   "source": [
    "### 10: Live Update of Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "976610fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_group_configuration_details = oci.data_science.models.UpdateModelGroupConfigurationDetails( model_group_id=new_model_group_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0107ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deployment_configuration_details = oci.data_science.models.UpdateModelGroupDeploymentConfigurationDetails(\n",
    " deployment_type=\"MODEL_GROUP\",\n",
    " update_type=\"LIVE\",\n",
    " model_group_configuration_details=update_model_group_configuration_details\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75726040",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_model_deployment_details = oci.data_science.models.UpdateModelDeploymentDetails(\n",
    " display_name=\"Business-Logic-Updated\",\n",
    " description=\"Live model update to deployment\",\n",
    " model_deployment_configuration_details=model_deployment_configuration_details\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2fc80fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update submitted. Status: 202\n"
     ]
    }
   ],
   "source": [
    "response = data_science_client.update_model_deployment(\n",
    " model_deployment_id=model_deployment_ocid, # from the model deployment response object above\n",
    " update_model_deployment_details=update_model_deployment_details\n",
    " ) \n",
    "print(\"Update submitted. Status:\", response.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd6e30",
   "metadata": {},
   "source": [
    "## Supplementary information on automation task\n",
    "\n",
    "You can use the below to help productionise the creation of new Model Groups and subsequent Live Updates of the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b33c0",
   "metadata": {},
   "source": [
    "### 1: How to get Latest OCID of Model in Model Version Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9e092c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs = ModelVersionSet.from_name(name='business-model-live-1',compartment_id=compartment_ocid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e41f3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = len(mvs.models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d874e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  2 in model version set\n"
     ]
    }
   ],
   "source": [
    "print('there are ',num_models,'in model version set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in mvs.models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f0753",
   "metadata": {},
   "source": [
    "### 2: How to Get latest OCI of Model Group History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001be680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model group id of latest model group in version history\n",
    "\n",
    "def find_latest_model_group_id(compartment_ocid, project_ocid, group_history_name):\n",
    "    config = oci.config.from_file()\n",
    "    data_science_client = oci.data_science.DataScienceClient(config)\n",
    "    try:\n",
    "        response =data_science_client.list_model_group_version_histories(\n",
    "            compartment_id=compartment_ocid,\n",
    "           project_id=project_ocid,\n",
    "            sort_by=\"timeCreated\",\n",
    "            sort_order=\"DESC\"\n",
    "        )\n",
    "        # Filter inPython\n",
    "        filtered = [\n",
    "            h for h in response.data\n",
    "            if h.display_name == group_history_name\n",
    "       ]\n",
    "        if not filtered:\n",
    "            print(\"No model group version histories found with given display name.\")\n",
    "            return None\n",
    "        history = filtered[0]\n",
    "        target_model_history = history.id\n",
    "        response =data_science_client.get_model_group_version_history(model_group_version_history_id=target_model_history)\n",
    "        model_history = response.data\n",
    "\n",
    "        return {'history_id': history.id, 'history_name':history.display_name,'latest_model_group_id':model_history.latest_model_group_id}\n",
    "    except oci.exceptions.ServiceError as e:\n",
    "        print(f\"ServiceError: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "find_latest_id = find_latest_model_group_id(compartment_ocid,project_ocid,'Business-Model-Group-Live')\n",
    "print(find_latest_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
