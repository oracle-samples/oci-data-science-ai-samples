{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>ADS Sample Notebook.\n",
    "    \n",
    "Copyright (c) 2020 Oracle, Inc. All rights reserved.\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>\n",
    "\n",
    "***\n",
    "\n",
    "# Data Flow\n",
    "\n",
    "<p style='margin-left:10%, margin-right:10%'>by the <font color='teal'>Oracle Cloud Infrastructure Data Science Team</font></p>\n",
    "\n",
    "***\n",
    "\n",
    "## Overview of this Notebook\"\n",
    "\n",
    "This notebook demonstrates operations that can be performed using the Advanced Data Science (ADS) Data Flow module. The demonstrated operations are: \n",
    "\n",
    "* How to prepare and create an application.\n",
    "* How to prepare and create a run.\n",
    "* How to list existing dataflow applications.\n",
    "* How to retrieve and display the logs.\n",
    "\n",
    "The purpose of the `dataflow` module is to provide an efficient and convenient way for users to launch a Spark application and run Spark jobs.\n",
    "\n",
    "Important:\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "***\n",
    "   \n",
    "## Objectives:\n",
    "* <a href='#instance'>Creating a Data Flow application</a>\n",
    "    * <a href='#instance'>Create a Data Flow instance</a>\n",
    "       * <a href='#templates'>Leveraging PySpark and Apache Spark SQL templates</a>\n",
    "       * <a href='#appscript'>application script</a>\n",
    "    * <a href='#app'>Preparing the application</a>\n",
    "    * <a href='#regapp'>Registering the application</a>\n",
    "* <a href='#run'>Running a Data Flow application</a>\n",
    "    * <a href='#run'>Preparing the run</a>\n",
    "* <a href='#logs'>Working with logs</a>\n",
    "* <a href='#sync'>Editing and synchronizing a PySpark script</a>\n",
    "* <a href='#params'>Passing Arguments to the script</a>\n",
    "* <a href='#list'>Listing, filtering, and sorting existing Data Flow applications and runs</a>\n",
    "* <a href='#load'>Loading an existing Data Flow application</a>\n",
    "* <a href='#reference'>References\n",
    "</a>\n",
    "\n",
    "***\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "Configured your tenancy for use with the Data Flow service by following the steps in [Getting Started with Data Flow](https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_getting_started.htm#getting_started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "from ads.common import auth as authutil\n",
    "from ads.dataflow.dataflow import DataFlow\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell can be commented it out if API keys are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(auth=\"resource_principal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Uses:\n",
    "The `DataFlow` object enables a data scientist to compute a large amount of data that cannot reasonably be processed on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Datasets are provided as a convenience. Datasets are considered Third Party Content and are not considered Materials \n",
    "under your agreement with Oracle applicable to the services. \n",
    "\n",
    "The <a href=\"http://insideairbnb.com/get-the-data.html\">`kaggle_berlin_airbnb_listings_summary.csv` dataset</a> is distributed under the [Universal Permissive License](oracle_data/UPL.txt). \n",
    "\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Flow application\n",
    "\n",
    "<a id='instance'></a>\n",
    "## Create a Data Flow instance\n",
    "\n",
    "A `DataFlow` object is used to interact with the Data Flow service. The optional `dataflow_base_folder` parameter defines the path where the Data Flow artifacts are stored. It defaults to the `~/dataflow` folder. A compartment can be specified with the optional `compartment_id` parameter. The default behavior is to use the compartment of the notebook session.\n",
    "\n",
    "The optional parameters like `os_auth` can be used to specify the preferred authentication method to access OCI Object Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_base_folder = tempfile.mkdtemp()\n",
    "data_flow = DataFlow(dataflow_base_folder=dataflow_base_folder, os_auth=authutil.api_keys(), df_auth=authutil.api_keys())\n",
    "print(\"Data flow directory: {}\".format(dataflow_base_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='template'></a>\n",
    "### Leveraging PySpark and Apache Spark SQL templates\n",
    "\n",
    "The PySpark and Apache Spark SQL templates assist you to get started with Data Flow. Use `data_flow.template()` to generate a template file. \n",
    "\n",
    "The supported templates are:\n",
    "1. `standard_pyspark`: template, which is for standard PySpark jobs.\n",
    "2. `sparksql`: template, which is for Apache Spark SQL jobs.\n",
    "\n",
    "For example, to create an Apache Spark SQL template use:\n",
    "```python\n",
    "script = data_flow.template(job_type='sparksql')\n",
    "```\n",
    "This creates a Python file in the `dataflow_base_folder`. The `template()` method returns the path to the file.\n",
    "\n",
    "<a id='appscript'></a>\n",
    "### Application script\n",
    "\n",
    "In addition to the template scripts, custom scripts are supported. The following writes a python script that loads comma separated value (CSV) files from Object Storage and applies filtering. In this example, the data is read in from a publically accessible Object Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_file_path = path.join(dataflow_base_folder, \"example-{}.py\".format(str(uuid.uuid4())[-6:]))\n",
    "script = '''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Create a Spark session\n",
    "    spark = SparkSession \\\\\n",
    "        .builder \\\\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Load a csv file from dataflow public storage\n",
    "    df = spark \\\\\n",
    "        .read \\\\\n",
    "        .format(\"csv\") \\\\\n",
    "        .option(\"header\", \"true\") \\\\\n",
    "        .option(\"multiLine\", \"true\") \\\\\n",
    "        .load(\"oci://oow_2019_dataflow_lab@bigdatadatasciencelarge/usercontent/kaggle_berlin_airbnb_listings_summary.csv\")\n",
    "    \n",
    "    # Create a temp view and do some SQL operations\n",
    "    df.createOrReplaceTempView(\"berlin\")\n",
    "    query_result_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            city,  \n",
    "            zipcode,  \n",
    "            CONCAT(latitude,',', longitude) AS lat_long\n",
    "        FROM berlin \n",
    "    \"\"\")\n",
    "    \n",
    "    # Convert the filtered Spark DataFrame into JSON format\n",
    "    # Note: we are writing to the spark stdout log so that we can retrieve the log later at the end of the notebook.\n",
    "    print('\\\\n'.join(query_result_df.toJSON().collect()))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open(pyspark_file_path, 'w') as f:\n",
    "    print(script.strip(), file=f)\n",
    "    \n",
    "print(\"Script path: {}\".format(pyspark_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='app'></a>\n",
    "## Preparing the application\n",
    "\n",
    "The application creation process contains preparation and creation stages. In the preparation stage, a configuration object is created with a call to the `prepare_app()` method. The following parameters are required:\n",
    "\n",
    "* `display_name`: The application name.\n",
    "* `script_bucket`: The bucket used to read and write the `pyspark` script in Object Storage\n",
    "* `pyspark_file_path`: The path to the `pyspark` script\n",
    "\n",
    "There are also a number of common optional parameters:\n",
    "* `logs_bucket`: Bucket for the run logs. Default: `dataflow-logs`\n",
    "* `compartment_id`: compartment used to run the job. Default: Compartment of the notebook session.\n",
    "* `driver_shape`: CPU shape for the driver VM. Default: VM.Standard2.4\n",
    "* `executor_shape`: CPU shape for the executor VMs. Default: VM.Standard2.4 \n",
    "* `num_executors`: Number of executor machines. Default: 1\n",
    "\n",
    "To use a private bucket as the `logs_bucket`, ensure that a Data Flow Service policy has been added. See the [prerequisite step](#prereq) and the [policy setup page](https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_getting_started.htm#policy_set_up) for more details.\n",
    "\n",
    "**Update the `script_bucket` and `logs_bucket` variables to match your tenancy's configuration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_bucket = \"test\"                     # Update the value\n",
    "logs_bucket = \"dataflow-log\"               # Update the value\n",
    "display_name = \"sample_Data_Flow_app\"      \n",
    "\n",
    "app_config = data_flow.prepare_app(display_name=display_name,\n",
    "                                   script_bucket=script_bucket,\n",
    "                                   pyspark_file_path=pyspark_file_path,\n",
    "                                   logs_bucket=logs_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regapp'></a>\n",
    "## Registering the application\n",
    "\n",
    "A Data Flow application must be registered within the Oracle Cloud Infrastructure using the `create_app()` method. This method accepts the `app_config` dictionary and creates a `DataFlowApp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = data_flow.create_app(app_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config` attribute in a `DataFlowApp` object returns a dictionary of configuration information about the Data Flow application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `oci_link` attribute returns a link to the Oracle Cloud Infrastructure Console Application Details page: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.oci_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='run'></a>\n",
    "# Running a Data Flow application\n",
    "\n",
    "<a id='preprun'></a>\n",
    "## Preparing a Data Flow run\n",
    "\n",
    "To run a Data Flow application, a run configuration is created using the `prepare_run()` method. The application is then executed with the `run()` method.\n",
    "\n",
    "The `prepare_run()` method has the following common parameters:\n",
    "* `run_display_name`: Name of the run.\n",
    "* `compartment_id`: Compartment used to run the job. Default: Compartment of the notebook session.\n",
    "* `logs_bucket`: (optional) Bucket for the run logs. Default: Inherited from the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_display_name = \"sample_Data_Flow_run\"\n",
    "run_config = app.prepare_run(run_display_name=run_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Data Flow Application\n",
    "\n",
    "Execute a Data Flow application with the `run()` method. This returns a `DataFlowRun` object.\n",
    "\n",
    "The `run()` method accepts the `run_config` dictionary. When the optional `save_log_to_local` parameter is set to `True`, it pulls a copy of the logs into a subfolder of the `dataflow_base_folder`. The subfolder name is based on the application display name with a random extension and it contains another folder whos name is based on the run display name with a random extension.\n",
    "\n",
    "The run configuration is stored in the run subfolder in the file `run_metadata.json`. This subfolder also has a copy of the executed script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = app.run(run_config, save_log_to_local=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `status` attribute of a `DataFlowRun` object provides the execution status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary of a run's configuration is accessible from the `config` attribute. This is the same information that is stored in the `run_metadata.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `oci_link` attribute gives a link to the Oracle Cloud Infrastructure Run Details page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.oci_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional `wait` parameter can be set to `False` to have the run be asynchronous. Using `run.status()` you can monitor when run is accepted, in progress and finally complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_observer = app.run(run_config, wait=False)\n",
    "run_observer.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment following line to run synchronously\n",
    "#run = run_observer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='logs'></a>\n",
    "# Working with logs\n",
    "\n",
    "The Data Flow logs are stored in Object Storage. If the parameter `save_log_to_local` is set to `True`, then the logs are pulled onto the local drive. The `fetch_log()` method returns a `DataFlowLog` object. The pass in `\"stdout\"` or `\"stderr\"` to get the standard out and error logs, respectively. Using the `save()` method on the `DataFlowLog` object causes the logs to be stored on the local drive.\n",
    "\n",
    "The following example pulls the standard out and error logs onto the local storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fetch_log(\"stdout\").save()\n",
    "run.fetch_log(\"stderr\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `log_stdout` and `log_stderr` attributes return `DataFlowLog` objects for the standard out and error logs. The `head()` and `tail()` methods prints the beginning or end of the log files. By default, 10 lines are printed though they accept a parameter to specify the number of lines to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_stdout.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFlowLog` objects that are returned by `log_stdout` and `log_stderr` also have the `oci_path` and `local_path` attributes. These return the bucket and local file path of the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_stdout.oci_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_stdout.local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sync'></a>\n",
    "# Editing and synchronizing a PySpark script\n",
    "\n",
    "A Data Flow application can be run multiple times by calling `run()`. A common use case would be when the data has changed and an updated analysis is wantede. \n",
    "\n",
    "Another common use case is when there have been changes to the PySpark script. The Data Flow application keeps a copy of script. However, if local changes are made to the script the default behavior is to synchronize the local script with the Data Flow application. Setting the `sync` parameter in the `run()` method to `False` prevents the local copy of the script from being updated in the Data Flow application so the existing script in the application is executed.\n",
    "\n",
    "<a id='params'></a>\n",
    "# Passing Arguments to the script\n",
    "\n",
    "To pass command line arguments to the Data Flow application, set the value of the `arguments` parameter in the `prepare_app()` method. The `arguments` parameter takes a list of command line arguments to be passed to the PySpark script. For example:\n",
    "```python\n",
    "arguments = ['-f', 'foobar', '-d', '--file', 'file.txt']\n",
    "```\n",
    "In this example, the arguments are hardcoded. Data Flow supports mechanism to parameterize arguments. The `script_parameter` option accepts a dictionary that is used to update values in the `arguments` parameter. The arguments must be in the format of `'${key}'` and they are replaced by the value associated with the key. The following demonstrates this process:\n",
    "\n",
    "```python\n",
    "arguments = ['${foo}', '-d', '--file', '${filename}'], \n",
    "script_parameters={'foo': '-bar', 'filename': 'file.txt'}\n",
    "```\n",
    "\n",
    "The command line argument seen by the PySpark script is:\n",
    "```bash\n",
    "-bar -d --file file.txt\n",
    "```\n",
    "\n",
    "An example workflow would look like:\n",
    "```python\n",
    "app_config = data_flow.prepare_app(\n",
    "    display_name, script_bucket, pyspark_file_path, \n",
    "    arguments = ['${foo}', 'bar', '-d', '--file', '${filename}'], \n",
    "    script_parameters={'foo': '-bar', 'filename': 'file.txt'})\n",
    "app = data_flow.create_app(app_config)\n",
    "run_config = app.prepare_run(\"Argument_run\")\n",
    "run = app.run(run_config)\n",
    "```\n",
    "\n",
    "In this example, the script parameters are associated with the application configuration. They can be overridden in the `prepare_run()` method by passing a parameter to this method that has the same name as the script parameter that is to be updated. In this example, the value of `foo` is `-bar`. If this was replaced with `-babar`, then the following call could be used:\n",
    "```python\n",
    "run_config = app.prepare_run(\"Override_Argument_run\", foo='-babar')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='list'></a>\n",
    "# Listing, filtering, and sorting existing Data Flow applications and runs\n",
    "\n",
    "From `ADS` you can list applications and runs. They are returned as a list of dictionaries, with a function to display the data in a Pandas dataframe. The default sort order is the most recent application or run first.\n",
    "\n",
    "## Listing Applications\n",
    "\n",
    "The method `list_apps()` returns a `SummaryList` object with the list of Data Flow applications, which can be sliced:\n",
    "```python\n",
    "data_flow.list_apps()[0:2]\n",
    "```\n",
    "Or it can be converted to a dataframe with the `to_dataframe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flow.list_apps().to_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs\n",
    "### Listing runs\n",
    "\n",
    "The method `list_runs()` method on a `DataFlowApp` object returns a `SummaryList` object with the list of runs for that application, which can be sliced:\n",
    "```python\n",
    "app.list_runs()[0:2]\n",
    "```\n",
    "Or it can  be converted to a dataframe with the `to_dataframe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.list_runs().to_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information on a run\n",
    "\n",
    "The `get_run()` method returns information about a run. It requires the OCID for a run. This can be the shortened id or the complete OCID. In this example, the full OCID is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_run(app.list_runs()[0].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "# Loading an existing Data Flow application\n",
    "\n",
    "`ADS` uses a Data Flow application's OCID to load an existing application. These Data Flow applications must be Python applications. The OCID must be provided with the `app_id` parameter. The OCID can be obtained from the Oracle Cloud Infrastructure Console or by listing existing applications using the `list_apps()` method. \n",
    "\n",
    "Optionally, the `target_folder` parameter defines the directory in which application artifacts are copied to. If `target_folder` is not provided, by default the application artifacts are stored in the `dataflow_base_folder` defined by the Data Flow object.\n",
    "\n",
    "Once an application has been loaded, the `DataFlowApp` object can be used to update the application script, and run a Data Flow job using the previously defined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_app = data_flow.load_app(app_id=data_flow.list_apps()[0].id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "# References\n",
    " - <a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Oracle Data Science</a>\n",
    " - <a href=\"https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/dataflow/dataflow.html\">Oracle ADS Data Flow</a>\n",
    " - <a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_data_flow.htm\">Oracle Data Flow</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark24_p37_cpu_v1_0]",
   "language": "python",
   "name": "conda-env-pyspark24_p37_cpu_v1_0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
