{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8ce4f16c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "@notebook{feature_store_spark_magic.ipynb,\n",
    "    title: Data Flow Studio : Big Data Operations in Feature Store.,\n",
    "    summary: Run Feature Store on interactive Spark workloads on a long lasting Data Flow Cluster.,\n",
    "    developed_on: fspyspark32_p38_cpu_v3,\n",
    "    keywords: feature store, querying,spark magic,data flow\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "!odsc conda install -s fspyspark32_p38_cpu_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade Oracle ADS to pick up the latest preview version to maintain compatibility with Oracle Cloud Infrastructure.\n",
    "!pip install --pre --no-deps oracle-ads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5598bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2022, 2023 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "***\n",
    "\n",
    "# <font color=\"red\">Data Flow Studio: Big Data Operations in Feature Store</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "\n",
    "This notebook demonstrates how to run Feature Store on interactive Spark workloads on a long lasting [Oracle Cloud Infrastructure Data Flow](https://docs.oracle.com/en-us/iaas/data-flow/using/home.htm) cluster through [Apache Livy](https://livy.apache.org/) integration. **Data Flow Spark Magic** is used for interactively working with remote Spark clusters using Livy (a Spark REST server) in Jupyter notebooks. Data Flow Spark Magic includes a set of magic commands for interactively running Spark code.\n",
    "\n",
    "\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href=\"#introduction\">1. Introduction</a>\n",
    "- <a href='#pre_requisites'>2. 2. Pre-requisites to Running this Notebook</a>\n",
    "    - <a href='#policies_'>2.1 Policies</a>\n",
    "    - <a href='#prerequisites_helpers'>2.2 Helpers</a>\n",
    "    - <a href='#authentication'>2.3 Authentication</a>\n",
    "    - <a href='#variables'>2.4 Variables</a>\n",
    "- <a href='#dataflow_magic'>3. Data Flow Spark Magic</a>\n",
    "    - <a href='#load_extension'>3.1. Load Spark Magic Commands and Getting Help</a>\n",
    "    - <a href='#create_session'>3.2. Create DataFlow Session</a>\n",
    "    - <a href='#data_exploration'>3.3. Data exploration</a>\n",
    "    - <a href='#load_featuregroup'>3.4. Create Feature Store Logical Entities</a>\n",
    "        - <a href='#create_feature_store'>3.4.1 Creating a feature store</a>\n",
    "        - <a href='#create_entity'>3.4.2 Creating an entity</a>\n",
    "        - <a href='#create_feature_group'>3.4.3 Creating a feature group</a>\n",
    "        - <a href='#materialise_feature_store'>3.4.4 Materialising a Feature Group</a>\n",
    "        - <a href='#query_feature_group'>3.4.5 Querying a Feature group</a>\n",
    "- <a href='#references'>4. References</a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Compatible conda pack: [PySpark 3.2 and Feature Store](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8 (version 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84d936",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# 1. Introduction\n",
    "\n",
    "OCI Data Science feature store is a stack-based API solution that's deployed using OCI Resource Manager in your tenancy.\n",
    "\n",
    "Review the following key terms to understand the Data Science feature store:\n",
    "\n",
    "\n",
    "* **Feature Vector**: Set of feature values for any one primary or identifier key. For example, all or a subset of features of customer id ‘2536’ can be called as one feature vector.\n",
    "\n",
    "* **Feature**: A feature is an individual measurable property or characteristic of a phenomenon being observed.\n",
    "\n",
    "* **Entity**: An entity is a group of semantically related features. The first step a consumer of features would typically do when accessing the feature store service is to list the entities and the entities associated features. Or, an entity is an object or concept that is described by its features. Examples of entities are customer, product, transaction, review, image, document, and so on.\n",
    "\n",
    "* **Feature Group**: A feature group in a feature store is a collection of related features that are often used together in machine learning (ML) models. It serves as an organizational unit within the feature store for you to manage, version, and share features across different ML projects. By organizing features into groups, data scientists and ML engineers can efficiently discover, reuse, and collaborate on features reducing the redundant work and ensuring consistency in feature engineering.\n",
    "\n",
    "* **Feature Group Job**: A feature group job is the processing instance of a feature group. Each feature group job includes validation and statistics results.\n",
    "\n",
    "* **Dataset**: A dataset is a collection of features that are used together to either train a model or perform model inference.\n",
    "\n",
    "* **Dataset Job**: A dataset job is the processing instance of a dataset. Each dataset job includes validation and statistics results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76acf33b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='pre_requisites'></a>\n",
    "# 2. Pre-requisites to Running this Notebook\n",
    "\n",
    "Data Flow Sessions are accessible using the PySpark 3.2 and Feature Store Python 3.8 (fspyspark32_p38_cpu_v3) conda environment.\n",
    "\n",
    "The [Data Catalog Hive Metastore](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm) provides schema definitions for objects in structured and unstructured data assets. The Metastore is the central metadata repository to understand tables backed by files on object storage. You can customize `fs_pyspark32_p38_cpu_v1`, publish it, and use it as a runtime environment for a Data Flow session cluster. The metastore id of hive metastore is tied to feature store construct of feature store service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2b6a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='policies_'></a>\n",
    "## 2.1. Policies\n",
    "This section covers the creation of dynamic groups and policies needed to use the service.\n",
    "\n",
    "* [Data Flow Policies](https://docs.oracle.com/iaas/data-flow/using/policies.htm)\n",
    "* [Getting Started with Data Flow](https://docs.oracle.com/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "* [About Data Science Policies](https://docs.oracle.com/iaas/data-science/using/policies.htm)\n",
    "* [Data Catalog Metastore](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207bfb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"prerequisites_helpers\"></a>\n",
    "## 2.2 Helpers\n",
    "This helper method is used across the notebook to prepare arguments for the magic commands. This function is particularly useful when you want to pass Python variables as arguments to the spark magic commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32894857",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prepare_command(command: dict) -> str:\n",
    "    \"\"\"Converts dictionary command to the string formatted commands.\"\"\"\n",
    "    return f\"'{json.dumps(command)}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b610391",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"authentication\"></a>\n",
    "## 2.3. Authentication\n",
    "The [Oracle Accelerated Data Science SDK (ADS)](https://docs.oracle.com/iaas/tools/ads-sdk/latest/index.html) controls the authentication mechanism with the Data Flow Session Spark cluster.<br> \n",
    "To setup authentication use the ```ads.set_auth(\"resource_principal\")``` or ```ads.set_auth(\"api_key\")```. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1080f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "\n",
    "ads.set_auth(\"resource_principal\")  # Supported values: resource_principal, api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa57661",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"variables\"></a>\n",
    "## 2.4. Variables\n",
    "To run this notebook, you must provide some information about your tenancy configuration. To connect to the HIVE metastore, replace `<metastore_id>` with the OCID for the HIVE metastore.\n",
    "\n",
    "To create and run a Data Flow session, you must specify a `<compartment_id>`, `<metastoreId>`, bucket `<logs_bucket_uri>` and `<custom_conda_environment_uri>` for storing logs. These resources must be in the same compartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157113d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "compartment_id = os.environ.get(\"NB_SESSION_COMPARTMENT_OCID\")\n",
    "metastore_id = \"<metastore_id>\"\n",
    "logs_bucket_uri = \"<logs-bucket-url>\"\n",
    "\n",
    "custom_conda_environment_uri = \"oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/PySpark_3.2_and_Feature_Store/1.0/fspyspark32_p38_cpu_v3#conda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426be51d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"dataflow_magic\"></a>\n",
    "# 3. Data Flow Spark Magic\n",
    "Data Flow Spark Magic commands allow you to interactively work with Data Flow Spark clusters (sessions) in Jupyter notebooks using the Livy REST API. The commands provide a set of Jupyter notebook cell magic commands to turn Jupyter into an integrated Spark development environment for remote clusters. \n",
    "\n",
    "**Data Flow Spark Magic allows you to:**\n",
    "\n",
    "* Run Spark code against a Data Flow remote Spark cluster.\n",
    "* Create a Data Flow Spark session with SparkContext and HiveContext against Data Flow remote Spark cluster.\n",
    "* Capture the output of Spark queries as a local Pandas dataframe to interact with other Python libraries (such as matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f403f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"load_extension\"></a>\n",
    "### 3.1. Load Spark Magic Commands and Getting Help\n",
    "Data Flow Spark Magic is a JupyterLab extension that you need to activate in your notebook using the `%load_ext dataflow.magics` magic command.<br>\n",
    "After the extension is activated, you can use the `%help` command to view the list of supported commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61b5fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext dataflow.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec076494",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_session\"></a>\n",
    "### 3.2. Create DataFlow Session.\n",
    "Create a new Data Flow cluster session using the `%create_session` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba2243",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "command = prepare_command(\n",
    "    {\n",
    "        \"compartmentId\": compartment_id,\n",
    "        \"displayName\": \"spark_session_via_notebook\",\n",
    "        \"language\": \"PYTHON\",\n",
    "        \"sparkVersion\": \"3.2.1\",\n",
    "        \"numExecutors\": 8,\n",
    "        \"metastoreId\": metastore_id,\n",
    "        \"driverShape\": \"VM.Standard2.1\",\n",
    "        \"executorShape\": \"VM.Standard2.1\",\n",
    "        \"driverShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 16},\n",
    "        \"executorShapeConfig\": {\"ocpus\": 2, \"memoryInGBs\": 16},\n",
    "        \"type\": \"SESSION\",\n",
    "        \"logsBucketUri\": logs_bucket_uri,\n",
    "        \"configuration\": {\n",
    "            \"spark.archives\": custom_conda_environment_uri,\n",
    "            \"fs.oci.client.hostname\": \"https://objectstorage.us-ashburn-1.oraclecloud.com\"\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "%create_session -l python -c $command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad36e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "\n",
    "import ads\n",
    "from ads.feature_store.entity import Entity\n",
    "from ads.feature_store.feature_group import FeatureGroup\n",
    "from ads.feature_store.feature_group_expectation import ExpectationType\n",
    "from ads.feature_store.feature_store import FeatureStore\n",
    "from ads.feature_store.input_feature_detail import FeatureDetail, FeatureType\n",
    "from ads.feature_store.statistics_config import StatisticsConfig\n",
    "from ads.feature_store.transformation import TransformationMode\n",
    "import os\n",
    "\n",
    "# Set the Authentications for the feature store operations\n",
    "ads.set_auth(auth=\"resource_principal\", client_kwargs={\"fs_service_endpoint\": \"https://{api_gateway}/20230101\"})\n",
    "\n",
    "# Variables\n",
    "compartment_id = \"<compartment_id>\"\n",
    "metastore_id = \"<metastore_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9fafb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"data_exploration\"></a>\n",
    "### 3.3. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde877c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "df_nyc_tlc = spark.read.parquet(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/nyc_tlc/201[1,2,3,4,5,6,7,8]/**/data.parquet\", header=False, inferSchema=True)\n",
    "df_nyc_tlc = df_nyc_tlc.select(\"vendor_id\", \"pickup_at\", \"dropoff_at\")\n",
    "\n",
    "df_nyc_tlc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180b15c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"load_featuregroup\"></a>\n",
    "### 3.4. Create feature store logical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f9744",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_store\"></a>\n",
    "#### 3.4.1. Creating a Feature Store\n",
    "Feature store is the top level entity for feature store service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8d3a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_store_resource = FeatureStore(). \\\n",
    "    with_description(\"Feature Store Description\"). \\\n",
    "    with_compartment_id(compartment_id). \\\n",
    "    with_name(\"FeatureStore\"). \\\n",
    "    with_offline_config(metastore_id=metastore_id)\n",
    "\n",
    "feature_store = feature_store_resource.create()\n",
    "feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569a4f9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_entity\"></a>\n",
    "#### 3.4.2. Creating an Entity\n",
    "An entity is a group of semantically related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d5002",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "entity = feature_store.create_entity()\n",
    "entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029a900",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"create_feature_group\"></a>\n",
    "#### 3.4.3. Creating a Feature group\n",
    "A feature group is an object that represents a logical group of time-series feature data as it is found in a datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab895fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# Initialize Expectation Suite\n",
    "expectation_suite_trans = ExpectationSuite(expectation_suite_name=\"feature_definition\")\n",
    "expectation_suite_trans.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"EXPECT_COLUMN_VALUES_TO_NOT_BE_NULL\",\n",
    "        kwargs={\"column\": \"vendor_id\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "stats_config = StatisticsConfig().with_is_enabled(False)\n",
    "\n",
    "feature_group = entity.create_feature_group(\n",
    "    primary_keys=[\"vendor_id\"],\n",
    "    schema_details_dataframe=df_nyc_tlc, #infer the schema from the data frame\n",
    "    expectation_suite=expectation_suite_trans,\n",
    "    expectation_type=ExpectationType.LENIENT,\n",
    "    statistics_config=stats_config,\n",
    "    name=\"feature_group_big_data\",\n",
    ")\n",
    "\n",
    "feature_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147916a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"materialise_feature_store\"></a>\n",
    "#### 3.4.4. Materialising a Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26caf3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pandas as pd\n",
    "df_nyc_tlc = spark.read.parquet(\"oci://hosted-ds-datasets@bigdatadatasciencelarge/nyc_tlc/201[1,2,3,4,5,6,7,8]/**/data.parquet\", header=False, inferSchema=True)\n",
    "df_nyc_tlc = df_nyc_tlc.select(\"vendor_id\", \"pickup_at\", \"dropoff_at\").limit(1000)\n",
    "\n",
    "feature_group.materialise(df_nyc_tlc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a2317",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"query_feature_group\"></a>\n",
    "#### 3.4.5. Querying a Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede99da4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_group.select().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b4b1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_group.select([\"vendor_id\", \"pickup_at\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de58a22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "feature_group.filter(feature_group.vendor_id == \"CMT\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dff2d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id='references'></a>\n",
    "# 4. References\n",
    "- [Feature Store Documentation](https://feature-store-accelerated-data-science.readthedocs.io/en/latest/overview.html)\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9babbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
