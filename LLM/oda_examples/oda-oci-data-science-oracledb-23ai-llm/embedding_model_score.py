# score.py 1.0 generated by ADS 2.8.9 on 20231213_231428
import os
from functools import lru_cache
import onnxruntime as rt
 
model_name = 'model-w-mean-pooling.onnx'
 
 
"""
   Inference script. This script is used for prediction by scoring server when schema is known.
"""
 
 
@lru_cache(maxsize=10)
def load_model(model_file_name=model_name):
    """
    Loads model from the serialized format
 
    Returns
    -------
    model:  a model instance on which predict API can be invoked
    """
    model_dir = os.path.dirname(os.path.realpath(__file__))
    contents = os.listdir(model_dir)
    if model_file_name in contents:
        loaded_model= rt.InferenceSession(os.path.join(model_dir, model_file_name),providers=["CPUExecutionProvider"])
        print("Model is successfully loaded.")
        return loaded_model
    else:
        raise Exception(f'{model_file_name} is not found in model directory {model_dir}')
 
 
def pre_inference(data):
    """
    Preprocess data
 
    Parameters
    ----------
    data: Data format as expected by the predict API of the core estimator.
    input_schema_path: path of input schema.
 
    Returns
    -------
    data: Data format after any processing.
 
    """
    from transformers import BertTokenizer
     
    model_dir = os.path.dirname(os.path.realpath(__file__))
    tokenizer = BertTokenizer.from_pretrained(model_dir)
    inputs = tokenizer(data,return_tensors="np", padding=True)
    onnx_inputs = {key: [l.tolist()for l in inputs[key] ] for key in inputs}
    return onnx_inputs
 
def post_inference(outputs):
    """
    Post-process the model results
 
    Parameters
    ----------
    yhat: Data format after calling model.predict.
 
    Returns
    -------
    yhat: Data format after any processing.
    """
    outputs = [embed.tolist() for embed in outputs]
#     for  embed in outputs:
#         print(len(embed))
    return outputs
 
def predict(data, model=load_model()):
    """
    Returns prediction given the model and data to predict
 
    Parameters
    ----------
    model: Model instance returned by load_model API.
    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Pandas DataFrame.
    input_schema_path: path of input schema.
 
    Returns
    -------
    predictions: Output from scoring server
        Format: {'prediction': output from model.predict method}
 
    """
    onnx_transformed_input= pre_inference(data)
     
    embeds= model.run(None, dict(onnx_transformed_input))[0]
    return {'embeddings':post_inference(embeds)}