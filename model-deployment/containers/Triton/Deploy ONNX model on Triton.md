# Deploy ONNX model in NVIDIA Triton Inference Server
This document provides a walkthrough for deploying an ONNX model into a NVIDIA [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) using OCI Data Science Modle Deployment's custom containers support.

The sample used here is based on [Triton's ONNX sample](https://github.com/triton-inference-server/tutorials/tree/main/Quick_Deploy/ONNX).

## Step 1: Set up Triton Inference Server
### Step 1.1: Create Model Artifact
To use Triton, we need to build a model repository. The structure of the repository as follows:
```
model_repository
|
+-- densenet_onnx
    |
    +-- config.pbtxt
    +-- 1
        |
        +-- model.onnx
```

The config.pbtxt configuration file is optional. The configuration file is autogenerated by Triton Inference Server if the user doesn't provide it. If you are new to Triton, it is highly recommended to review Part 1 of the conceptual guide.
```
mkdir -p model_repository/densenet_onnx/1
```
```
wget -O model_repository/densenet_onnx/1/model.onnx https://github.com/oracle-samples/oci-data-science-ai-samples/blob/master/model-deployment/containers/Triton/model.onnx
```

### Step 1.2  Upload NVIDIA base triton server image to OCI Container Registry

```
docker login $(OCIR_REGION).ocir.io
mkdir -p tritonServer
cd tritonServer
git clone https://github.com/triton-inference-server/server.git -b v2.30.0 --depth 1
cd server
python compose.py --backend onnxruntime --repoagent checksum --output-name $(OCIR_REGION).ocir.io/$(OCIR_NAMESPACE)/oci-datascience-triton-server/onnx-runtime:1.0.0
docker push $(OCIR_REGION).ocir.io/$(OCIR_NAMESPACE)/oci-datascience-triton-server/onnx-runtime:1.0.0
```

### Step 1.3 Upload model artifact to Model catalog
Compress model_repository folder created in Step 1.1 in zip format and upload it to model catalog. Refer to https://docs.oracle.com/en-us/iaas/data-science/using/models_saving_catalog.htm for details


### Step 1.4 Create Model Deployment
OCI Data Science Model Deployment has a dedicated support for Triton image, to make it easier to manage the Triton image by mapping of service-mandated endpoints to the Triton's inference and health HTTP/REST endpoint. To enable this support, enter the following environment variable when creating the Model Deployment:
```
CONTAINER_TYPE = TRITON
```

#### Using python sdk
```
# Create a model configuration details object
model_config_details = ModelConfigurationDetails(
    model_id= <model_id>,
    bandwidth_mbps = <bandwidth_mbps>,
    instance_configuration = <instance_configuration>,
    scaling_policy = <scaling_policy>
)
  
# Create the container environment configuration
environment_config_details = OcirModelDeploymentEnvironmentConfigurationDetails(
    environment_configuration_type="OCIR_CONTAINER",
    environment_variables={'CONTAINER_TYPE': 'TRITON'},
    image="iad.ocir.io/testtenancy/oci-datascience-triton-server/onnx-runtime:1.0.0",
    image_digest=<image_digest>,
    cmd=[
        "/opt/nvidia/nvidia_entrypoint.sh",
        "tritonserver",
        "--model-repository=/opt/ds/model/deployed_model/model_repository"
    ],
    server_port=8000,
    health_check_port=8000
)
  
# create a model type deployment
single_model_deployment_config_details = data_science.models.SingleModelDeploymentConfigurationDetails(
    deployment_type="SINGLE_MODEL",
    model_configuration_details=model_config_details,
    environment_configuration_details=environment_config_details
)
  
# set up parameters required to create a new model deployment.
create_model_deployment_details = CreateModelDeploymentDetails(
    display_name= <deployment_name>,
    model_deployment_configuration_details = single_model_deployment_config_details,
    compartment_id = <compartment_id>,
    project_id = <project_id>
)
```

## Step 2: Using Python SDK to query the Inference Server
Download an example image to test inference:

`wget  -O ${HOME}/img1.jpg "https://www.hakaimagazine.com/wp-content/uploads/header-gulf-birds.jpg"`

Specify the JSON inference payload with input and output layers for the model as well as describe the shape and datatype of the expected input and output:
```
from PIL import Image
import numpy as np
import json
# Load the image and resize it to 224x224
img = Image.open(<path to image>).resize((224, 224))
# Convert the image to a 3D NumPy array
img_array = np.array(img)
 
# Add a batch dimension to the array
input_data = np.expand_dims(img_array, axis=0)
 
# Convert the array to the appropriate data type
request_data = input_data.astype(np.float32).tolist()
 
request_body = {"inputs": [{"name": "data_0", "shape": [1,3,224,224], "datatype": "FP32", "data": request_data}], "outputs": [{"name": "fc6_1", "shape":[1,1000],"datatype": "FP32"}]}
 
request_body = json.dumps(request_body)
```

Specify the request headers indicating model name and version:
```
request_headers = {"model_name":"densenet_onnx", "model_version":"1"}
```

Now, you can send an inference request to the Triton Inference Server:
```
# The OCI SDK must be installed for this example to function properly.
# Installation instructions can be found here: https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/pythonsdk.htm
 
import requests
import oci
from oci.signer import Signer
 
config = oci.config.from_file("~/.oci/config") # replace with the location of your oci config file
auth = Signer(
  tenancy=config['tenancy'],
  user=config['user'],
  fingerprint=config['fingerprint'],
  private_key_file_location=config['key_file'],
  pass_phrase=config['pass_phrase'])
 
endpoint = <modelDeploymentEndpoint>
 
inference_output = requests.request('POST',endpoint, data=request_body, auth=auth, headers=request_headers).json()['outputs'][0]['data'][:5]
```
