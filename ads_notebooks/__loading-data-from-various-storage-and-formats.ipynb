{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@notebook{loading-data-from-various-storage-and-formats.ipynb,\n",
    "    title: Loading Data with DatasetFactory,\n",
    "    summary: Load data from a variety of sources and in different formats. Sources include local storage, OCI storage, and different databases. Formats include Pandas DataFrames, parquet, excel, csv, and Python primitives.\n",
    "    developed on: generalml_p37_cpu_v1,\n",
    "    keywords: loading data,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2020, 2022 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "---\n",
    "\n",
    "# <font color=\"red\">Loading Data with DatasetFactory</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "\n",
    "# Overview:\n",
    "\n",
    "One of the most important elements of any data science project is the data itself. This notebook demonstrates how to use `ADSDataset` to read in data from a wide selection of standard formats. The focus is on the `DatasetFactory.open()` command.\n",
    "\n",
    "Developed on [General Machine Learning](https://docs.oracle.com/en-us/iaas/data-science/using/conda-gml-fam.htm) for CPU on Python 3.7 (version 1.0)\n",
    "\n",
    "## Contents:\n",
    "\n",
    " - <a href='#src'>Loading Datasets from Various Sources</a>\n",
    "     - <a href='#loc'>Local File Storage</a>\n",
    "     - <a href='#cloud'>Oracle Cloud Infrastructure Object Storage</a>\n",
    "     - <a href='#s3'>AWS S3</a>\n",
    "     - <a href='#adb'>Oracle Autonomous Database</a>\n",
    "     - <a href='#sql'>SQLite Database</a>\n",
    "     - <a href='#lib'>Libraries (such as Sklearn)</a>\n",
    "     - <a href='#ddf'>Dask Datasets</a>\n",
    " - <a href='#fileformat'>Loading Datasets of Various File Formats</a>\n",
    "     - <a href='#pd'>Pandas DataFrame</a>\n",
    "     - <a href='#list'>Python List</a>\n",
    "     - <a href='#dict'>Python Dictionary</a>\n",
    "     - <a href='#csv'>Comma Separated Values</a>\n",
    "     - <a href='#tsv'>Tab Separated Values</a>\n",
    "     - <a href='#delimited'>Delimited Files</a>\n",
    "     - <a href='#json'>Javascript Object Notation</a>\n",
    "     - <a href='#hdf'>Hierarchical Data Format</a>\n",
    "     - <a href='#parquet'>Parquet</a>\n",
    "     - <a href='#avro'>Avro</a>\n",
    "     - <a href='#excel'>Excel</a>\n",
    "     - <a href='#daskdataframe'>Dask Dataframe</a>\n",
    " - <a href='#buildindataset'>Using Built In Datasets</a>\n",
    "     - <a href='#datasetbrowser'>DatasetBrowser Method</a>\n",
    " - <a href='#reference'>References</a>\n",
    " \n",
    "---\n",
    " \n",
    " **Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---\n",
    "\n",
    "Datasets are provided as a convenience.  Datasets are considered third-party content and are not considered materials \n",
    "under your agreement with Oracle.\n",
    "    \n",
    "You can access the `oracle_traffic_timeseries_dataset1.csv` dataset license [here](https://oss.oracle.com/licenses/upl). \n",
    "    \n",
    "You can access the `timeseries` dataset license [here](https://github.com/dask/dask/blob/master/LICENSE.txt).\n",
    "    \n",
    "You can access the `wine` dataset license is available [here](https://github.com/scikit-learn/scikit-learn/blob/master/COPYING).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import tempfile\n",
    "import warnings\n",
    "from ads.dataset.dataset_browser import DatasetBrowser\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from os import path\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='src'></a>\n",
    "## Loading Datasets From Various Sources\n",
    "\n",
    "Loading data into ADS can be done in several different ways. Data can load from a local, network file system, Hadoop Distributed File System (HDFS), Oracle Object Storage, Amazon S3, Google Cloud Service, Azure Blob, Oracle Database, ADW, elastic search instance, NoSQL DB instance, Mongodb and many more sources. This notebook demonstrates how to do this for some of the more common data sources. However, the approach can be generalized to the other data sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='local'></a>\n",
    "### Local File Storage\n",
    "\n",
    "Files that are stored locally in the notebook environment can also be read with the same command. The notebook environment provides a number of sample datasets in the `/opt/notebooks/ads-examples/oracle_data` and `/opt/notebooks/ads-examples/3P_data` directory. `DatasetFactory.open()` understands a number of file extensions and makes best efforts to set the parameters needed to read the file. This decreases workload and reduces the number of coding errors.\n",
    "\n",
    "In the next cell, reading from a CSV file is demonstrated. However, `DatasetFactory.open()` can read from a variety of file formats, see <a href='#fileformat'>Loading Datasets of Various File Formats</a> for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = path.join(\n",
    "    \"/\",\n",
    "    \"opt\",\n",
    "    \"notebooks\",\n",
    "    \"ads-examples\",\n",
    "    \"oracle_data\",\n",
    "    \"oracle_traffic_timeseries_dataset1.csv\",\n",
    ")\n",
    "ds = DatasetFactory.from_dataframe(ds_path)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ds` variable is an object from the class `ads.dataset.dataset.ADSDataset`. Objects of this class have a method `show_in_notebook` that provides a wealth of exploratory data analysis (EDA) information. It displays summary statistics, correlations, visualizations, and warnings about the condition of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cloud'></a>\n",
    "### Oracle Cloud Infrastructure Object Storage\n",
    "\n",
    "[Oracle Cloud Infrastructure Object Storage service](https://docs.cloud.oracle.com/iaas/Content/Object/Concepts/objectstorageoverview.htm) is an internet-scale, high-performance storage platform that offers reliable and cost-efficient data durability. The Object Storage service can store an unlimited amount of structured and unstructured data. The content type does not limit it. Therefore, it can store log files, text data, images, videos, and much more.\n",
    "\n",
    "\n",
    "To retrieve data from the Object Storage service, the system uses the Oracle Cloud Infrastructure configuration file (`~/.oci/config`) or any other configuration file that is specified. If this file is not configured, see the `getting-started.ipynb` notebook example for instructions. The information in that file is used to define the tenancy, region, user, and credentials that are needed to make a secure connection to the bucket. Configuring this file is generally a one-time operation. Identifying the file to access is done through the use of a URI. Use `oci` or `ocis` as the protocol then specify the bucket name and key (filename). The URI should have the following format:\n",
    "\n",
    "```\n",
    "oci://<bucket>/<key>\n",
    "```\n",
    "\n",
    "The `DatasetFactory.open()` method is used to read in the file from the Object Storage service. In addition to the URI parameter, it takes an optional `storage_options` parameter. The `config` key sets the path to the Oracle Cloud Infrastructure configuration file that defines the tenancy, region, user and credentials to use. The `profile` key identifies what profile, within the configuration file, to use.\n",
    "\n",
    "A sample command is:\n",
    "\n",
    "```\n",
    "ds = DatasetFactory.open(\n",
    "    \"oci://my-bucket/my-favorite-dataset.csv\", \n",
    "    storage_options={\"config\": \"~/.oci/config\", \"profile\": \"DEFAULT\"}, \n",
    "    delimiter=\",\")\n",
    "```\n",
    "\n",
    "The next cell demonstrates how to access a file. It requires a bucket name, key and a properly configured configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"<bucket-name>\"\n",
    "key = \"<key>\"\n",
    "\n",
    "if bucket_name != \"<bucket-name>\" and key != \"<key>\":\n",
    "    ds = DatasetFactory.open(\n",
    "        \"oci://{}/{}\".format(bucket_name, key),\n",
    "        storage_options={\"config\": \"~/.oci/config\", \"profile\": \"DEFAULT\"},\n",
    "    )\n",
    "    ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='s3'></a>\n",
    "### AWS S3\n",
    "\n",
    "S3 is an object storage system that is provided by Amazon Web Services. To access files in S3 a URI must be passed to the `DatasetFactory.open()` method along with credential information. The credentials are passed in the `storage_options` parameter where the `key` and `secret` are given. In addition, the `client_kwargs` option specifies the endpoint that should be used to access the bucket. S3 buckets are globally distributed and this allows for the selection of an endpoint that is closest to the user.\n",
    "\n",
    "Natively, Amazon uses similar URI approach, but their URI format is different from that used by the `DatasetFactory.open()` method. Amazon uses `http://s3.amazon.com/[bucket_name]` or `http://[bucket_name].s3.amazonaws.com/`. However, `DatasetFactory.open()` uses a URI format that is consistent with access to the Object Storage service. That is, the URI starts with a protocal, which is `s3`. It is then followed by the bucket name and the key so the format is:\n",
    "\n",
    "```\n",
    "s3://<bucket>/<key>\n",
    "```\n",
    "\n",
    "A sample command is:\n",
    "\n",
    "```\n",
    "ds = DatasetFactory.open(\n",
    "    \"s3://my-bucket/my-favorite-dataset.csv\", \n",
    "    storage_options = {\n",
    "        \"key\": \"AKIAS4F2DENPM4R5KV5T\", \n",
    "        \"secret\": \"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\", \n",
    "        \"client_kwargs\": {\n",
    "            \"endpoint_url\": \"https://s3-us-west-1.amazonaws.com\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "The next cell demonstrates how to access a file. It requires a bucket name, key, and security credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"<bucket-name>\"\n",
    "key = \"<key>\"\n",
    "aws_key = \"<aws-key>\"\n",
    "aws_secret = \"<aws-secret>\"\n",
    "\n",
    "if (\n",
    "    bucket_name != \"<bucket-name>\"\n",
    "    and key != \"<key>\"\n",
    "    and aws_key != \"<aws-key>\"\n",
    "    and aws_secret != \"<aws-secret>\"\n",
    "):\n",
    "    ds = DatasetFactory.open(\n",
    "        \"s3://{}/{}\".format(bucket_name, key),\n",
    "        storage_options={\n",
    "            \"key\": aws_key,\n",
    "            \"secret\": aws_secret,\n",
    "            \"client_kwargs\": {\"endpoint_url\": \"https://s3-us-west-1.amazonaws.com\"},\n",
    "        },\n",
    "    )\n",
    "    ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adb'></a>\n",
    "### Oracle Autonomous Database\n",
    "\n",
    "The Oracle Autonomous Database (ADB) is a cloud-based database that has minimal administration requirements. There are two different configurations that are optimized for different use cases. The Autonomous Data Warehouse (ADW) and the Autonomous Transaction Processing (ATP) databases. Once the security credential configuration has been set up, an `ADSDataset` can be obtained just like any other file that is supported by the `DatasetFactory.open()` method.\n",
    "\n",
    "ADB credentials and connection information is provided in two parts. The first part comes from the ADB Wallet file. The `TNS_ADMIN` environment variable must be specified to put at `sqlnet.ora` file in the wallet directory. In addition, a URI must be defined. The protocol used is the database type plus the driver type. Specifically, this would be `oracle+cx_oracle`. The URI also includes the username and password along with the ADB consumer group (SID). The URI would look something like:\n",
    "```\n",
    "oracle+cx_oracle://admin:mypassword@mydatabase_medium'\n",
    "```\n",
    "\n",
    "In the `DatasetFactory.open()` method, the `table` parameter can list a table that is to be returned or it can be a Data Query Language command, such as SELECT, that returns a set of records. The `format='sql'` setting lets the method know that the connection is to a database.\n",
    "\n",
    "There is a notebook that details how to set up a connection to the ADB. If that connection is already configured, the following code can be run to test a connection. Revise the connection information before executing the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"<database_name>\"  # Name of the database.\n",
    "database_user = \"<database_user>\"  # User to connect to the database.\n",
    "database_password = \"<database_password>\"  # database_user's password.\n",
    "tns_admin = \"<tns_admin>\"  # Path to the sqlnet.ora file.\n",
    "sid = \"<sid>\"  # The ADB Consumer Group (SID).\n",
    "\n",
    "if (\n",
    "    database_name != \"<database_name>\"\n",
    "    and database_user != \"<database_user>\"\n",
    "    and database_password != \"<database_password>\"\n",
    "    and tns_admin != \"<tns_admin>\"\n",
    "    and sid != \"<sid>\"\n",
    "):\n",
    "\n",
    "    # Add TNS_ADMIN to the environment.\n",
    "    os.environ[\"TNS_ADMIN\"] = tns_admin\n",
    "\n",
    "    # The following assumes that the default SH namespace exists and the CUSTOMERS table is present.\n",
    "    # This exists by default in the database, but may have been removed.\n",
    "    uri = \"oracle+cx_oracle://\" + database_user + \":\" + database_password + \"@\" + sid\n",
    "    customers = DatasetFactory.open(uri, format=\"sql\", table=\"SH.CUSTOMERS\")\n",
    "    customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sql'></a>\n",
    "### SQLite Database\n",
    "\n",
    "SQLite is a fast, compact database that is based on a single file that is stored locally. `DatasetFactory.open()` uses a URI to connect to it. The protocol is `sqlite` followed by 3 forward slashes `/`. For most URIs, there are 2 forward slashes `/` after the protocol, but that is not the case in this notebook. The next component of the URI path to the file. It can be a relative or absolute path. If it is an absolute path there is 4 forward slashes `/` after the protocol.\n",
    "\n",
    "An example URI with a relative path is:\n",
    "\n",
    "```\n",
    "sqlite:///user.db\n",
    "```\n",
    "\n",
    "An absolute path has 4 forward slashes `/` after the protocol. An example URI is:\n",
    "\n",
    "```\n",
    "sqlite:////home/datascience/user.db\n",
    "```\n",
    "\n",
    "In the next cell, a dataframe is created and the `to_sql` method is used to write out an SQLite file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_file = tempfile.NamedTemporaryFile()\n",
    "engine = create_engine(\"sqlite:///\" + sqlite_file.name, echo=False)\n",
    "DatasetBrowser.sklearn().open(\"wine\").to_pandas_dataframe().to_sql(\n",
    "    \"wine\", con=engine, if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DatasetFactory.open()` method can be used to access the data in a SQLite database. If the URI has a file extension, then `DatasetFactory.open()` is smart enough to know how to process the file. However, if the extension is missing, a `format='sql'` parameter tells it how to access the data. The `table` parameter lists a table that is to be returned or it can be a Data Query Language command, such as SELECT, that returns a set of records. SQLite files also require that an index column is provided using the `index_col` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new = DatasetFactory.open(\n",
    "    \"sqlite:///\" + sqlite_file.name, format=\"sql\", table=\"wine\"\n",
    ")\n",
    "sqlite_file.close()\n",
    "ds_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fileformat'></a>\n",
    "## Loading Datasets of Various File Formats\n",
    "\n",
    "Data comes in a wide variety of file formats and the `DatasetFactory.open()` method supports the most popular of these. Above, the support for <a href='#sql'>SQLite databases</a> and <a href='#local'>CSV</a> have already been demonstrated. There is support for the following file formats: CSV, TSV, Parquet, libsvm, JSON, Excel, HDF5, SQL, xml, Apache server log files (`clf`, `log`), and ARFF. In addition, there is support for `ADSDataset`, and <a href=\"#pd\">Pandas Dataframes</a>. Support of <a href='#arr'>python arrays</a> and <a href='#dict'>dictionaries</a> are indirectly provided by first converting them to a supported data format, such as a Pandas dataframe.\n",
    "\n",
    "**Note** `DatasetFactory.open()` has been deprecated. With Pandas dataframes use `DatasetFactory.from_dataframe()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pd'></a>\n",
    "### Pandas Dataframe\n",
    "\n",
    "A Pandas dataframe can be converted to an `ADSDataset` object by passing it to the `DatasetFactory.open()` method. In the following example, a Panadas dataframe is created, `df`, and then passed to `DatasetFactory.open(df)` that returns an `ADSDataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Pandas dataframe that is to convert to an ADSDataset\n",
    "df = DatasetBrowser.sklearn().open(\"wine\").to_pandas_dataframe()\n",
    "\n",
    "# Convert the Pandas dataframe to an ADSDataset\n",
    "ds = DatasetFactory.from_dataframe(df)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you open a dataset, ADS detects data types in the dataset. The ADS `semantic` dtypes assigned to features in dataset, can be:\n",
    "* categorical\n",
    "* continuous\n",
    "* datetime\n",
    "* ordinal\n",
    "\n",
    "ADS semantic `dtypes` are based on ADS low-level `dtypes`. They match with the Pandas dtypes `object`, `int64`, `float64`, `datetime64`, `category`, and so on. When you use an `open()` statement for a dataset, ADS detects both its semantic and low-level data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pandas dataframe that is to convert to an ADSDataset\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"ID\": [1.1, 2.0, 3.0],\n",
    "        \"Name\": [\"Bob\", \"Sam\", \"Erin\"],\n",
    "        \"GPA_rounded\": [4.0, 4.0, 3.0],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print out initial dtype of a column\n",
    "print(df.dtypes[\"GPA_rounded\"])\n",
    "\n",
    "# Convert to an ADSDataset and set type of column to int32\n",
    "ds = DatasetFactory.from_dataframe(\n",
    "    df, target=\"GPA_rounded\", types={\"GPA_rounded\": \"int32\"}\n",
    ")\n",
    "\n",
    "# Print out ADS \"semantic\" dtype of a column\n",
    "print(ds.feature_types[\"GPA_rounded\"][\"type\"])\n",
    "\n",
    "# Print out ADS \"low-level\" dtype of a column\n",
    "print(ds.feature_types[\"GPA_rounded\"][\"low_level_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='list'></a>\n",
    "### Python List\n",
    "\n",
    "Support for python lists are provided indirectly by converting them to a Pandas dataframe. This can be done by calling `pd.DataFrame()`. Once a <a href='#pd'>Pandas dataframe</a> object is obtained. A call to `DatasetFactory.open()` converts the data structure to an `ADSDataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The array to be converted to an ADSDataset object\n",
    "arr = [[\"ID\", \"Name\", \"GPA\"], [1, \"Bob\", 3.7], [2, \"Sam\", 4.3], [3, \"Erin\", 2.6]]\n",
    "\n",
    "# Convert it to a Pandas dataframe\n",
    "df = pd.DataFrame(arr[1:], columns=arr[0])\n",
    "\n",
    "# Convert to an ADSDataset\n",
    "ds = DatasetFactory.open(df)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dict'></a>\n",
    "### Python Dictionary\n",
    "\n",
    "Support for python dictionaries are provided indirectly by converting them to a Pandas dataframe. This can be done by calling `pd.DataFrame()`. Once a <a href='#pd'>Pandas dataframe</a> object is obtained. A call to `DatasetFactory.open()` converts the data structure to an `ADSDataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dictionary to be converted to an ADSDataset object\n",
    "dict = {\"ID\": [1.1, 2.0, 3.0], \"Name\": [\"Bob\", \"Sam\", \"Erin\"], \"GPA\": [3.7, 4.3, 2.6]}\n",
    "\n",
    "# Convert it to a Pandas dataframe\n",
    "df = pd.DataFrame(dict)\n",
    "ds = DatasetFactory.open(df)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='csv'></a>\n",
    "### Comma Separated Values\n",
    "Comma Separated Values (CSV) files can be opened using the standard parameterless call to `DatasetFactory.open()` if the file name has a `.csv` extension. Otherwise, use the `delimiter=','` parameter or the `format='csv'` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = path.join(\n",
    "    \"/\",\n",
    "    \"opt\",\n",
    "    \"notebooks\",\n",
    "    \"ads-examples\",\n",
    "    \"oracle_data\",\n",
    "    \"oracle_traffic_timeseries_dataset1.csv\",\n",
    ")\n",
    "ds = DatasetFactory.open(file)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tsv'></a>\n",
    "### Tab Separated Values\n",
    "\n",
    "Tab separated values (TSV) files can be opened using the standard parameterless call to `DatasetFactory.open()` if the file name has a `.tsv` extension. Otherwise, use the `delimiter='\\t,'` parameter or the `format='tsv'` parameter. In this example, a TSV file is created and then read in with `DatasetFactory.open()`. The `DatasetFactory.open()` method attempts to determine the column names from the first line of the file. The `column_names` option can be used to specify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TSV file\n",
    "file = tempfile.NamedTemporaryFile()\n",
    "for i in range(5):\n",
    "    for j in range(7):\n",
    "        term = \"\\t\" if j != 6 else \"\\n\"\n",
    "        file.write(bytes(\"{}.{}\".format(i, j) + term, \"utf-8\"))\n",
    "file.flush()\n",
    "\n",
    "# Print the raw file\n",
    "file.seek(0)\n",
    "for line in file:\n",
    "    print(line.decode(\"utf-8\"))\n",
    "\n",
    "# Read in the TSV file and specify the column names.\n",
    "ds = DatasetFactory.open(\n",
    "    file.name, format=\"tsv\", column_names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    ")\n",
    "file.close()\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='delimited'></a>\n",
    "### Delimited Files\n",
    "\n",
    "CSV and TSV are the most common delimited files. However, files can have other forms of delimitation. To read them with the `DatasetFactory.open()` method, the `delimiter` parameter must be given with the delimiting value. `DatasetFactory.open()` considers all delimited files as CSV and therefore, the `format='csv'` or `format='csv'` parameter must also be specified; even though the delimiter is not a comma or tab. The `DatasetFactory.open()` will attempt to determine the column names from the first line of the file. The `column_names` option can be used to specify them, otherwise.\n",
    "\n",
    "In the next cell, a file is created that is delimited with a vertical bar, `|`, and then read in with the `DatasetFactory.open()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a delimited file with a '|' as a separator\n",
    "file = tempfile.NamedTemporaryFile()\n",
    "for i in range(5):\n",
    "    for j in range(7):\n",
    "        term = \"|\" if j != 6 else \"\\n\"\n",
    "        file.write(bytes(\"{}.{}\".format(i, j) + term, \"utf-8\"))\n",
    "file.flush()\n",
    "\n",
    "# Print the raw file\n",
    "file.seek(0)\n",
    "for line in file:\n",
    "    print(line.decode(\"utf-8\"))\n",
    "\n",
    "# Read in the delimited file and specify the column names.\n",
    "ds = DatasetFactory.open(\n",
    "    file.name, delimiter=\"|\", format=\"csv\", column_names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    ")\n",
    "file.close()\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='json'></a>\n",
    "### Javascript Object Notation\n",
    "\n",
    "Javascript Object Notation (JSON) files are supported by `DatasetFactory.open()` as long as the data can be restructured into a rectangular form. There are two formats of JSON, called orientations, that are supported. The orientation is given by `orient=index` or `orient=records`.\n",
    "\n",
    "For the index orientation, there is a single JSON object that the format of:\n",
    "```\n",
    "{\n",
    "    <index>: <value>,\n",
    "    <index>: <value>\n",
    "}\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"946684800000\": {\"id\": 982, \"name\": \"Yvonne\", \"x\": -0.3289461521, \"y\": -0.4301831275},\n",
    "    \"946684801000\": {\"id\": 1031, \"name\": \"Charlie\", \"x\": 0.9002882524, \"y\": -0.2144513329}\n",
    "}\n",
    "```\n",
    "\n",
    "For the records format, there is a collection of JSON objects. No index value is give. There is no comma between records. The format is:\n",
    "\n",
    "```\n",
    "{<key>: <value>, <key>: <value>}\n",
    "{<key>: <value>, <key>: <value>}\n",
    "```\n",
    "For example:\n",
    "```\n",
    "{\"id\": 982, \"name\": \"Yvonne\", \"x\": -0.3289461521, \"y\": -0.4301831275}\n",
    "{\"id\": 1031, \"name\": \"Charlie\", \"x\": 0.9002882524, \"y\": -0.2144513329}\n",
    "```\n",
    "\n",
    "In the next cell, a JSON file is created and then read back in with `DatasetFactory.open()`. If the file extension ends in `.json`, then the method loads it as a JSON file. If this is not the case, then use `format='json'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the JSON file that is to be read\n",
    "file = path.join(tempfile.mkdtemp(), \"wine.json\")\n",
    "DatasetBrowser.sklearn().open(\"wine\").to_json(file, orient=\"records\")\n",
    "\n",
    "# Read in the JSON file\n",
    "ds = DatasetFactory.open(file, format=\"json\", orient=\"records\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parquet'></a>\n",
    "### Parquet\n",
    "\n",
    "Parquet, is an open source file format for that is commonly used in Hadoop. `DatasetFactory.open()` can access Parquet files by setting the `format='parquet'` parameter. A directory is given to `DatasetFactory.open()` not a file. The method processes all files in that directory.\n",
    "\n",
    "In the next cell, a set of Parquet files is created, and then read back in by the `DatasetFactory.open()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Parquet file to be read\n",
    "parquet_dir = path.join(tempfile.mkdtemp(), \"wine\")\n",
    "DatasetBrowser.sklearn().open(\"wine\").to_parquet(parquet_dir)\n",
    "\n",
    "# Read in the Parquet file\n",
    "ds = DatasetFactory.open(parquet_dir, format=\"parquet\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='avro'></a>\n",
    "### Avro\n",
    "\n",
    "Avro is a remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format. `DatasetFactory.open()` can access Avro files by setting the `format='avro'` parameter. A directory is given to `DatasetFactory.open()` not a file. The method processes all files in that directory.\n",
    "\n",
    "In the next cell, a set of Avro files is created, and then read back in by the `DatasetFactory.open()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Avro file to be read\n",
    "avro_dir = \"./avro_files/my_data.avro\"\n",
    "os.makedirs(os.path.dirname(avro_dir), exist_ok=True)\n",
    "DatasetBrowser.sklearn().open(\"iris\").to_avro(avro_dir)\n",
    "\n",
    "# Read in the Avro file\n",
    "ds = DatasetFactory.open(avro_dir, format=\"avro\")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='excel'></a>\n",
    "### Excel\n",
    "\n",
    "Data scientists often have to work with Excel files as a data source. If the file extension is `.xlsx`, then `DatasetFactory.open()` automatically processes it as an Excel file. Otherwise, use `format='xlsx'`. By default, the first sheet in the file is read in. This behavior can be modified with the `sheetname` parameter. It accepts the sheet number (it is zero-indexed) or a string with the name of the sheet. \n",
    "\n",
    "`DatasetFactory.open()` reads in all columns that have values. This behavior can be modified with the `usecols` parameter. It accepts a list of column numbers to be read in, such as `usecols=[1, 3, 5]` or it can accept a range as a string, `usecols='A:C'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Excel file to read in. Put the data on a sheet called 'wine'\n",
    "file = tempfile.NamedTemporaryFile()\n",
    "writer = pd.ExcelWriter(file.name, engine=\"xlsxwriter\")\n",
    "DatasetBrowser.sklearn().open(\"wine\").to_pandas_dataframe().to_excel(\n",
    "    writer, sheet_name=\"wine\"\n",
    ")\n",
    "writer.save()\n",
    "\n",
    "# Read in the Excel file and clean up\n",
    "ds = DatasetFactory.open(file.name, format=\"xlsx\", sheet_name=\"wine\", usecols=\"A:C\")\n",
    "file.close()\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='buildindataset'></a>\n",
    "## Using Built In Datasets\n",
    "\n",
    "The Accelerated Data Science (ADS) SDK comes with a number of datasets in the `DatasetBrowser` object. These datasets have their target value set so are already objects that are derived from the `ADSDataset` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='datasetbrowser'></a>\n",
    "### DatasetBrowser Method\n",
    "\n",
    "The `DatasetBrowser` method provides access to a number of collections of datasets. A list of these dataset libraries can be obtained using the `list` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetBrowser.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these libraries has a collection of datasets. They can be accessed by calling a method with the same name as the library. For example, the `sklearn` library can be accessed with `DatasetBrowser.sklearn()`. This returns a `SklearnDatasets` object. Using the `list` method on that object provides a list of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn = DatasetBrowser.sklearn()\n",
    "sklearn.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `open` method accepts a string that is the name of the dataset. It returns an object that inherits the `ADSDataset` object and it is specific to the type of data. In this case, the `wine` dataset is a multiclass classification dataset so a `MultiClassClassificationDataset` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sklearn.open(\"wine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reference\"></a>\n",
    "# References\n",
    "\n",
    "- [ADS Library Documentation](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
