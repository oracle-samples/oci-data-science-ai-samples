{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5052c2e9",
   "metadata": {},
   "source": [
    "@notebook{read-write-big_data_service-(BDS).ipynb,\n",
    "    title: How to Read Data with fsspec from Oracle Big Data Service (BDS),\n",
    "    summary: Manage data using fsspec file system. Read and save data using pandas and pyarrow through fsspec file system.,\n",
    "    developed_on: pyspark30_p37_cpu_v5,\n",
    "    keywords: bds, fsspec,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6627d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade Oracle ADS to pick up latest features and maintain compatibility with Oracle Cloud Infrastructure.\n",
    "\n",
    "!pip install -U oracle-ads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be29b2",
   "metadata": {},
   "source": [
    "<font color=gray>Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2022 Oracle, Inc.  All rights reserved.\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>\n",
    "\n",
    "***\n",
    "# <font color=red>How to Read Data with fsspec from Oracle Big Data Service (BDS)</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Service Team </font></p>\n",
    "\n",
    "***\n",
    "\n",
    "## Overview:\n",
    "\n",
    "Oracle Big Data Service is an Oracle Cloud Infrastructure service designed for a diverse set of big data use cases and workloads. From short-lived clusters used to tackle specific tasks to long-lived clusters that manage large data lakes, Big Data Service scales to meet an organizationâ€™s requirements at a low cost and with the highest levels of security. To connect to the BDS from the notebook session, you can reference the notebook `Connect_to_the_Oracle_Big_Data_Service.ipynb`. This notebook will demonstrate how to manage data using fsspec file system, read and save data using pandas and pyarrow through fsspec file system.\n",
    "\n",
    "Compatible conda pack: [PySpark 3.0 and Data Flow](https://docs.oracle.com/en-us/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.7 (version 5.0)\n",
    "\n",
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "* <a href='#setup'>Set Up</a>\n",
    "* <a href='#fsspec'>FSSpec</a>\n",
    "    * <a href='#list'>Listing</a>\n",
    "    * <a href='#save_f'>Saving File</a>\n",
    "    * <a href='#save_folder'>Saving Folder</a>\n",
    "* <a href='#pandas'>Pandas</a>\n",
    "* <a href='#pyarrow'>PyArrow</a>\n",
    "    * <a href='#write'>Write</a>\n",
    "    * <a href='#read'>Read</a>\n",
    "    * <a href='#partition'>Partitioned Dataset</a>\n",
    "        * <a href='#write'>Write</a>\n",
    "        * <a href='#read'>Read</a>\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Datasets are provided as a convenience. Datasets are considered third-party content and are not considered materials under your agreement with Oracle.\n",
    "      \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d15b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ads\n",
    "import fsspec\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ads.secrets.big_data_service import BDSSecretKeeper\n",
    "from ads.bds.auth import has_kerberos_ticket, krbcontext\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.WARN)\n",
    "\n",
    "ads.set_auth(\"resource_principal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201600d",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "# Setup\n",
    "\n",
    "The following assumes that you have already saved your configuration with `BDSSecretKeeper` so that you can use `BDSSecretKeeper.load_secret` to load the configuration. To see how to connect without Vault or how to save configuration with `BDSSecretKeeper`, see the `Connect_to_the_Oracle_Big_Data_Service.ipynb.ipynb` notebook. In the next cell, you can replace `bds_secret_id` with your secret OCID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ca79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bds_secret_id = \"<bds-secret-id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd17626",
   "metadata": {},
   "source": [
    "<a id='fsspec'></a>\n",
    "## FSSpec\n",
    "\n",
    "`FSSpec` provides a pythonic interface to local, remote and embedded file systems.  This notebook shows you how to use it to work with files in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b31716",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bds_secret_id != \"<bds-secret-id>\":\n",
    "    with BDSSecretKeeper.load_secret(bds_secret_id) as cred:\n",
    "        with krbcontext(principal=cred[\"principal\"], keytab_path=cred[\"keytab_path\"]):\n",
    "            hdfs_config = {\n",
    "                \"protocol\": \"webhdfs\",\n",
    "                \"host\": cred[\"hdfs_host\"],\n",
    "                \"port\": cred[\"hdfs_port\"],\n",
    "                \"kerberos\": \"True\",\n",
    "            }\n",
    "    print(hdfs_config)\n",
    "else:\n",
    "    hdfs_config = None\n",
    "    print(\n",
    "        \"The secret OCID, bds_secret_id, is not defined. Enter configuration values in the Setup section.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebc877",
   "metadata": {},
   "source": [
    "Instantiate filesystem for `hdfs_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe36406",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    fs = fsspec.filesystem(**hdfs_config)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a85bb",
   "metadata": {},
   "source": [
    "<a id='list'></a>\n",
    "\n",
    "After HDFS is instantiated, you can view and work with the files in the filesystem.\n",
    "#### Listing\n",
    "List objects at root path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    print(fs.ls(\"/\"))\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39be800",
   "metadata": {},
   "source": [
    "You can also specify a path and list the objects in it.   For example, the next cell shows how to list objects in the `/data/biketrips` path. You can replace `file_path` with the path that you want to list the objects from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b064bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/data/biketrips\"\n",
    "if hdfs_config is not None and file_path != \"\":\n",
    "    print(fs.ls(file_path))\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a14910",
   "metadata": {},
   "source": [
    "<a id='save_f'></a>\n",
    "#### Saving File\n",
    "You can download files from the HDFS file system to the local directory using `get` using `rpath` to specify the path of the source data. `lpath` defines where to save this data. The next cell saves the `/data/biketrips/JC-201901-citibike-tripdata.csv` file to the local directory, `./JC-201901-citibike-tripdata.csv`. You can replace `hdfs_data_path` and `dest_path` with your HDFS file system and destination information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73640a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_data_path = \"/data/biketrips/JC-201901-citibike-tripdata.csv\"\n",
    "dest_path = \"./JC-201901-citibike-tripdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    fs.get(rpath=hdfs_data_path, lpath=dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae46b4f",
   "metadata": {},
   "source": [
    "<a id='save_folder'></a>\n",
    "#### Saving Folder\n",
    "You can also copy a folder from the HDFS file system to the local directory. If lpath\n",
    "ends with a \"/\", it is assumed to be a directory and target files\n",
    "will go within. You can pass in a list of paths, which may be glob-patterns\n",
    "are expanded. You can replace `source_directory_1`, `source_directory_2`, and `dest_directory` with your HDFS file system directories and local directory information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory_1 = \"/data/biketrip*/\"\n",
    "source_directory_2 = \"/data/station*/\"\n",
    "dest_directory = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604eebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    fs.get([source_directory_1, source_directory_2], dest_directory, recursive=True)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb2685",
   "metadata": {},
   "source": [
    "<a id='pandas'></a>\n",
    "## Pandas\n",
    "You can also open the data using Pandas through fsspec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ae794",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    with fs.open(hdfs_data_path, \"r\") as f:\n",
    "        df = pd.read_csv(f)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cb9fd",
   "metadata": {},
   "source": [
    "Call `to_csv` to save to a local directory. You can replace `csv_dest_path` with the destination path you want to save the file to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dest_path = \"./tripdata_example1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    df.to_csv(csv_dest_path)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b943de",
   "metadata": {},
   "source": [
    "Since pandas has integration with fsspec, you can also use pandas directly.\n",
    "\n",
    "There are two ways to pass parameters to the backend file system driver. One way is to extend the URL to include the username, password, server, port, etc. and provide the storage_options. For example, `protocol://host:port/path/to/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    df = pd.read_csv(\n",
    "        f\"webhdfs://{hdfs_config['host']}:{hdfs_config['port']}/{hdfs_data_path}\",\n",
    "        storage_options={\"kerberos\": \"True\"},\n",
    "    )\n",
    "    display(df.head())\n",
    "    df.to_csv(csv_dest_path)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6e780",
   "metadata": {},
   "source": [
    "Call `to_csv` to save to the local directory. You can replace `csv_dest_path` with the destination path you want to save the file to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060be25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    df.to_csv(csv_dest_path)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65744f5",
   "metadata": {},
   "source": [
    "The second method is more general, protocol://path/to/data and pass a dictionary of parameters to storage_options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eae00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    df = pd.read_csv(f\"webhdfs://{hdfs_data_path}\", storage_options=hdfs_config)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d4884",
   "metadata": {},
   "source": [
    "Again, you can call `to_csv` to save the file locally. You can replace `csv_dest_path` with the destination path you want to save the file to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dest_path = \"./tripdata_example3.csv\"\n",
    "if hdfs_config is not None:\n",
    "    df.to_csv(csv_dest_path)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e23b5",
   "metadata": {},
   "source": [
    "<a id='pyarrow'></a>\n",
    "## PyArrow\n",
    "\n",
    "Apache Arrow is a cross-language development platform for in-memory analytics. It is the Python implementation for Arrow which contains a set of technologies that enable big data systems to store, process and move data fast. For more details, check this [link](https://arrow.apache.org/docs/python/index.html#:~:text=This%20is%20the%20documentation%20of,process%20and%20move%20data%20fast).\n",
    "\n",
    "\n",
    "You can customize the connection parameters, create a file system object, and then pass it to the `filesystem` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a731a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "\n",
    "if hdfs_config is not None:\n",
    "    ds = ds.dataset(hdfs_data_path, format=\"csv\", filesystem=fs)\n",
    "    display(ds.to_table().to_pandas().head())\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d04f55",
   "metadata": {},
   "source": [
    "<a id='write'></a>\n",
    "### Write\n",
    "Call `write_table` to write back to a HDSF file system on BDS using `fsspec`. You can replace the `parquet_file_name`, `table_path`, and `dest_hdfs_path` values with the values you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f25f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_name = \"biketrips1.parquet\"\n",
    "table_path = \"/data/test/parquet\"\n",
    "dest_hdfs_path = f\"{table_path}/{parquet_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    pq.write_table(ds.to_table(), dest_hdfs_path, filesystem=fs)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6a5ca",
   "metadata": {},
   "source": [
    "<a id='read'></a>\n",
    "### Read\n",
    "Call `read_table` to load the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5871fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    display(pq.read_table(f\"{table_path}/\", filesystem=fs).to_pandas().head())\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c479ae",
   "metadata": {},
   "source": [
    "Remove the added parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad97687",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    fs.rm(dest_hdfs_path)\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d2918",
   "metadata": {},
   "source": [
    "<a id='partition'></a>\n",
    "### Partitioned Dataset\n",
    "\n",
    "You can write a partitioned dataset to HDFS file system using [PyArrow](https://arrow.apache.org/docs/python/parquet.html#partitioned-datasets-multiple-files). First create a Pandas DataFrame and load it into PyArrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range(\"2022-01-01 12:00:00.000\", \"2022-03-01 12:00:00.000\", freq=\"T\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"numeric_col\": np.random.rand(len(idx)),\n",
    "        \"string_col\": pd._testing.rands_array(8, len(idx)),\n",
    "    },\n",
    "    index=idx,\n",
    ")\n",
    "df[\"dt\"] = df.index\n",
    "df[\"dt\"] = df[\"dt\"].dt.date\n",
    "\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381f45c",
   "metadata": {},
   "source": [
    "<a id='write_p'></a>\n",
    "### Write\n",
    "\n",
    "Call `write_to_dataset` to write the partitioned data. The keyword `root_path` specifies the root directory on the HDFS where the data is saved. `partition_cols` takes a list of column names by which to partition the dataset The flavor='spark' option sets these constraints on the types of Parquet files that it reads automatically, and also sanitizes field characters unsupported by Spark SQL. You can replace `root_path` with your path name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = f\"{table_path}/partitioned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    pq.write_to_dataset(\n",
    "        table, root_path=root_path, partition_cols=[\"dt\"], flavor=\"spark\", filesystem=fs\n",
    "    )\n",
    "\n",
    "    # Check if the data is successfully written.\n",
    "    print(fs.ls(root_path))\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ceb8d",
   "metadata": {},
   "source": [
    "<a id='read_p'></a>\n",
    "### Read\n",
    "\n",
    "Call `read_table` to load the partitioned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff49766",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hdfs_config is not None:\n",
    "    display(pq.read_table(root_path, filesystem=fs).to_pandas().head())\n",
    "else:\n",
    "    print(\"hdfs config is not specified.  Provide value to complete setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f332dbfc",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a>\n",
    "# References\n",
    "\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
