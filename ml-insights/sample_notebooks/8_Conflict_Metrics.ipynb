{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ML Insights Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case  \n",
    "\n",
    "This Notebook shows how to configure following Conflict Metrics using ML Insights .\n",
    "\n",
    "- ConflictPrediction : Computes Conflict Prediction metric based on the input features and predict feature .This   metric calculates the number of times a model gave conflicting output results for the same set of input features . This is an approximate metric.\n",
    "    \n",
    "- ConflictLabel : Computes Conflict Label metric based on the input features and target feature .This metric calculates and return the number of times dataset has different label values for the same set of input features . This is an approximate metric.\n",
    "\n",
    "## Note\n",
    "- Conflict metrics are approximate metric.\n",
    "- Conflict metrics needs to have target and prediction features in feature schema. This includes:\n",
    "    - Column Type as Target for ground truth column ,If these columns are missing or not configured, Insights throw validation errors\n",
    "    - Column Type as Prediction for prediction column ,If these columns are missing or not configured, Insights throw validation errors\n",
    "- All Conflict metrics can be view using following Profile API:\n",
    "    - to_json\n",
    "\n",
    "### About Dataset \n",
    "\n",
    "The data set contains data that has been collected across various property real estate aggregators. It consists of various features such as Floor, LotSize, LotArea, SalePrice, SaleTarget .\n",
    "\n",
    "Dataset Source : https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install ML Observability Insights Library SDK\n",
    "\n",
    "- Prerequisites\n",
    "    - Linux/Mac (Intel CPU)\n",
    "    - Python 3.8 and 3.9 only\n",
    "\n",
    "\n",
    "- Installation\n",
    "    - ML Insights is made available as a Python package (via Artifactory) which can be installed using pip install as shown below. Depending on the execution engine on which to do the run, one can use scoped package. For eg: if we want to run on dask, use oracle-ml-insights[dask], for spark use oracle-ml-insights[spark], for native use oracle-ml-insights. One can install all the dependencies as use oracle-ml-insights[all]\n",
    "\n",
    "      !pip install oracle-ml-insights\n",
    "\n",
    "Refer : [Installation and Setup](https://docs.oracle.com/en-us/iaas/tools/ml-insights-docs/latest/ml-insights-documentation/html/user_guide/tutorials/install.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install oracle-ml-insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ML Insights Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from typing import Any\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import metrics\n",
    "from mlm_insights.core.features.feature import FeatureMetadata\n",
    "\n",
    "# Import Data Quality metrics \n",
    "from mlm_insights.core.metrics.count import Count\n",
    "\n",
    "# Import Data Set Metric\n",
    "from mlm_insights.core.metrics.rows_count import RowCount\n",
    "\n",
    "# Import Conflict metrics\n",
    "from mlm_insights.core.metrics.conflict_metrics.conflict_label import ConflictLabel\n",
    "from mlm_insights.core.metrics.conflict_metrics.conflict_prediction import ConflictPrediction\n",
    "\n",
    "from mlm_insights.builder.builder_component import MetricDetail, EngineDetail\n",
    "from mlm_insights.constants.types import FeatureType, DataType, VariableType, ColumnType\n",
    "from mlm_insights.core.metrics.metric_metadata import MetricMetadata\n",
    "\n",
    "# import data reader\n",
    "from mlm_insights.core.data_sources import LocalDatePrefixDataSource\n",
    "from mlm_insights.mlm_native.readers import CSVNativeDataReader\n",
    "\n",
    "# import post processor \n",
    "from mlm_insights.core.post_processors.local_writer_post_processor import LocalWriterPostProcessor\n",
    "from mlm_insights.builder.insights_builder import InsightsBuilder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Configure Feature schema\n",
    "\n",
    "Feature Schema defines the structure and metadata of the input data, which includes data type, column type, column mapping . The framework, uses this information as the ground truth and any deviation in the actual data is taken as an anomaly and the framework usually will ignore such all such anomaly in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_schema():\n",
    "    return {\n",
    "        \"Floor\": FeatureType(data_type=DataType.INTEGER, variable_type=VariableType.CONTINUOUS, column_type=ColumnType.INPUT),\n",
    "        \"LotSize\": FeatureType(data_type=DataType.INTEGER, variable_type=VariableType.CONTINUOUS,column_type=ColumnType.INPUT),\n",
    "        \"LotArea\": FeatureType(data_type=DataType.INTEGER, variable_type=VariableType.CONTINUOUS, column_type=ColumnType.INPUT),\n",
    "        \"SalePrice\": FeatureType(data_type=DataType.INTEGER, variable_type=VariableType.CONTINUOUS, column_type = ColumnType.PREDICTION),\n",
    "        \"SaleTarget\": FeatureType(data_type=DataType.INTEGER, variable_type=VariableType.CONTINUOUS, column_type = ColumnType.TARGET)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Configure Metrics\n",
    "\n",
    "Metrics are the core construct for the framework. This component is responsible for calculating all statistical metrics and algorithms. Metric components work based on the type of features (eg. input feature, output feature etc.) available, their data type (eg. int, float, string etc.) as well as additional context (e.g. if any previous computation is available to compare against). ML Insights provides commonly used metrics out of the box for different ML observability use cases.\n",
    "\n",
    "Refer : [Metrics Component Documentation](https://docs.oracle.com/en-us/iaas/tools/ml-insights-docs/latest/ml-insights-documentation/html/user_guide/getting_started/metrics_component.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    metrics = [\n",
    "               MetricMetadata(klass=Count)\n",
    "              ]\n",
    "    uni_variate_metrics = {\n",
    "        \"SalePrice\": metrics,\n",
    "        \"SaleTarget\": metrics\n",
    "    }\n",
    "    \n",
    "    dataset_metrics = [MetricMetadata(klass=RowCount),\n",
    "                       MetricMetadata(klass=ConflictPrediction),\n",
    "                       MetricMetadata(klass=ConflictLabel)]\n",
    "    metric_details = MetricDetail(univariate_metric=uni_variate_metrics,\n",
    "                                  dataset_metrics=dataset_metrics)\n",
    "    return metric_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Configure Data Reader\n",
    "\n",
    "Data Reader allows for ingestion of raw data into the framework. This component is primarily responsible for understanding different formats of data (e.g. jsonl, csv) etc. and how to properly read them. At its essence, the primary responsibility of this component is that given a set of valid file locations which represents file of a specific type, reader can properly decode the content and load them in memory.\n",
    "\n",
    "Additionally, Data Source component is an optional subcomponent, which is usually used along side the Reader. The primary responsibility of the data source component is to embed logic on filtering and partitioning of files to be read by the framework.\n",
    "\n",
    "Refer : [Data Reader Documentation](https://docs.oracle.com/en-us/iaas/tools/ml-insights-docs/latest/ml-insights-documentation/html/user_guide/getting_started/data_reader_component.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_reader(start_date, end_date):\n",
    "    \n",
    "    # Define Data Format\n",
    "    data = {\n",
    "        \"file_type\": \"csv\",\n",
    "        \"date_range\": {\"start\": start_date, \"end\": end_date}\n",
    "    }\n",
    "    \n",
    "    # Define Data Location\n",
    "    base_location =\"input_data/house_price_prediction_dataset\"\n",
    "    \n",
    "    # Create new Dataset\n",
    "    ds = LocalDatePrefixDataSource(base_location, **data)\n",
    "    \n",
    "    # Load Dataset\n",
    "    csv_reader = CSVNativeDataReader(data_source=ds)\n",
    "    \n",
    "    \n",
    "    return csv_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Compute the Profile \n",
    "\n",
    "Create the builder object which provides core set of api, using which user can set the behavior of their monitoring. By selecting what components and variants to run all aspects of the monitoring task can be customised and configured. \n",
    "\n",
    "The run() method is responsible to run the internal workflow. It also handles the life cycle of each component passed, which includes creation (if required), invoking interface functions, destroying etc . Additionally, runner also handles some more advanced operations like thread pooling, compute engine abstraction etc.\n",
    "\n",
    "Refer : [Builder Object Documentation](https://docs.oracle.com/en-us/iaas/tools/ml-insights-docs/latest/ml-insights-documentation/html/user_guide/getting_started/builder_object.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ML Monitoring Metrics\n",
    "def run_evaluation(start_date, end_date, output_location, output_file):    \n",
    "    \n",
    "    # Set up the insights builder by passing: input schema, metric, reader and engine details\n",
    "    runner = InsightsBuilder(). \\\n",
    "        with_input_schema(get_input_schema()). \\\n",
    "        with_metrics(metrics=get_metrics()). \\\n",
    "        with_reader(reader=get_data_reader(start_date, end_date)). \\\n",
    "        with_post_processors(post_processors=[LocalWriterPostProcessor(file_location=output_location, file_name=output_file)]). \\\n",
    "        build()\n",
    "\n",
    "    # Run the Evaluation of Metrics\n",
    "    run_result = runner.run()\n",
    "    \n",
    "    return run_result.profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Baseline Dates\n",
    "base_start_date = '2023-06-26'\n",
    "base_end_date = '2023-06-27'\n",
    "\n",
    "# Execute Base Profile - Pass in Data Start, Data End, Output Location, Output File\n",
    "profile = run_evaluation(base_start_date, base_end_date, 'output_data/profiles', 'conflict_metrics_profile.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Profile Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Visualize the Profile in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count.total_count</th>\n",
       "      <th>Count.missing_count</th>\n",
       "      <th>Count.missing_count_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SalePrice</th>\n",
       "      <td>1770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SaleTarget</th>\n",
       "      <td>1770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Count.total_count  Count.missing_count  \\\n",
       "SalePrice              1770.0                  0.0   \n",
       "SaleTarget             1770.0                  0.0   \n",
       "\n",
       "            Count.missing_count_percentage  \n",
       "SalePrice                              0.0  \n",
       "SaleTarget                             0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Visualize the Profile in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ConflictLabel\": {\n",
      "        \"metadata\": {},\n",
      "        \"metric_data\": [\n",
      "            7\n",
      "        ],\n",
      "        \"metric_description\": \"Computes Conflict Label metric based on the input features and target feature. This metric calculates and returns the number of times dataset has different label values for the same set of input features . This is an approximate metric\",\n",
      "        \"metric_name\": \"ConflictLabel\",\n",
      "        \"variable_count\": 1,\n",
      "        \"variable_dimensions\": [\n",
      "            0\n",
      "        ],\n",
      "        \"variable_dtypes\": [\n",
      "            \"INTEGER\"\n",
      "        ],\n",
      "        \"variable_names\": [\n",
      "            \"conflict_label_count\"\n",
      "        ],\n",
      "        \"variable_types\": [\n",
      "            \"DISCRETE\"\n",
      "        ]\n",
      "    },\n",
      "    \"ConflictPrediction\": {\n",
      "        \"metadata\": {},\n",
      "        \"metric_data\": [\n",
      "            2\n",
      "        ],\n",
      "        \"metric_description\": \"Computes Conflict Prediction metric based on the input features and predict feature .This metric calculates the number of times a model gave conflicting output results for the same set of input features . This is an approximate metric.\",\n",
      "        \"metric_name\": \"ConflictPrediction\",\n",
      "        \"variable_count\": 1,\n",
      "        \"variable_dimensions\": [\n",
      "            0\n",
      "        ],\n",
      "        \"variable_dtypes\": [\n",
      "            \"INTEGER\"\n",
      "        ],\n",
      "        \"variable_names\": [\n",
      "            \"conflict_prediction_count\"\n",
      "        ],\n",
      "        \"variable_types\": [\n",
      "            \"DISCRETE\"\n",
      "        ]\n",
      "    },\n",
      "    \"RowCount\": {\n",
      "        \"metadata\": {},\n",
      "        \"metric_data\": [\n",
      "            1770.0\n",
      "        ],\n",
      "        \"metric_description\": \"Dataset-level Metric to compute the total row count of the dataset\",\n",
      "        \"metric_name\": \"RowCount\",\n",
      "        \"variable_count\": 1,\n",
      "        \"variable_dimensions\": [\n",
      "            0\n",
      "        ],\n",
      "        \"variable_dtypes\": [\n",
      "            \"INTEGER\"\n",
      "        ],\n",
      "        \"variable_names\": [\n",
      "            \"rows_count\"\n",
      "        ],\n",
      "        \"variable_types\": [\n",
      "            \"DISCRETE\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "profile_json = profile.to_json()\n",
    "dataset_metrics = profile_json['dataset_metrics']\n",
    "print(json.dumps(dataset_metrics,sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.metric_name</th>\n",
       "      <td>ConflictPrediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.metric_description</th>\n",
       "      <td>Computes Conflict Prediction metric based on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.variable_count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.variable_names</th>\n",
       "      <td>[conflict_prediction_count]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.variable_types</th>\n",
       "      <td>[DISCRETE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.variable_dtypes</th>\n",
       "      <td>[INTEGER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.variable_dimensions</th>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictPrediction.metric_data</th>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.metric_name</th>\n",
       "      <td>RowCount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.metric_description</th>\n",
       "      <td>Dataset-level Metric to compute the total row ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.variable_count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.variable_names</th>\n",
       "      <td>[rows_count]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.variable_types</th>\n",
       "      <td>[DISCRETE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.variable_dtypes</th>\n",
       "      <td>[INTEGER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.variable_dimensions</th>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RowCount.metric_data</th>\n",
       "      <td>[1770.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.metric_name</th>\n",
       "      <td>ConflictLabel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.metric_description</th>\n",
       "      <td>Computes Conflict Label metric based on the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.variable_count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.variable_names</th>\n",
       "      <td>[conflict_label_count]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.variable_types</th>\n",
       "      <td>[DISCRETE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.variable_dtypes</th>\n",
       "      <td>[INTEGER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.variable_dimensions</th>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConflictLabel.metric_data</th>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        0\n",
       "ConflictPrediction.metric_name                                         ConflictPrediction\n",
       "ConflictPrediction.metric_description   Computes Conflict Prediction metric based on t...\n",
       "ConflictPrediction.variable_count                                                       1\n",
       "ConflictPrediction.variable_names                             [conflict_prediction_count]\n",
       "ConflictPrediction.variable_types                                              [DISCRETE]\n",
       "ConflictPrediction.variable_dtypes                                              [INTEGER]\n",
       "ConflictPrediction.variable_dimensions                                                [0]\n",
       "ConflictPrediction.metric_data                                                        [2]\n",
       "RowCount.metric_name                                                             RowCount\n",
       "RowCount.metric_description             Dataset-level Metric to compute the total row ...\n",
       "RowCount.variable_count                                                                 1\n",
       "RowCount.variable_names                                                      [rows_count]\n",
       "RowCount.variable_types                                                        [DISCRETE]\n",
       "RowCount.variable_dtypes                                                        [INTEGER]\n",
       "RowCount.variable_dimensions                                                          [0]\n",
       "RowCount.metric_data                                                             [1770.0]\n",
       "ConflictLabel.metric_name                                                   ConflictLabel\n",
       "ConflictLabel.metric_description        Computes Conflict Label metric based on the in...\n",
       "ConflictLabel.variable_count                                                            1\n",
       "ConflictLabel.variable_names                                       [conflict_label_count]\n",
       "ConflictLabel.variable_types                                                   [DISCRETE]\n",
       "ConflictLabel.variable_dtypes                                                   [INTEGER]\n",
       "ConflictLabel.variable_dimensions                                                     [0]\n",
       "ConflictLabel.metric_data                                                             [7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.json_normalize(dataset_metrics).T.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "06b89612a5e9c675d881f7c391886fce9eabd2126328a7f9c136f634c360fd8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
