{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f4ab9cd8",
   "metadata": {},
   "source": [
    "@notebook{automlx-forecasting.ipynb,\n",
    "    title: Building a Forecaster using AutoMLx,\n",
    "    summary: Use Oracle AutoMLx to build a forecast model with real-world data sets.,\n",
    "    developed on: automlx_p38_cpu_v2,\n",
    "    keywords: language services, string manipulation, regex, regular expression, natural language processing, NLP, part-of-speech tagging, named entity recognition, sentiment analysis, custom plugins,\n",
    "    license: Universal Permissive License v 1.0,\n",
    "    original source: https://github.com/oracle-samples/automlx/blob/main/demos/OracleAutoMLx_Forecasting.ipynb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504bac5",
   "metadata": {},
   "source": [
    "***\n",
    "# <font color=red>Building a Forecaster using AutoMLx</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle AutoMLx Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973918ef",
   "metadata": {},
   "source": [
    "AutoMLx Forecasting Demo version 23.1.1.\n",
    "\n",
    "Copyright (c) 2023 Oracle, Inc.  \n",
    "\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5131c6",
   "metadata": {},
   "source": [
    "## Overview of this Notebook\n",
    "\n",
    "In this notebook we will build a forecaster using the Oracle AutoMLx tool for three real-world datasets. We explore the various options available in the Oracle AutoMLx Forecasting module, allowing the user to control the AutoML training process. We finally evaluate the statistical forecasting algorithms using in-built visualization tools. Note that contrary to other tasks like Classification, Regression or Anomaly detection, the AutoML package does not yet support explainability for forecasting tasks.\n",
    "\n",
    "---\n",
    "## Prerequisites\n",
    "\n",
    "  - Experience level: Novice (Python and Machine Learning)\n",
    "  - Professional experience: Some industry exprience\n",
    "\n",
    "Compatible conda pack: [Oracle AutoML and Model Explanation for Python 3.8 (version 2.0)](oci://service-conda-packs@id19sfcrra6z/service_pack/cpu/Oracle_AutoML_and_Model_Explanation_for_Python_3.8/2.0/automlx_p38_cpu_v2)\n",
    "\n",
    "---\n",
    "\n",
    "## Business Use\n",
    "\n",
    "Forecasting uses historical time series data as input to make informed estimates of future trends. Learning accurate statistical forecasting model requires expertise in data science and statistics. This process typically comprises of: \n",
    "- Preprocess dataset (clean, impute, engineer features, normalize).\n",
    "- Pick an appropriate model for the given dataset and prediction task at hand.\n",
    "- Tune the chosen modelâ€™s hyperparameters for the given dataset.\n",
    "\n",
    "These steps are significantly time consuming and heavily rely on data scientist expertise. Unfortunately, to make this problem harder, the best feature subset, model, and hyperparameter choice widely varies with the dataset and the prediction task. Hence, there is no one-size-fits-all solution to achieve reasonably good model performance. Using a simple Python API, AutoML can quickly (faster) jump-start the datascience process with an accurately-tuned model and appropriate features for a given prediction task.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- <a href='#setup'>0. Setup</a>\n",
    "- <a href='#univariate'>1. Univariate time series\n",
    "    - <a href='#load-m4'>1.1. Load the M4 Forecasting Competition dataset\n",
    "    - <a href='#task'>1.2. Split data into train and test for the forecasting task</a>\n",
    "    - <a href='#Engine'>1.3. Set the AutoML engine</a>\n",
    "    - <a href='#provider'>1.4. Create an instance of Oracle AutoMLx</a>\n",
    "    - <a href='#default'>1.5. Train a forecasting model using AutoMLx</a>\n",
    "    - <a href='#forecast'>1.6. Generate and visualize forecasts</a>\n",
    "    - <a href='#analysis'>1.7. Analyze the AutoML optimization process</a>\n",
    "        - <a href='#algorithm-selection'>1.7.1 Algorithm Selection</a>\n",
    "        - <a href='#hyperparameter-tuning'>1.7.2 Hyperparameter Tuning</a>\n",
    "    - <a href='#load-data-air'>1.8. Load the Airline Dataset</a>\n",
    "    - <a href='#scoringstr'>1.9. Specify a different score metric for optimization</a>\n",
    "    - <a href='#WFCV'>1.10. Specify the number of cross-validation (CV) folds</a>\n",
    "- <a href='#multi'>2. Multivariate time series</a>\n",
    "    - <a href='#multi-generating'>2.1. Generate the data</a>\n",
    "    - <a href='#multi-fitting'>2.2. Train a model using Oracle AutoMLx</a>\n",
    "    - <a href='#multi-making'>2.3. Make predictions</a>\n",
    "    - <a href='#multi-visualization'>2.4. Visualization</a>\n",
    "- <a href='#ref'>References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9808160",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup\n",
    "\n",
    "Basic setup for the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002e1f0",
   "metadata": {},
   "source": [
    "Load the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72229662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.datasets import load_airline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "plt.rcParams['font.size'] = 15\n",
    "sns.set(color_codes=True)\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_palette(\"bright\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import automl\n",
    "from automl import init\n",
    "from automl.interface.utils import plot_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736afe8",
   "metadata": {},
   "source": [
    "<a id='univariate'></a>\n",
    "# Univariate time series\n",
    "The Oracle AutoMLx solution for forecasting can process both univariate and multivariate time series. We start by displaying an example of use for univariate time series, and will adress multivariate data at the end of this notebook.<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b29d8",
   "metadata": {},
   "source": [
    "<a id='load-m4'></a>\n",
    "### Load the M4 Forecasting Competition dataset \n",
    "\n",
    "\n",
    "We fetch a  univariate timeseries from the repository of the [M4 forecasting competition](https://mofc.unic.ac.cy/m4/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badafbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_url = \"https://github.com/Mcompetitions/M4-methods/raw/master/Dataset/Train/Weekly-train.csv\"\n",
    "m4_metadata_url = \"https://github.com/Mcompetitions/M4-methods/raw/master/Dataset/M4-info.csv\"\n",
    "\n",
    "all_series = pd.read_csv(m4_url, index_col=0)  # consists of thousands of series\n",
    "metadata_csv = pd.read_csv(m4_metadata_url, index_col=0)  # describes their datetime index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ffab0",
   "metadata": {},
   "source": [
    "We select a series from the finance sector with weekly collection frequency. M4 dataset requires additional preprocessing to reconstruct the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_id = 'W142'\n",
    "series_metadata = metadata_csv.loc[series_id]\n",
    "series_values = all_series.loc[series_id]\n",
    "\n",
    "# drop NaNs for the time period where data wasn't recorded\n",
    "series_values.dropna(inplace=True)\n",
    "\n",
    "# retrieve starting date of recording and series length to generate the datetimeindex\n",
    "start_date = pd.to_datetime(series_metadata.StartingDate)\n",
    "future_dates = pd.date_range(start=start_date,\n",
    "                             periods=len(series_values),\n",
    "                             freq='W', closed=None)\n",
    "y = pd.DataFrame(series_values.to_numpy(),\n",
    "                 index=future_dates,\n",
    "                 columns=[(series_metadata.category+\"_\"+series_id)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad03336",
   "metadata": {},
   "source": [
    "We can now visualize the last 200 weeks of data we have on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.tail(n=200)  # approximately 4 years of data\n",
    "y.plot(ylabel='Weekly Series '+series_id, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ecd8f",
   "metadata": {},
   "source": [
    "One must ensure that the data points are in a Pandas DataFrame, sorted in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c376d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.index)\n",
    "print(\"Time Index is\", \"\" if y.index.is_monotonic else \"NOT\", \"monotonic.\")\n",
    "print(\"Train datatype\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70120dff",
   "metadata": {},
   "source": [
    "<a id='task'></a>\n",
    "### Split data into train and test for the forecasting task\n",
    "As can be seen above, the data contains 200 weekly recorded values over the past 5 years. We will try to predict electricity consumption for the last 0.5 year of data (26 data points), using the previous years as training data. Hence, we separate the dataset into training and testing sets using Temporal train-test split, which ensures that the continuity of the input time series is preserved. Each point in the series represents a month, so we will hold out the last 26 points as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = temporal_train_test_split(y, test_size=26)\n",
    "print(\"Training length: \", len(y_train),\" Testing length: \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873eb23",
   "metadata": {},
   "source": [
    "We see that the train data ranges from December 2012 to April 2016, while the test data ranges from April to October 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204edecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train\", y_train)\n",
    "print(\"\\ny_test\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0c3af",
   "metadata": {},
   "source": [
    "<a id='Engine'></a>\n",
    "### Set the AutoML engine\n",
    "The AutoML pipeline offers the function `init`, which allows to initialize the parallel engine. By default, the AutoML pipeline uses the `dask` parallel engine. One can also set the engine to `local`, which uses python's multiprocessing library for parallelism instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76202724",
   "metadata": {},
   "outputs": [],
   "source": [
    "init(engine='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af7159",
   "metadata": {},
   "source": [
    "<a id='provider'></a>\n",
    "### Create an instance of Oracle AutoMLx\n",
    "\n",
    "The Oracle AutoMLx solution automatically provides a tuned forecasting pipeline that best models the given training dataset and a prediction task at hand. Here the dataset can be any univariate time-series.\n",
    "\n",
    "AutoML for Forecasting consists of three main modules: \n",
    "- **Preprocessing**\n",
    "    - Impute any missing values using back fill or forward fill mechanisms to ensure input has a well-defined and consistent frequency.\n",
    "    - Identify seasonalities present in the data by detrending and analyzing the Autocorrelation Function (ACF) of the series.\n",
    "    - Decide appropriate number of cross-validation (CV) folds and the forecast horizons based on the datetime frequency of data.\n",
    "- **Algorithm Selection**: Identify the right algorithm for a given dataset, choosing from the following:\n",
    "    - NaiveForecaster - Naive and Seasonal Naive method\n",
    "    - ThetaForecaster - Equivalent to Simple Exponential Smoothing (SES) with drift\n",
    "    - ExpSmoothForecaster - Holt-Winters' damped method\n",
    "    - STLwESForecaster - Seasonal Trend LOESS (locally weighted smoothing) with Exponential Smoothing substructure\n",
    "    - STLwARIMAForecaster - Seasonal Trend LOESS (locally weighted smoothing) with ARIMA substructure\n",
    "    - SARIMAForecaster - Seasonal Autoregressive Integrated Moving Average\n",
    "    - ETSForecaster - Error, Trend, Seasonality (ETS) Statespace Exponential Smoothing\n",
    "    - ProphetForecaster (optional) - Facebook Prophet. Only available if installed locally with `pip install fbprophet`\n",
    "    - OrbitForecaster (optional) - Uber Orbit model with Exogenous Variables. (Available if a supported version is installed)\n",
    "    - VARMAXForecaster - Vector AutoRegressive Moving Average with Exogenous Variables (Available for multivariate datasets)\n",
    "    - DynFactorForecaster - Dynamic Factor Models in state-space form with Exogenous Variables (Available for multivariate datasets)\n",
    "- **Hyperparameter Tuning** \n",
    "    - Find the right model parameters that maximize score for the given dataset. \n",
    "\n",
    "These pieces are readily combined into a simple AutoML pipeline which automates the entire forecasting process with minimal user input/interaction. One can then evaluate and visualize the forecast produced by the selected model, and optionally the other tuned models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12bf13",
   "metadata": {},
   "source": [
    "<a id='default'></a>\n",
    "### Train a forecasting model using Oracle AutoMLx\n",
    "\n",
    "The AutoML API is quite simple to work with. We first create an instance of the pipeline. Next, the training data is passed to the `fit()` function which successively executes the previously mentioned modules.\n",
    "\n",
    "The generated model can then be used for forecasting tasks. By default, we use the negative of symmetric mean absolute percentage error (sMAPE)  scoring metric to evaluate the model performance. The parameter `n_algos_tuned` sets the number of algorithms whose hyperparameters are fully tuned. For highest accuracy results, it is recommended to set this value to >=2 and preferably to 8 such that all models are fully tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = automl.Pipeline(task='forecasting', n_algos_tuned=4)\n",
    "est1.fit(X=None, y=y_train)\n",
    "\n",
    "print('Selected model: {}'.format(est1.selected_model_))\n",
    "print('Selected model params: {}'.format(est1.selected_model_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f8663",
   "metadata": {},
   "source": [
    "The selected model params indicate a good fit at sp (seasonal periodicity) of 52, which typically corresponds to a yearly seasonality for data that is weekly collected (i.e., there are 52 weeks in a year)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bb733",
   "metadata": {},
   "source": [
    "<a id='forecast'></a>\n",
    "### Generating and visualizing forecasts\n",
    "There are two interfaces that support generating future forecasts using the trained forecasting pipeline.\n",
    "The preferred function, `forecast()`, accepts a user-input value for the number of periods to forecast into the future, i.e., relative to the end of the training series. It also accepts a significance level to generate prediction confidence intervals (CIs). When the methods support intervals, confidence intervals at 1-alpha are generated, e.g., significance level alpha=0.05 generates 95% confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_frame = est1.forecast(periods=len(y_test), alpha=0.05)\n",
    "print(summary_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee8b14",
   "metadata": {},
   "source": [
    "The `predict(X)` interface supports absolute index-based forecasts, but does not support confidence intervals (CIs). It also downcasts the index to int64. \n",
    "\n",
    "It should be utilized only when you want to get both in-sample predictions (predictions at timestamps that are part of the training set) and out-of-sample predictions (predictions at timestamps that are not in the training set). The X variable should be an empty dataframe containing only the requested timestamps as index. Here we request 5 in-sample model fit values and 5 out-of-sample forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_index = y_train.index[-5:].union(y_test.index[:5])\n",
    "print(future_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "est1.predict(X=pd.DataFrame(index=future_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f60112",
   "metadata": {},
   "source": [
    "AutoML provides a simple one-line tool to visualize forecasts and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.interface.utils.plot_forecast(fitted_pipeline=est1, summary_frame=summary_frame, \n",
    "                                           additional_frames=dict(y_test=y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151845fb",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "### Analyze the AutoML optimization process\n",
    "During the AutoML process, a summary of the optimization process is logged. It consists of:\n",
    "- Information about the training data \n",
    "- Information about the AutoML Pipeline, such as:\n",
    "    - selected features that AutoML found to be most predictive in the training data;\n",
    "    - selected algorithm that was the best choice for this data;\n",
    "    - hyperparameters for the selected algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd4c1b",
   "metadata": {},
   "source": [
    "AutoML provides a `print_summary()` API to output all the different trials performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "est1.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a337f8d",
   "metadata": {},
   "source": [
    "We also provide the capability to visualize the results of each stage of the AutoML pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe317d0",
   "metadata": {},
   "source": [
    "<a id='algorithm-selection'></a>\n",
    "#### Algorithm Selection\n",
    "\n",
    "The plot below shows the scores predicted by Algorithm Selection for each algorithm. Since negative sMAPE is used by default, higher values (closer to zero) are better. The horizontal line shows the average score across all algorithms. Algorithms with better score than average are colored turquoise, whereas those with worse score than average are colored teal. Here we can see that the `SARIMAXForecaster` algorithm achieved the best predicted score (orange bar), and is chosen for subsequent stages of the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trial is a tuple of\n",
    "# (algorithm, no. samples, no. features, mean CV score, hyperparameters, \n",
    "# all CV scores, total CV time (s), memory usage (Gb))\n",
    "trials = est1.model_selection_trials_ \n",
    "scores = [x[3] for x in trials]\n",
    "models = [x[0] for x in trials]\n",
    "y_margin = 0.10 * (max(scores) - min(scores))\n",
    "s = pd.Series(scores, index=models).sort_values(ascending=False)\n",
    "\n",
    "colors = []\n",
    "for f in s.keys():\n",
    "    if f == '{}_AS'.format(est1.selected_model_):\n",
    "        colors.append('orange')\n",
    "    elif s[f] >= s.mean():\n",
    "        colors.append('teal')\n",
    "    else:\n",
    "        colors.append('turquoise')\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Algorithm Selection Trials\")\n",
    "ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "ax.set_ylabel(est1.inferred_score_metric[0])\n",
    "s.plot.bar(ax=ax, color=colors, edgecolor='black')\n",
    "ax.axhline(y=s.mean(), color='black', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71de92a",
   "metadata": {},
   "source": [
    "<a id='hyperparameter-tuning'></a>\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the last stage of the Oracle AutoMLx pipeline, and focuses on improving the chosen algorithm's score. We use a novel algorithm to search across many hyperparameter dimensions, and converge automatically when optimal hyperparameters are identified. Each trial in the graph below represents a particular hyperparameter combination for the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each trial is a tuple of\n",
    "# (algorithm, no. samples, no. features, mean CV score, hyperparameters, \n",
    "# all CV scores, total CV time (s), memory usage (Gb))\n",
    "trials = np.array(est1.tuning_trials_)\n",
    "scores = np.array([x[3] for x in reversed(trials)])\n",
    "finite_indexes = np.where(np.isfinite(scores))\n",
    "\n",
    "trials = trials[finite_indexes]\n",
    "scores = scores[finite_indexes]\n",
    "y_margin = 0.10 * (max(scores) - min(scores))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Hyperparameter Tuning Trials\")\n",
    "ax.set_xlabel(\"Iteration $n$\")\n",
    "ax.set_ylabel(est1.inferred_score_metric[0])\n",
    "ax.grid(color='g', linestyle='-', linewidth=0.1)\n",
    "ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "ax.plot(range(1, len(trials) + 1), scores, 'k:', marker=\"s\", color='teal', markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67caf8",
   "metadata": {},
   "source": [
    "We can also view all tuned algorithms, as well as their validation and testing performance. This provides a good sanity check for the decision making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f340de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f'### Plotting is enabled for total models tuned = {len(est1.pipelines_)}.')\n",
    "print('### Model_name\\t\\t Val_score\\t Test_score ')\n",
    "for i in range(0, len(est1.pipelines_)):\n",
    "    pipe_ = est1.pipelines_[i]\n",
    "    print(  pipe_.__dict__['selected_model_'] , \" \\t\",\n",
    "            '%.4f'%pipe_.k_results[pipe_.__dict__['selected_model_']]['best_score'],'\\t',   # validation_score\n",
    "            '%.4f'%pipe_.score(pd.DataFrame(index=y_test.index), y=y_test))            # testing_score\n",
    "    summary_frame = pipe_.forecast(len(y_test), alpha=0.05)                     # out-of-sample forecast\n",
    "    fig = automl.interface.utils.plot_forecast(fitted_pipeline=pipe_, summary_frame=summary_frame, \n",
    "                                               additional_frames=dict(y_text=y_test))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269bdab",
   "metadata": {},
   "source": [
    "<a id='load-data-air'></a>\n",
    "### Load the Airline Dataset\n",
    "The  Airline  Passenger univariate series represents  the  monthly  total  number  of  international airline passengers (in thousands) from January 1949 to December 1960. To showcase AutoML's functionality in the absence of a datetime index, we drop the datetime index and utilize the series with only an int64index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33234ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yair = pd.DataFrame(load_airline()) # Input must be a pd.DataFrame type\n",
    "yair.index = np.arange(0, len(yair)) # replace the datetime index with an integer index.\n",
    "yair.plot(ylabel='Number of Airline Passengers', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c28559",
   "metadata": {},
   "source": [
    "We will forecast the last 20% of data, using the previous years as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "yair_train, yair_test = temporal_train_test_split(yair, test_size=0.2)\n",
    "print(\"Training length: \", len(yair_train),\" Testing length: \", len(yair_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388fd60",
   "metadata": {},
   "source": [
    "<a id='scoringstr'></a>\n",
    "### Specify a different score metric for Oracle AutoMLx optimization\n",
    "The pipeline tries to maximize a given score metric, by looking at different methods and hyperparameter choices. By default, the score metric is set to negative of sMAPE. The user can also choose another metric. For the forecasting task, possible metrics are:\n",
    "'neg_sym_mean_abs_percent_error', 'neg_root_mean_squared_percent_error', 'neg_mean_abs_scaled_error', \n",
    "                            'neg_root_mean_squared_error', 'neg_mean_squared_error', 'neg_max_absolute_error', 'neg_mean_absolute_error' is accepted.\n",
    "\n",
    "\n",
    "Here, we ask AutoML to optimize for MASE ('neg_mean_abs_scaled_error'), a scale-invariant scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "est2 = automl.Pipeline(task='forecasting', n_algos_tuned=3, score_metric='neg_mean_abs_scaled_error')\n",
    "est2.fit(y=yair_train)\n",
    "\n",
    "test_score = automl.models.score.time_series_loss(est2, X=pd.DataFrame(index=yair_test.index), \n",
    "                                                  y=yair_test, scoring='neg_mean_abs_scaled_error')\n",
    "print('Selected model: {}'.format(est2.selected_model_))\n",
    "print('Selected model params: {}'.format(est2.selected_model_params_))\n",
    "print(f'Score on test data : {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.interface.utils.plot_forecast(fitted_pipeline=est2, summary_frame=est2.forecast(len(yair_test)), \n",
    "                                           additional_frames=dict(y_test=yair_test, y_train=yair_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1d443",
   "metadata": {},
   "source": [
    "<a id='WFCV'></a>\n",
    "### Specify the number of cross-validation (CV) folds\n",
    "AutoML automatically decides how many folds to create, given the length of the input series. This is dependent on the frequency and length of the series. \n",
    "In the above, the preprocessor chose to create two folds. In the following we set the number of folds to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f40841",
   "metadata": {},
   "outputs": [],
   "source": [
    "est3 = automl.Pipeline(task='forecasting')\n",
    "est3.fit(y=yair_train, cv=8)\n",
    "\n",
    "print('Selected model: {}'.format(est3.selected_model_))\n",
    "print('Selected model params: {}'.format(est3.selected_model_params_))\n",
    "print(f'Score on test data : {est3.score(pd.DataFrame(index=yair_test.index), y=yair_test)}')\n",
    "\n",
    "fig = automl.interface.utils.plot_forecast(fitted_pipeline=est3, summary_frame=est3.forecast(len(yair_test)), \n",
    "                                           additional_frames=dict(y_test=yair_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10166521",
   "metadata": {},
   "source": [
    "<a id='multi'></a>\n",
    "## Multivariate time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0ff5a",
   "metadata": {},
   "source": [
    "<a id='multi-generating'></a>\n",
    "### Generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4707e",
   "metadata": {},
   "source": [
    "We now display the use of the Oracle AutoMLx solution for multivariate timeseries. We load the 10-dimensional Lutkepohl2 dataset. We then restrict the data to 4 variables : two exogenous variables (variables that are independent on all other data variables), and two endogenous variables (variables that are dependent on some other data variables). The endogenous variables will be the target predictions of the pipeline, while the exogenous variables will be used solely as explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d19023",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = sm.datasets.webuse('lutkepohl2', 'https://www.stata-press.com/data/r12/')\n",
    "dta.index = dta.qtr\n",
    "dta.index.freq = dta.index.inferred_freq\n",
    "endog = dta.loc['1960-04-01':'1978-10-01', ['dln_inv', 'dln_inc',]]\n",
    "exog = dta.loc['1960-04-01':'1978-10-01', ['dln_consump']]\n",
    "exog = sm.add_constant(exog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1db6c",
   "metadata": {},
   "source": [
    "We then split it using a temporal train-test split as done previously. Note that $X$ consists of the exogenous variables ('dln_consump' and another constant variable) while $y$ are the target variables ('dln_in' and 'dln_inc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4149ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df, X_test_df = temporal_train_test_split(exog, train_size=0.9)\n",
    "y_train_df, y_test_df = temporal_train_test_split(endog, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73422b",
   "metadata": {},
   "source": [
    "<a id='multi-fitting'></a>\n",
    "### Train a model using Oracle AutoMLx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c4d39",
   "metadata": {},
   "source": [
    "We can now fit the AutoML pipeline. For the multivariate forecasting task, the pipeline only considers two models : `VARMAX` and `DynFactor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5754907",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = automl.Pipeline(task='forecasting',\n",
    "                           n_algos_tuned=1,\n",
    "                           score_metric='neg_sym_mean_abs_percent_error')\n",
    "\n",
    "pipeline.fit(X=X_train_df, y=y_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e8b68",
   "metadata": {},
   "source": [
    "The AutoML pipeline provides attributes to get the selected features, the chosen model, hyperparameters as well as the score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60067cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selected features: {}'.format(pipeline.selected_features_))\n",
    "print('Ranked models: {}'.format(pipeline.ranked_models_))\n",
    "print('Selected model: {}'.format(pipeline.selected_model_))\n",
    "print('Selected model params: {}'.format(pipeline.selected_model_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11f1e2",
   "metadata": {},
   "source": [
    "<a id='multi-making'></a>\n",
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d962e7",
   "metadata": {},
   "source": [
    "As mention in the univariate data case, there are two ways of making a prediction : \n",
    "- `forecast(k)` allows one to predict k steps after the end of the training data. It should be used when one wants to make out-of-sample predictions\n",
    "- `predict(X)` returns predictions at the timestamps given as argument. It should be used when one wants to make in-sample predictions and out-of-sample predictions. It does not support confidence intervals.\n",
    "\n",
    "In the cell below `predict()` is used on the last 5 timestamps of the train set, and all timestamps of the test set. `forecast()` is used to predict k steps after the training set, where k is the size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(pd.concat([X_train_df[-5:0],X_test_df], axis=0) )\n",
    "y_forecast = pipeline.forecast(len(y_test_df), alpha=0.8, X=X_test_df)    # out-of-sample forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a673f3",
   "metadata": {},
   "source": [
    "The obtained forecast contains predictions for the two target variables, as well as lower and upper confidence intervals, for each timestamp in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebe883",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31f3eb",
   "metadata": {},
   "source": [
    "One can also directly compute the score of the tuned model on the test set, without needing to run `forecast()` or `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuned model testing score (negative sMAE): \", pipeline.score(X=X_test_df, y=y_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd3de1",
   "metadata": {},
   "source": [
    "<a id='multi-visualization'></a>\n",
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd45034",
   "metadata": {},
   "source": [
    "Finally, when given as input the forecasted variables, the `plot_forecast()` method displays an interactive plot of the predictions (for each target variable) and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast(fitted_pipeline=pipeline, summary_frame=y_forecast, additional_frames=dict(test=y_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b4ab8",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "## References\n",
    "* More examples and details: http://automl.oraclecorp.com/\n",
    "* Oracle AutoML: http://www.vldb.org/pvldb/vol13/p3166-yakovlev.pdf\n",
    "* sktime: https://www.sktime.org/en/latest/\n",
    "* statsmodels: https://www.statsmodels.org/stable/index.html\n",
    "* M4 Competition: https://mofc.unic.ac.cy/m4/\n",
    "* Airline Dataset: https://www.sktime.org/en/stable/api_reference/auto_generated/sktime.datasets.load_airline.html\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:automlx_p38_cpu_v2]",
   "language": "python",
   "name": "conda-env-automlx_p38_cpu_v2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
