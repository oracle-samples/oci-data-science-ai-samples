{
 "cells": [
  {
   "cell_type": "raw",
   "id": "972c0a8d",
   "metadata": {},
   "source": [
    "@notebook{feature_store-quickstart.ipynb,\n",
    "    title: Using Data Flow Run to create Feature Store Entities,\n",
    "    summary: Using Data Flow Run to create design time entities of OCI feature store\n",
    "    developed_on: fspyspark32_p38_cpu_v3,\n",
    "    keywords: feature store,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3e35c",
   "metadata": {},
   "source": [
    "# <font color=red>Using Data Flow Run to create design time entities of OCI Feature Store</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=teal> Oracle Cloud Infrastructure Data Science Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6d906",
   "metadata": {},
   "source": [
    "## Contents:\n",
    " - <a href='#intro'>Introduction</a>\n",
    "     - <a href='#prerequisite'>Setup</a>\n",
    "         - <a href='#policy'>Policy</a>\n",
    "         - <a href='#var'>Variables</a>\n",
    " - <a href='#appscript'>Application Script</a>\n",
    " - <a href='#jobs'>Create and Run a Data Flow Application</a>\n",
    "     - <a href='#conf'>Configurating Job</a>\n",
    "     - <a href='#run'>Run the Data Flow Application</a>\n",
    " - <a href='#clean_up'>Clean Up</a>\n",
    " - <a href='#ref'>References</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d2d5a",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction \n",
    "\n",
    "Data Flow is a hosted Apache Spark server. It is quick to start, and can scale to handle large datasets in parallel. ADS provides a convenient API for creating and maintaining workloads on Data Flow. In this notebook, we will use the Accelerated Data Science SDK (ADS) to help us define a Data Flow Job to create design time entities of OCI feature store which can later be used to ingest feature data.\n",
    "\n",
    "For more information on using ADS for data flow, you can go to our [documentation](https://docs.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/jobs/index.html).\n",
    "\n",
    "<a id='prerequisite'></a>\n",
    "### Setup\n",
    "\n",
    "<a id='policy'></a>\n",
    "#### Policy\n",
    "\n",
    "To control who has access to Data Flow, and the type of access for each group of users, you must create policies. See [Data Flow Policies](https://docs.oracle.com/en-us/iaas/data-flow/using/policies.htm) and [Data Catalog Metastore Required Policies](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm) for more details.\n",
    "\n",
    "<a id='var'></a>\n",
    "#### Variables\n",
    "\n",
    "To run this notebook, you must provide some information about your tenancy configuration. To connect to the metastore, replace `<metastore_id>` with the OCID for the metastore.A Hive Metastore is the central repository of metadata for a Hive cluster. It stores metadata for data structures such as databases, tables, and partitions in a relational database, backed by files on Object Storage. \n",
    "\n",
    "To create and run a Data Flow application, you must specify a compartment and buckets for storing logs and the Data Flow script. These resources must be in the same compartment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54dcd9f",
   "metadata": {},
   "source": [
    "<a id='jobs'></a>\n",
    "## Create and Run a Data Flow Application\n",
    "\n",
    "<a id='conf'></a>\n",
    "### Configurating Job\n",
    "\n",
    "The preferred method for running Data Flow applications is to run them as a Job. This Job allows you to better manage your resources and isolate the Data Flow application from the notebook. A `DataFlow` object must be created and is a subclass of `Infrastructure`. The object defines the metadata related to the Data Flow service. For example, the object stores properties specific to Data Flow service, such as `compartment_id`, `logs_bucket_uri`. This object also defines the connection between Data Flow and the metastore. To define the actual parameters needed to run the Data Flow job, a `DataFlowRuntime` object is required. The object is a subclass of `Runtime`. `DataFlowRuntime` stores properties related to the script to be run. The object defines the buckets used for the logs, the location of the Data Flow application script, and any command line options needed.\n",
    "\n",
    "To use a private bucket as the `logs_bucket`, ensure that a Data Flow Service policy has been added. See the [prerequisite step](#prerequisite) and the [policy setup page](https://docs.cloud.oracle.com/en-us/iaas/data-flow/using/dfs_getting_started.htm#policy_set_up) for more details.\n",
    "\n",
    "In the following example, the `dataflow_configs` variable is a `DataFlow` that has the compartment OCID, metastore OCID, log bucket URI, the compute shape for the driver, the compute shape that is used for the executor, and the version of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.jobs import DataFlow, Job, DataFlowRuntime\n",
    "import os\n",
    "import ads\n",
    "ads.set_auth(\"resource_principal\")\n",
    "logs_bucket_uri = \"oci://<logs_bucket_uri>\"\n",
    "compartment_id = \"<compartment_id>\"\n",
    "metastore_id = \"<metastore_id>\"\n",
    "\n",
    "dataflow_configs = (\n",
    "    DataFlow()\n",
    "    .with_compartment_id(compartment_id)\n",
    "    .with_logs_bucket_uri(logs_bucket_uri)\n",
    "    .with_driver_shape(\"VM.Standard.E4.Flex\")\n",
    "    .with_driver_shape_config(ocpus=2, memory_in_gbs=32)\n",
    "    .with_executor_shape(\"VM.Standard.E4.Flex\")\n",
    "    .with_executor_shape_config(ocpus=4, memory_in_gbs=64)\n",
    "    .with_spark_version(\"3.2.1\")\n",
    "    .with_metastore_id(metastore_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5919e30",
   "metadata": {},
   "source": [
    "<a id='appscript'></a>\n",
    "### Application Script\n",
    "\n",
    "An application script is used to execute the Data Flow job. The following cell creates this script and saves it to local storage. However, Data Flow requires that the script is stored in Object Storage as it cannot access your notebook session. The ADS framework takes care of uploading this script to Object Storage for you.\n",
    "\n",
    "ADS DataFlowRuntime doesnt support with_service_conda for now.User neede to publish the conda pack and use it as custom conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!odsc conda publish -s  /home/datascience/conda/fspyspark32_p38_cpu_v3 --uri oci://<object storage path> --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98579b30",
   "metadata": {},
   "source": [
    "The `runtime_config` variable is a `DataFlowRuntime` object. It contains information about the location of the script and the bucket for the script. The script URI defines the location of the Data Flow application script. This can be on local storage or in Object Storage. If the path is local, then the script bucket must be specified so that the framework can upload the script to the Object Storage bucket. Data Flow requires a script to be available in Object Storage. The URI for buckets must have the following format `oci://<bucket_name>@<namespace>/<prefix>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_config = (\n",
    "    DataFlowRuntime()\n",
    "    .with_script_uri(\"feature_store_creation.py\")\n",
    "    .with_script_bucket(\"oci://<script_bucket_uri>\")\n",
    "    .with_custom_conda(\"oci://<conda_pack_uri>\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45d199",
   "metadata": {},
   "source": [
    "The following cell creates a Job that executes the Data Flow application. The `Job` object needs a name, information about the Data Flow cluster infrastructure, and the runtime configuration. The `.create()` method is used to create the Data Flow application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Job(name=\"FS DF Application\", infrastructure=dataflow_configs, runtime=runtime_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb77b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.create(overwrite=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec0e80",
   "metadata": {},
   "source": [
    "<a id='run'></a>\n",
    "### Run the Data Flow Application\n",
    "\n",
    "To run this Data Flow application, call the `.run()` method. It creates a `DataFlowRun` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223c442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_run = df.run()\n",
    "print(df_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71021f9b",
   "metadata": {},
   "source": [
    "The `.watch()` method on the `DataFlowRun` object accesses the logs and prints them to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352047a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_run.watch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark_3_v1]",
   "language": "python",
   "name": "conda-env-pyspark_3_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
